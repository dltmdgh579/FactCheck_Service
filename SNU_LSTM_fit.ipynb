{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subtle-powder",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘정치 세대교체’ 바람 일으킨 이준석, 대선엔 출마 못한다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>액상형 전자담배는 니코틴이 함유되지 않아 담배가 아니다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>한국 백신 접종률 세계 100위권 이하다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>광역도시 없는 지역권은 실제 수요와 관계없이 획일적으로 국가교통망 정책에서 소외되고 있다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>\" ‘당·정·청 전원회의’는 '운동권 용어'다\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>\"늘어가는 해양사고, 전북 관내 잠수구조인력 전무(全無)\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>\"청와대 하루 평균 업무추진비, 이명박 768만원, 박근혜 814만원, 문재인 55만원\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>국민연금공단 기금운용본부가 전주로 이전한 후, 해외투자자가 국민연금을 '패싱'한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>문재인 대통령이 미국 방문에서 푸대접 받았다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1224 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               document  label\n",
       "0                  현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다      1\n",
       "1                      ‘정치 세대교체’ 바람 일으킨 이준석, 대선엔 출마 못한다      1\n",
       "2                        액상형 전자담배는 니코틴이 함유되지 않아 담배가 아니다      1\n",
       "3                                한국 백신 접종률 세계 100위권 이하다      1\n",
       "4     광역도시 없는 지역권은 실제 수요와 관계없이 획일적으로 국가교통망 정책에서 소외되고 있다      1\n",
       "...                                                 ...    ...\n",
       "1219                        \" ‘당·정·청 전원회의’는 '운동권 용어'다\"?      0\n",
       "1220                  \"늘어가는 해양사고, 전북 관내 잠수구조인력 전무(全無)\"?      0\n",
       "1221  \"청와대 하루 평균 업무추진비, 이명박 768만원, 박근혜 814만원, 문재인 55만원\"      0\n",
       "1222      국민연금공단 기금운용본부가 전주로 이전한 후, 해외투자자가 국민연금을 '패싱'한다      0\n",
       "1223                           문재인 대통령이 미국 방문에서 푸대접 받았다      0\n",
       "\n",
       "[1224 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sys\n",
    "\n",
    "# ==============================================================\n",
    "# 승호 S3로부터 CSV 파일 불러오기.\n",
    "if sys.version_info[0] < 3:\n",
    "    from io import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = ''\n",
    "aws_secret = ''\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'snucsv'\n",
    "\n",
    "object_key = 'SNU_FastText_1200.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "object_key2 = 'SNU_Data_1200.csv'\n",
    "csv_obj2 = client.get_object(Bucket=bucket_name, Key=object_key2)\n",
    "body2 = csv_obj2['Body']\n",
    "csv_string2 = body2.read().decode('cp949')\n",
    "\n",
    "df = pd.read_csv(StringIO(csv_string))\n",
    "df2 = pd.read_csv(StringIO(csv_string2))\n",
    "df\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "german-capital",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.fillna(0)\n",
    "df_mul = df_train.mul(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "otherwise-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = df_train.values.tolist()\n",
    "df_array = np.array(df_list)\n",
    "df_array\n",
    "\n",
    "df_list2 = df_mul.values.tolist()\n",
    "df_array2 = np.array(df_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eligible-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(df2['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mounted-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "organized-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(9294, 50))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "basic-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "expanded-destruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "196/196 [==============================] - 2s 6ms/step - loss: 0.6923 - acc: 0.5456 - val_loss: 0.7852 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.00000, saving model to best_model.h5\n",
      "Epoch 2/15\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 0.6458 - acc: 0.6258 - val_loss: 1.3121 - val_acc: 0.0490\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.00000 to 0.04898, saving model to best_model.h5\n",
      "Epoch 3/15\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 0.5728 - acc: 0.7161 - val_loss: 0.9865 - val_acc: 0.2939\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.04898 to 0.29388, saving model to best_model.h5\n",
      "Epoch 4/15\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 0.5646 - acc: 0.7322 - val_loss: 0.8496 - val_acc: 0.4898\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.29388 to 0.48980, saving model to best_model.h5\n",
      "Epoch 5/15\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 0.5172 - acc: 0.7576 - val_loss: 0.8592 - val_acc: 0.4122\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.48980\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(df_array2, y_train, epochs=15, callbacks=[es, mc], batch_size=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "spoken-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('SNU_LSTM_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "original-campus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 200)         1858800   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               168448    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,027,377\n",
      "Trainable params: 2,027,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fitting-conservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 100)         929400    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 971,705\n",
      "Trainable params: 971,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-animation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-proposition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "special-corporation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "879131it [01:24, 10411.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 879130 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1224/1224 [00:07<00:00, 157.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size :  3999\n",
      "number of null word embeddings: 354\n",
      "sample words not found:  ['6조' '82~88년' '2020년' '1억' '2017년' '1천' '미프진' '10년' '6.13' '360']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sys\n",
    "from gensim.models import FastText\n",
    "import os, csv, math, codecs\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "\n",
    "# =========================================================================\n",
    "# Vectorizer의 argument인 tokenizer에 KoNLPy의 pos 함수로 대체.\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        clean_words = []  # 정제된 단어 리스트\n",
    "        for word in pos:\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\n",
    "                    #if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\n",
    "                    clean_words.append(word[0])\n",
    "        return clean_words\n",
    "\n",
    "# =========================================================================\n",
    "\n",
    "embeddings_index = {}\n",
    "f = codecs.open('/home/ubuntu/FastText/wiki.ko.vec', encoding='utf-8')\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('found %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# ==============================================================\n",
    "if sys.version_info[0] < 3:\n",
    "    from io import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = ''\n",
    "aws_secret = ''\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'snucsv'\n",
    "\n",
    "object_key = 'SNU_Data_1200.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "# load data\n",
    "train_df = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train = train_df[label_names].values\n",
    "\n",
    "# test_df = pd.read_csv('D:\\pythonProject\\FakeNewsFiltering\\SNU_Validation_200.csv', encoding='CP949')\n",
    "# test_df = test_df.fillna('_NA_')\n",
    "\n",
    "# print(train_df)\n",
    "train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "# ==================================================================================\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "my_Tokenizer = MyTokenizer(Okt())\n",
    "\n",
    "raw_docs_train = train_df['document'].tolist()\n",
    "# raw_docs_test = test_df['document'].tolist()\n",
    "# print(raw_docs_test)\n",
    "num_classes = len(label_names)\n",
    "\n",
    "processed_docs_train = []\n",
    "# processed_docs_test = []\n",
    "\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = my_Tokenizer(doc)\n",
    "    processed_docs_train.append(tokens)\n",
    "\n",
    "# for doc in tqdm(raw_docs_test):\n",
    "#     tokens = my_Tokenizer(doc)\n",
    "#     processed_docs_test.append(tokens)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "# tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)\n",
    "tokenizer.fit_on_texts(processed_docs_train)\n",
    "# tokenizer.fit_on_texts(processed_docs_train+proccessed_docs_test)\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "# word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size : \", len(word_index))\n",
    "\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "# word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)\n",
    "# ====================================================================================\n",
    "# training params\n",
    "batch_size = 256\n",
    "num_epochs = 40\n",
    "\n",
    "# model parameters\n",
    "num_filters = 64\n",
    "embed_dim = 300\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# embedding matrix\n",
    "\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "\n",
    "# print(word_index)\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "        # cc.ko.300.vec에서 찾지 못한 단어들의 리스트.\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceramic-times",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extension horovod.torch has not been built: /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-37m-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n",
      "[2021-08-10 10:12:13.802 ip-172-31-2-138:1564 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-08-10 10:12:14.178 ip-172-31-2-138:1564 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 300)           1200000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10, 32)            9632      \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 10, 128)           49664     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 10, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 10, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 10, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,599,073\n",
      "Trainable params: 399,073\n",
      "Non-trainable params: 1,200,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "7/7 [==============================] - 32s 690ms/step - loss: 0.6922 - accuracy: 0.5131 - val_loss: 0.6809 - val_accuracy: 0.5897\n",
      "Epoch 2/40\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.6852 - accuracy: 0.5703 - val_loss: 0.6773 - val_accuracy: 0.5897\n",
      "Epoch 3/40\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.6837 - accuracy: 0.5623 - val_loss: 0.6788 - val_accuracy: 0.5897\n",
      "Epoch 4/40\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.6826 - accuracy: 0.5655 - val_loss: 0.6793 - val_accuracy: 0.5897\n",
      "Epoch 5/40\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.6833 - accuracy: 0.5759 - val_loss: 0.6800 - val_accuracy: 0.5897\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================================\n",
    "from keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(Embedding(nb_words, embed_dim, input_length=max_seq_len, weights=[embedding_matrix], trainable=False))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "# ================================================================================\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history = model.fit(word_seq_train_shuffle, y_train_shuffle, batch_size=128,\n",
    "          epochs=num_epochs, validation_split=0.3, callbacks=[es_callback], shuffle=False)\n",
    "\n",
    "# predictions = model.predict_classes(word_seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dominican-spring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1224/1224 [00:02<00:00, 502.09it/s]\n"
     ]
    }
   ],
   "source": [
    "word_seq_all_shuffle = train_df.sample(frac=1).reset_index(drop=True)\n",
    "word_seq_train_shuffle = word_seq_all_shuffle['document'].tolist()\n",
    "y_train_shuffle = word_seq_all_shuffle['label'].values\n",
    "\n",
    "processed_docs_train_shuffle = []\n",
    "\n",
    "for doc in tqdm(word_seq_train_shuffle):\n",
    "    tokens = my_Tokenizer(doc)\n",
    "    processed_docs_train_shuffle.append(tokens)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs_train_shuffle)\n",
    "word_seq_train_shuffle = tokenizer.texts_to_sequences(processed_docs_train_shuffle)\n",
    "word_seq_train_shuffle = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "purple-reflection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_seq_train_tuple = tuple(word_seq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hidden-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# seed = 7\n",
    "# kfold = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "# result = cross_val_score(model, word_seq_train_shuffle, y_train_shuffle, scoring='accuracy', cv=kfold)\n",
    "\n",
    "# print('교차 검증별 정확도:', np.round(result,4))\n",
    "# print('평균 검증 정확도:', np.round(np.mean(result), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gentle-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('SNU_LSTM_Model_shuffle.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "understood-count",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['현재', 'AI', '기술', '불완전하기', '때문', '가짜', '뉴스', '완전히', '걸러', '없다']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5054651]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ['현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다']\n",
    "tokens_response = []\n",
    "\n",
    "for i in response:\n",
    "    tokens = my_Tokenizer(response[0])\n",
    "    tokens_response.append(tokens)\n",
    "    print(tokens_response)\n",
    "\n",
    "word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "score = model.predict(word_seq_response)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

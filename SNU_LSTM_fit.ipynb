{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "special-corporation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "879131it [01:46, 8250.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 879130 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1224/1224 [00:09<00:00, 125.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size :  3999\n",
      "number of null word embeddings: 354\n",
      "sample words not found:  ['기본소득제' '30년' '파인을' '비분' '13일' '382' '5.5' '걸쳐도' '8.5' '연애횟수']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sys\n",
    "from gensim.models import FastText\n",
    "import os, csv, math, codecs\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "\n",
    "# =========================================================================\n",
    "# Vectorizer의 argument인 tokenizer에 KoNLPy의 pos 함수로 대체.\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        clean_words = []  # 정제된 단어 리스트\n",
    "        for word in pos:\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\n",
    "                    #if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\n",
    "                    clean_words.append(word[0])\n",
    "        return clean_words\n",
    "\n",
    "# =========================================================================\n",
    "\n",
    "embeddings_index = {}\n",
    "f = codecs.open('/home/ubuntu/FastText/wiki.ko.vec', encoding='utf-8')\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('found %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# ==============================================================\n",
    "if sys.version_info[0] < 3:\n",
    "    from io import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = 'AKIA2EDEFCNPX2G7FWNY'\n",
    "aws_secret = 'Xt1EJXPsRdI27VI7TBSCsRMNJWsewq9FY0g4vDU7'\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'snucsv'\n",
    "\n",
    "object_key = 'SNU_Data_1200.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "# load data\n",
    "train_df = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train = train_df[label_names].values\n",
    "\n",
    "# test_df = pd.read_csv('D:\\pythonProject\\FakeNewsFiltering\\SNU_Validation_200.csv', encoding='CP949')\n",
    "# test_df = test_df.fillna('_NA_')\n",
    "\n",
    "# print(train_df)\n",
    "train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "# ==================================================================================\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "my_Tokenizer = MyTokenizer(Okt())\n",
    "\n",
    "raw_docs_train = train_df['document'].tolist()\n",
    "# raw_docs_test = test_df['document'].tolist()\n",
    "# print(raw_docs_test)\n",
    "num_classes = len(label_names)\n",
    "\n",
    "processed_docs_train = []\n",
    "# processed_docs_test = []\n",
    "\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = my_Tokenizer(doc)\n",
    "    processed_docs_train.append(tokens)\n",
    "\n",
    "# for doc in tqdm(raw_docs_test):\n",
    "#     tokens = my_Tokenizer(doc)\n",
    "#     processed_docs_test.append(tokens)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "# tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)\n",
    "tokenizer.fit_on_texts(processed_docs_train)\n",
    "# tokenizer.fit_on_texts(processed_docs_train+proccessed_docs_test)\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "# word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size : \", len(word_index))\n",
    "\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "# word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)\n",
    "# ====================================================================================\n",
    "# training params\n",
    "batch_size = 256\n",
    "num_epochs = 40\n",
    "\n",
    "# model parameters\n",
    "num_filters = 64\n",
    "embed_dim = 300\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# embedding matrix\n",
    "\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "\n",
    "# print(word_index)\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "        # cc.ko.300.vec에서 찾지 못한 단어들의 리스트.\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caring-chemistry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 300)           1200000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10, 32)            9632      \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 10, 128)           49664     \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 10, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 10, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 10, 128)           98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,599,073\n",
      "Trainable params: 399,073\n",
      "Non-trainable params: 1,200,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "7/7 [==============================] - 28s 1s/step - loss: 0.6901 - accuracy: 0.5715 - val_loss: 0.6844 - val_accuracy: 0.5679\n",
      "Epoch 2/40\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.6844 - accuracy: 0.5715 - val_loss: 0.6846 - val_accuracy: 0.5679\n",
      "Epoch 3/40\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.6819 - accuracy: 0.5715 - val_loss: 0.6847 - val_accuracy: 0.5679\n",
      "Epoch 4/40\n",
      "7/7 [==============================] - 0s 54ms/step - loss: 0.6808 - accuracy: 0.5715 - val_loss: 0.6853 - val_accuracy: 0.5679\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================================\n",
    "from keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(Embedding(nb_words, embed_dim, input_length=max_seq_len, weights=[embedding_matrix], trainable=False))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "# ================================================================================\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history = model.fit(word_seq_train_shuffle, y_train_shuffle, batch_size=128,\n",
    "          epochs=num_epochs, validation_split=0.3, callbacks=[es_callback], shuffle=False)\n",
    "\n",
    "# predictions = model.predict_classes(word_seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compliant-sunday",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1224/1224 [00:02<00:00, 485.36it/s]\n"
     ]
    }
   ],
   "source": [
    "word_seq_all_shuffle = train_df.sample(frac=1).reset_index(drop=True)\n",
    "word_seq_train_shuffle = word_seq_all_shuffle['document'].tolist()\n",
    "y_train_shuffle = word_seq_all_shuffle['label'].values\n",
    "\n",
    "processed_docs_train_shuffle = []\n",
    "\n",
    "for doc in tqdm(word_seq_train_shuffle):\n",
    "    tokens = my_Tokenizer(doc)\n",
    "    processed_docs_train_shuffle.append(tokens)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs_train_shuffle)\n",
    "word_seq_train_shuffle = tokenizer.texts_to_sequences(processed_docs_train_shuffle)\n",
    "word_seq_train_shuffle = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gentle-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('SNU_LSTM_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "grave-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('SNU_LSTM_Model_shuffle.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "combined-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "SNU_model_shuffle = load_model('SNU_LSTM_Model_shuffle.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rolled-poker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[594, 2165, 663, 2166, 22, 76, 126, 2167, 2168, 8]\n",
      "(3, 10)\n",
      "[[ 594 2165  663 2166   22   76  126 2167 2168    8]]\n",
      "[0.47938355803489685]\n",
      "[[   0    0    0    0    0 2157 1842  236  243 1331]]\n",
      "[0.47938355803489685, 0.5113765001296997]\n",
      "[[0 0 0 0 0 0 0 0 0 0]]\n",
      "[0.47938355803489685, 0.5113765001296997, 0.5069465041160583]\n"
     ]
    }
   ],
   "source": [
    "response = ['현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다', '카카오톡 유료화 서비스 시행하나', '왜이러는거야 도대체']\n",
    "tokens_response = []\n",
    "\n",
    "for i in range(len(response)):\n",
    "    tokens = my_Tokenizer(response[i])\n",
    "    tokens_response.append(tokens)\n",
    "#     print(tokens_response)\n",
    "\n",
    "word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "print(word_seq_response[0])\n",
    "word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "print(word_seq_response.shape)\n",
    "\n",
    "score_list = []\n",
    "for sco in range(len(word_seq_response)):\n",
    "    word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "    print(word_seq_response_to_score)\n",
    "    score_list.append(float(SNU_model_shuffle.predict(word_seq_response_to_score)))\n",
    "    print(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "understood-count",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['현재', 'AI', '기술', '불완전하기', '때문', '가짜', '뉴스', '완전히', '걸러', '없다']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5054651]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ['현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다']\n",
    "tokens_response = []\n",
    "\n",
    "for i in response:\n",
    "    tokens = my_Tokenizer(response[0])\n",
    "    tokens_response.append(tokens)\n",
    "    print(tokens_response)\n",
    "\n",
    "word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "score = model.predict(word_seq_response)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

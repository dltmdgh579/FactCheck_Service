{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from http.server import BaseHTTPRequestHandler, HTTPServer\r\n",
    "import logging\r\n",
    "import requests\r\n",
    "import urllib.request\r\n",
    "import json\r\n",
    "import numpy as np\r\n",
    "from tensorflow.keras.models import load_model\r\n",
    "from gensim.models import FastText\r\n",
    "from konlpy.tag import Okt\r\n",
    "import boto3\r\n",
    "import sys\r\n",
    "import pandas as pd\r\n",
    "from tqdm import tqdm\r\n",
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from keras.preprocessing import sequence\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "import urllib.parse\r\n",
    "import urllib.request\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "import re\r\n",
    "import math\r\n",
    "\r\n",
    "MAX_NB_WORDS = 100000\r\n",
    "SNU_model_All = load_model('SNU_LSTM_Model_All_balance.h5')\r\n",
    "SNU_model_pol = load_model('SNU_LSTM_Model_정치_balance.h5')\r\n",
    "SNU_model_eco = load_model('SNU_LSTM_Model_경제_balance.h5')\r\n",
    "SNU_model_soc = load_model('SNU_LSTM_Model_사회_balance.h5')\r\n",
    "SNU_model_others = load_model('SNU_LSTM_Model_기타_balance.h5')\r\n",
    "\r\n",
    "Naver_Comments_All = load_model('Naver_Comments_Model_All.h5')\r\n",
    "Naver_Comments_pol = load_model('Naver_Comments_Model_정치.h5')\r\n",
    "Naver_Comments_eco = load_model('Naver_Comments_Model_경제.h5')\r\n",
    "Naver_Comments_soc = load_model('Naver_Comments_Model_사회.h5')\r\n",
    "Naver_Comments_others = load_model('Naver_Comments_Model_기타.h5')\r\n",
    "\r\n",
    "kakao_model = load_model('kakao_LSTM_model.h5')\r\n",
    "# SNU_model_shuffle = load_model('SNU_LSTM_Model_shuffle.h5')\r\n",
    "similar_model = FastText.load('similar_keyword_model')\r\n",
    "\r\n",
    "class MyTokenizer:\r\n",
    "    def __init__(self, tagger):\r\n",
    "        self.tagger = tagger\r\n",
    "\r\n",
    "    def __call__(self, sent):\r\n",
    "        pos = self.tagger.pos(sent)\r\n",
    "        clean_words = []  # 정제된 단어 리스트\r\n",
    "        for word in pos:\r\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\r\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\r\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\r\n",
    "                    #if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\r\n",
    "                    clean_words.append(word[0])\r\n",
    "        return clean_words\r\n",
    "\r\n",
    "\r\n",
    "my_Tokenizer = MyTokenizer(Okt())\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extension horovod.torch has not been built: /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-37m-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n",
      "[2021-09-24 10:10:57.643 ip-172-31-32-76:1711 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-09-24 10:10:57.716 ip-172-31-32-76:1711 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pymysql\r\n",
    "import sys\r\n",
    "import os\r\n",
    "import requests\r\n",
    "import base64\r\n",
    "import json\r\n",
    "import logging\r\n",
    "import time\r\n",
    "import pandas as pd\r\n",
    "import csv\r\n",
    "import datetime\r\n",
    "\r\n",
    "host = '' # rds endpoint\r\n",
    "port = 3306\r\n",
    "username = ''\r\n",
    "database = ''\r\n",
    "password = ''\r\n",
    "\r\n",
    "def connect_RDS(host, port, username, password, database):\r\n",
    "    try:\r\n",
    "        conn = pymysql.connect(host = host, user = username, password = password, db = database, port = port, use_unicode = True, charset = 'utf8')\r\n",
    "        cursor = conn.cursor()\r\n",
    "    \r\n",
    "    except:\r\n",
    "        logging.error('RDS에 연결되지 않았습니다.')\r\n",
    "        sys.exit(1)\r\n",
    "        \r\n",
    "    return conn, cursor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "if sys.version_info[0] < 3:\r\n",
    "    from io import StringIO # Python 2.x\r\n",
    "else:\r\n",
    "    from io import StringIO # Python 3.x\r\n",
    "\r\n",
    "# get your credentials from environment variables\r\n",
    "aws_id = ''\r\n",
    "aws_secret = ''\r\n",
    "\r\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\r\n",
    "        aws_secret_access_key=aws_secret)\r\n",
    "\r\n",
    "bucket_name = 'snucsv'\r\n",
    "label_names = [\"label\"]\r\n",
    "\r\n",
    "def s3_load(category):\r\n",
    "    object_key = category\r\n",
    "    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\r\n",
    "    body = csv_obj['Body']\r\n",
    "    csv_string = body.read().decode('cp949')\r\n",
    "\r\n",
    "    # load data\r\n",
    "    train_df = pd.read_csv(StringIO(csv_string))\r\n",
    "    y_train = train_df[label_names].values\r\n",
    "\r\n",
    "    train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\r\n",
    "    max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\r\n",
    "    \r\n",
    "    return train_df, y_train, max_seq_len\r\n",
    "\r\n",
    "def s3_Comments_load(category):\r\n",
    "    object_key = category\r\n",
    "    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\r\n",
    "    body = csv_obj['Body']\r\n",
    "    csv_string = body.read().decode('utf-8')\r\n",
    "\r\n",
    "    # load data\r\n",
    "    train_df = pd.read_csv(StringIO(csv_string))\r\n",
    "    y_train = train_df[label_names].values\r\n",
    "\r\n",
    "    train_df['doc_len'] = train_df['Comments'].apply(lambda words: len(words.split(\" \")))\r\n",
    "    max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\r\n",
    "    \r\n",
    "    return train_df, y_train, max_seq_len\r\n",
    "\r\n",
    "train_df_All_, y_train_All_, max_seq_len = s3_load('SNU_All_.csv')\r\n",
    "train_df_All_b, y_train_All_b, max_seq_len_All_b = s3_load('SNU_All_b.csv')\r\n",
    "train_df_eco, y_train_eco, max_seq_len_eco = s3_load('SNU_경제_b.csv')\r\n",
    "train_df_pol, y_train_pol, max_seq_len_pol = s3_load('SNU_정치_b.csv')\r\n",
    "train_df_soc, y_train_soc, max_seq_len_soc = s3_load('SNU_사회_b.csv')\r\n",
    "train_df_others, y_train_others, max_seq_len_others = s3_load('SNU_기타_b.csv')\r\n",
    "\r\n",
    "train_df_Comments_All, y_train_Comments_All, max_seq_len_Comments = s3_Comments_load('Naver_Comments_All.csv')\r\n",
    "train_df_Comments_pol, y_train_Comments_pol, max_seq_len_Comments_pol = s3_Comments_load('Naver_Comments_정치.csv')\r\n",
    "train_df_Comments_eco, y_train_Comments_eco, max_seq_len_Comments_eco = s3_Comments_load('Naver_Comments_경제.csv')\r\n",
    "train_df_Comments_soc, y_train_Comments_soc, max_seq_len_Comments_soc = s3_Comments_load('Naver_Comments_사회.csv')\r\n",
    "train_df_Comments_others, y_train_Comments_All, max_seq_len_Comments_others = s3_Comments_load('Naver_Comments_기타.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_df_Comments_All"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"독립유공자 후손지원, 청년수당, 여성안심택배, 올빼미 버스, 서울로 7017, 도...</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"문정부 집값 잡을 생각이 없구나~~~그래 서민들을 우롱했다~~당신들도 가진자들이다...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"부정적으로만 보지말시구여 얼마나 살기가 개같으면 코로나인데도 불구하고 모이는지가 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 뭐? 한반도 운전자론? 중립외교? 양쪽에서 이득을 취해?...</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"성범죄는 그나마 개선된거죠 사기죄는 한결같이 제자리...사기 당한 사람들 보면 거...</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9610</th>\n",
       "      <td>\"어이 기자양반 그럼 분단비용은 싸게 먹힙니까? 돈으로 추산 안되는 한반도 디스카운...</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9611</th>\n",
       "      <td>\"주휴시간 폐지, 최저임금 차등적용, 세금, 4대보험 본인부담 하게 만들어라.\"</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9612</th>\n",
       "      <td>\"연동률 20프로로 낮추어라\"</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9613</th>\n",
       "      <td>\"헛다리에 헛발질 자한당 답다\"</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9614</th>\n",
       "      <td>\"미국영화보면 아동보호 엄격하던데 왜 그리 좋은건 못따라하냐. 미국이란 나라 축소모...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9615 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Comments  label  doc_len\n",
       "0     \"독립유공자 후손지원, 청년수당, 여성안심택배, 올빼미 버스, 서울로 7017, 도...      1       26\n",
       "1     \"문정부 집값 잡을 생각이 없구나~~~그래 서민들을 우롱했다~~당신들도 가진자들이다...      1       18\n",
       "2     \"부정적으로만 보지말시구여 얼마나 살기가 개같으면 코로나인데도 불구하고 모이는지가 ...      0       21\n",
       "3     \"ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 뭐? 한반도 운전자론? 중립외교? 양쪽에서 이득을 취해?...      0       17\n",
       "4     \"성범죄는 그나마 개선된거죠 사기죄는 한결같이 제자리...사기 당한 사람들 보면 거...      1       41\n",
       "...                                                 ...    ...      ...\n",
       "9610  \"어이 기자양반 그럼 분단비용은 싸게 먹힙니까? 돈으로 추산 안되는 한반도 디스카운...      0       26\n",
       "9611       \"주휴시간 폐지, 최저임금 차등적용, 세금, 4대보험 본인부담 하게 만들어라.\"      0        9\n",
       "9612                                   \"연동률 20프로로 낮추어라\"      0        3\n",
       "9613                                  \"헛다리에 헛발질 자한당 답다\"      0        4\n",
       "9614  \"미국영화보면 아동보호 엄격하던데 왜 그리 좋은건 못따라하냐. 미국이란 나라 축소모...      1       16\n",
       "\n",
       "[9615 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**단어 벡터화 및 패딩**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def embedding_padding(train_df):\r\n",
    "    raw_docs_train = train_df['document'].tolist()\r\n",
    "    num_classes = len(label_names)\r\n",
    "    print(num_classes)\r\n",
    "    processed_docs_train = []\r\n",
    "\r\n",
    "    for doc in tqdm(raw_docs_train):\r\n",
    "        tokens = my_Tokenizer(doc)\r\n",
    "        processed_docs_train.append(tokens)\r\n",
    "\r\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\r\n",
    "    tokenizer.fit_on_texts(processed_docs_train)\r\n",
    "    return tokenizer\r\n",
    "\r\n",
    "def Comments_embedding_padding(train_df):\r\n",
    "    raw_docs_train = train_df['Comments'].tolist()\r\n",
    "    num_classes = len(label_names)\r\n",
    "    print(num_classes)\r\n",
    "    processed_docs_train = []\r\n",
    "\r\n",
    "    for doc in tqdm(raw_docs_train):\r\n",
    "        tokens = my_Tokenizer(doc)\r\n",
    "        processed_docs_train.append(tokens)\r\n",
    "\r\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\r\n",
    "    tokenizer.fit_on_texts(processed_docs_train)\r\n",
    "    return tokenizer\r\n",
    "\r\n",
    "tokenizer_All = embedding_padding(train_df_All_b)\r\n",
    "tokenizer_pol = embedding_padding(train_df_pol)\r\n",
    "tokenizer_soc = embedding_padding(train_df_soc)\r\n",
    "tokenizer_eco = embedding_padding(train_df_eco)\r\n",
    "tokenizer_others = embedding_padding(train_df_others)\r\n",
    "\r\n",
    "tokenizer_Comments_All = Comments_embedding_padding(train_df_Comments_All)\r\n",
    "tokenizer_Comments_pol = Comments_embedding_padding(train_df_Comments_pol)\r\n",
    "tokenizer_Comments_eco = Comments_embedding_padding(train_df_Comments_eco)\r\n",
    "tokenizer_Comments_soc = Comments_embedding_padding(train_df_Comments_soc)\r\n",
    "tokenizer_Comments_others = Comments_embedding_padding(train_df_Comments_others)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/1441 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1441/1441 [00:16<00:00, 89.07it/s] \n",
      "  1%|          | 4/509 [00:00<00:23, 21.30it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 509/509 [00:01<00:00, 307.24it/s]\n",
      "  8%|▊         | 41/490 [00:00<00:01, 401.59it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 490/490 [00:01<00:00, 327.03it/s]\n",
      " 18%|█▊        | 35/198 [00:00<00:00, 345.84it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 198/198 [00:00<00:00, 305.02it/s]\n",
      " 17%|█▋        | 43/257 [00:00<00:00, 427.12it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 257/257 [00:00<00:00, 439.59it/s]\n",
      "  0%|          | 0/9615 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 9615/9615 [02:05<00:00, 76.68it/s] \n",
      "  0%|          | 0/3478 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3478/3478 [00:51<00:00, 68.11it/s]\n",
      "  0%|          | 0/1231 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1231/1231 [00:18<00:00, 68.33it/s]\n",
      "  0%|          | 0/2934 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2934/2934 [00:45<00:00, 64.77it/s]\n",
      "  0%|          | 0/1537 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1537/1537 [00:22<00:00, 67.94it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# word_seq_train = tokenizer_All.texts_to_sequences(processed_docs_train)\r\n",
    "# word_index = tokenizer_All.word_index\r\n",
    "# print('dictionary size :', len(word_index))\r\n",
    "\r\n",
    "# word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\r\n",
    "# word_seq_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# 카카오 응답 데이터 전처리\r\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\r\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\r\n",
    "train_data = pd.read_table('ratings_train.txt')\r\n",
    "test_data = pd.read_table('ratings_test.txt')\r\n",
    "\r\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\r\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\r\n",
    "text = 'do!!! you expect... people~ to~ read~ the FAQ, etc. and actually accept hard~! atheism?@@'\r\n",
    "re.sub(r'[^a-zA-Z ]', '', text) #알파벳과 공백을 제외하고 모두 제거\r\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")# 한글과 공백을 제외하고 모두 제거\r\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\r\n",
    "train_data['document'].replace('', np.nan, inplace=True)\r\n",
    "train_data = train_data.dropna(how = 'any')\r\n",
    "\r\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\r\n",
    "okt = Okt()\r\n",
    "X_train = []\r\n",
    "for sentence in tqdm(train_data['document']):\r\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\r\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\r\n",
    "    X_train.append(temp_X)\r\n",
    "\r\n",
    "tokenizer2 = Tokenizer()\r\n",
    "tokenizer2.fit_on_texts(X_train)\r\n",
    "\r\n",
    "threshold = 3\r\n",
    "total_cnt = len(tokenizer2.word_index) # 단어의 수\r\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\r\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\r\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\r\n",
    "\r\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\r\n",
    "for key, value in tokenizer2.word_counts.items():\r\n",
    "    total_freq = total_freq + value\r\n",
    "\r\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\r\n",
    "    if(value < threshold):\r\n",
    "        rare_cnt = rare_cnt + 1\r\n",
    "        rare_freq = rare_freq + value\r\n",
    "\r\n",
    "vocab_size = total_cnt - rare_cnt + 1\r\n",
    "tokenizer_kakao = Tokenizer(vocab_size) \r\n",
    "tokenizer_kakao.fit_on_texts(X_train)\r\n",
    "X_train = tokenizer_kakao.texts_to_sequences(X_train)\r\n",
    "\r\n",
    "max_len = 30"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if sys.path[0] == '':\n",
      "100%|██████████| 145393/145393 [23:30<00:00, 103.05it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**사용자에게 받은 데이터 전처리 테스트**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# response = ['게르마늄 팔찌, 과학적으로 효능 있을까?', '빌려쓰는 IT서비스 클라우드컴퓨팅, 실제 비용절감 효과 있나', '\"심장마비 왔을 때 강하게 기침하면 막을 수 있다\"\t']\r\n",
    "# tokens_response = []\r\n",
    "\r\n",
    "# for i in range(len(response)):\r\n",
    "#     tokens = my_Tokenizer(response[i])\r\n",
    "#     tokens_response.append(tokens)\r\n",
    "# print(tokens_response)\r\n",
    "\r\n",
    "# word_seq_response = tokenizer.texts_to_sequences(tokens_response)\r\n",
    "# print(word_seq_response)\r\n",
    "# word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\r\n",
    "# print(word_seq_response.shape)\r\n",
    "\r\n",
    "# score_list = []\r\n",
    "# for sco in range(len(word_seq_response)):\r\n",
    "#     word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\r\n",
    "#     print(word_seq_response_to_score)\r\n",
    "#     score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\r\n",
    "# print(score_list)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**입력 데이터 처리 함수**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "# ===========================================================================================================================\r\n",
    "# 카카오톡 응답 데이터 score\r\n",
    "\r\n",
    "def kakao_data_score(response):\r\n",
    "    objectivity = ''\r\n",
    "    response = okt.morphs(response, stem=True) # 토큰화\r\n",
    "    response = [word for word in response if not word in stopwords] # 불용어 제거\r\n",
    "    encoded = tokenizer_kakao.texts_to_sequences([response]) # 정수 인코딩\r\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\r\n",
    "    score = float(kakao_model.predict(pad_new)) # 예측\r\n",
    "    if(score > 0.8 or score < 0.2):\r\n",
    "        objectivity = '객관성이 매우 낮습니다.'\r\n",
    "    elif(score > 0.65 or score < 0.35):\r\n",
    "        objectivity = '객관성이 낮습니다.'\r\n",
    "    else:\r\n",
    "        objectivity = '객관성이 높습니다.'\r\n",
    "    \r\n",
    "    return objectivity\r\n",
    "\r\n",
    "\r\n",
    "# ===========================================================================================================================\r\n",
    "# 단어 토큰화 및 빈도순 정렬 후 추출\r\n",
    "\r\n",
    "def tfidf_Vectorizer(response):\r\n",
    "    response_list = []\r\n",
    "    response_list.append(response)\r\n",
    "    tfidf_Vectorizer = TfidfVectorizer(tokenizer=my_Tokenizer, min_df=1) # df 값(단어가 몇 문장들에서 등장하였는지)을 최소 'min_df' 값으로 설정.\r\n",
    "    X = tfidf_Vectorizer.fit_transform(response_list).toarray()\r\n",
    "#     print(X.shape)    # X(2차원 배열)의 행,열 수를 출력.\r\n",
    "#     print(tfidf_Vectorizer.vocabulary_)   # 각 단어들이 배열 X에서 몇번째 열(인덱스 값)에 해당하는지 출력.\r\n",
    "\r\n",
    "\r\n",
    "    #pandas를 활용하여 각 단어들의 각 문장에서의 tf-idf 값들을 모두 더하고, 내림차순으로 정렬하여 상위 n개 출력\r\n",
    "    count = X.sum(axis=0)    # 2차원 배열 X에서 각 열을 기준으로 합을 구함. (각 단어들의 '최종' tf-idf 값으로 간주.)\r\n",
    "    word_count = pd.DataFrame({\r\n",
    "        '단어' : tfidf_Vectorizer.get_feature_names(),\r\n",
    "        '빈도' : count.flat\r\n",
    "    })\r\n",
    "    sorted_df = word_count.sort_values('빈도', ascending=False)\r\n",
    "#     print(sorted_df.head(10), \"\\n\")\r\n",
    "\r\n",
    "    word_ = list(np.array(sorted_df['단어'].tolist()))\r\n",
    "    return(word_)\r\n",
    "\r\n",
    "\r\n",
    "# ===========================================================================================================================\r\n",
    "# 네이버 댓글 분석\r\n",
    "\r\n",
    "def Naver_Score(response, tokenizer, tokenizer_Comments, SNU_Model, Naver_Comments_Model):\r\n",
    "    response = re.compile(\"[^A-Za-z0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]+\").sub('',response)\r\n",
    "    response = okt.morphs(response, stem=True) # 토큰화\r\n",
    "    response = [word for word in response if not word in stopwords] # 불용어 제거\r\n",
    "    response = ' '.join(response)\r\n",
    "    \r\n",
    "    word_ = tfidf_Vectorizer(response)\r\n",
    "    word__ = \"\"\r\n",
    "    if len(word_) <= 6:\r\n",
    "        word__ = response\r\n",
    "    else:\r\n",
    "        for i in range(1, min(len(word_), 7)):\r\n",
    "            word__ += word_[i]\r\n",
    "            word__ += \" \"\r\n",
    "    \r\n",
    "    baseurl = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=\"\r\n",
    "\r\n",
    "    url = baseurl + urllib.parse.quote_plus(word__)\r\n",
    "\r\n",
    "    html = urllib.request.urlopen(url).read()\r\n",
    "    soup = BeautifulSoup(html, 'html.parser')\r\n",
    "\r\n",
    "    title = soup.find_all(class_ = 'news_tit')\r\n",
    "\r\n",
    "    title_list = []\r\n",
    "    link_list = []\r\n",
    "\r\n",
    "    for i in title:\r\n",
    "        title_list.append(i.attrs['title'])\r\n",
    "        link_list.append(i.attrs['href'])\r\n",
    "\r\n",
    "# ================================================================\r\n",
    "\r\n",
    "    tokens_response = []\r\n",
    "    SNU_score_list = []\r\n",
    "\r\n",
    "    for i in range(len(title_list)):\r\n",
    "        tokens = my_Tokenizer(title_list[i])\r\n",
    "        tokens_response.append(tokens)\r\n",
    "\r\n",
    "    word_seq_response = tokenizer.texts_to_sequences(tokens_response)\r\n",
    "    word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\r\n",
    "\r\n",
    "    for sco in range(len(title_list)):\r\n",
    "        word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\r\n",
    "        SNU_score_list.append(float(SNU_Model.predict(word_seq_response_to_score)))\r\n",
    "        \r\n",
    "    df_qq = []\r\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}\r\n",
    "\r\n",
    "    allComments = []\r\n",
    "    def create_soup(url):\r\n",
    "        res = requests.get(url, headers=headers)\r\n",
    "        res.raise_for_status()\r\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\r\n",
    "        return soup\r\n",
    "\r\n",
    "    Comments_score_list = []\r\n",
    "    # 네이버 뉴스 url을 입력합니다.\r\n",
    "    url1 = \"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}\".format(word__)\r\n",
    "    soup = create_soup(url1)\r\n",
    "    url_check = []\r\n",
    "    for i in range(1, 30):\r\n",
    "        List = []\r\n",
    "        try:\r\n",
    "            url = soup.select_one(\"#sp_nws{} > div.news_wrap.api_ani_send > div > div.news_info > div.info_group > a:nth-of-type(2)\".format(i))['href']\r\n",
    "            url_check.append(url)\r\n",
    "            oid = url.split(\"oid=\")[1].split(\"&\")[0] #422\r\n",
    "            aid = url.split(\"aid=\")[1] #0000430957\r\n",
    "            page = 1\r\n",
    "            header = {\r\n",
    "                \"User-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\",\r\n",
    "                \"referer\": url,\r\n",
    "            }\r\n",
    "            # time.sleep(random.randint(1,2))\r\n",
    "\r\n",
    "            while True:\r\n",
    "                c_url = \"https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json?ticket=news&templateId=default_society&pool=cbox5&_callback=jQuery1707138182064460843_1523512042464&lang=ko&country=&objectId=news\" + oid + \"%2C\" + aid + \"&categoryId=&pageSize=20&indexSize=10&groupId=&listType=OBJECT&pageType=more&page=\" + str(\r\n",
    "                    page) + \"&refresh=false&sort=FAVORITE\"\r\n",
    "                # 파싱하는 단계입니다.\r\n",
    "                r = requests.get(c_url, headers=header)\r\n",
    "                cont = BeautifulSoup(r.content, \"html.parser\")\r\n",
    "                total_comm = str(cont).split('comment\":')[1].split(\",\")[0]\r\n",
    "\r\n",
    "                match = re.findall('\"contents\":([^\\*]*),\"userIdNo\"', str(cont))\r\n",
    "                # 댓글을 리스트에 중첩합니다.\r\n",
    "                try:\r\n",
    "                    for b in range(0,5):\r\n",
    "                        List.append(match[b])\r\n",
    "                except:\r\n",
    "                    pass\r\n",
    "                break\r\n",
    "\r\n",
    "            tokens_response = []\r\n",
    "            score_list = []\r\n",
    "\r\n",
    "            for i in range(len(List)):\r\n",
    "                tokens = my_Tokenizer(List[i])\r\n",
    "                tokens_response.append(tokens)\r\n",
    "\r\n",
    "            word_seq_response2 = tokenizer_Comments.texts_to_sequences(tokens_response)\r\n",
    "            word_seq_response2 = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len_Comments)\r\n",
    "\r\n",
    "            for sco in range(len(List)):\r\n",
    "                word_seq_response_to_score2 = word_seq_response2[sco].reshape(1,max_seq_len_Comments)\r\n",
    "                score_list.append(float(Naver_Comments_Model.predict(word_seq_response_to_score2)))\r\n",
    "\r\n",
    "            Comments_score_list.append(np.mean(score_list))\r\n",
    "\r\n",
    "        except:\r\n",
    "            pass\r\n",
    "        if len(url_check) == len(title_list):\r\n",
    "            break\r\n",
    "    \r\n",
    "    print(SNU_score_list)\r\n",
    "    print(Comments_score_list)\r\n",
    "    a = [score * 0.5 for score in SNU_score_list]\r\n",
    "#     b = [score * 0.5 if math.isnan(score)==False else score2 * 0.5 for score2 in SNU_score_list for score in Comments_score_list]\r\n",
    "    b = [score * 0.5 for score in Comments_score_list]\r\n",
    "    for i in range(len(b)):\r\n",
    "        if math.isnan(b[i]):\r\n",
    "            b[i] = a[i]\r\n",
    "            \r\n",
    "    if len(b) < len(a):\r\n",
    "        len_b = len(b)\r\n",
    "        for i in range(len(a)-len(b)):\r\n",
    "            b.append(a[i+len_b])\r\n",
    "    elif len(a) < len(b):\r\n",
    "        len_a = len(a)\r\n",
    "        for i in range(len(b)-len(a)):\r\n",
    "            a.append(b[i+len_a])\r\n",
    "        \r\n",
    "    score_list = list(map(lambda x,y:x+y, a,b))\r\n",
    "    print('-'*50)\r\n",
    "    print(a)\r\n",
    "    print(b)\r\n",
    "    print('-'*50)\r\n",
    "    print(score_list)\r\n",
    "    \r\n",
    "    return title_list, link_list, score_list\r\n",
    "\r\n",
    "# ===========================================================================================================================\r\n",
    "# 연관키워드 추출\r\n",
    "\r\n",
    "def similar_keyword(word_):\r\n",
    "    similar_list = []\r\n",
    "\r\n",
    "    if len(word_) == 1:\r\n",
    "        similar_keywords = similar_model.wv.most_similar(word_[0])\r\n",
    "\r\n",
    "        for i in range(len(similar_keywords)):\r\n",
    "            similar_list.append(similar_keywords[i][0])\r\n",
    "    return similar_list\r\n",
    "\r\n",
    "\r\n",
    "# ===========================================================================================================================\r\n",
    "# MySQL 데이터베이스에 응답데이터, 시간 저장\r\n",
    "\r\n",
    "def MySQL_data_save(response):\r\n",
    "    global cursor, conn\r\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\r\n",
    "\r\n",
    "    now = datetime.datetime.now()\r\n",
    "    Korea_DatetimeNow = now + datetime.timedelta(hours = 9)\r\n",
    "    DatetimeNow = Korea_DatetimeNow.strftime('%Y-%m-%d %H:%M:%S')\r\n",
    "\r\n",
    "    try:\r\n",
    "        with conn.cursor() as cursor:\r\n",
    "            query = \"INSERT INTO factcheck.article_data (article_data, date) VALUES(%s, %s)\"\r\n",
    "            val = (response, DatetimeNow)\r\n",
    "            cursor.execute(query, val)\r\n",
    "            conn.commit()\r\n",
    "\r\n",
    "    finally:\r\n",
    "        conn.close()\r\n",
    "        \r\n",
    "        \r\n",
    "# ===========================================================================================================================\r\n",
    "# MySQL 데이터베이스에 키워드 랭킹 반영\r\n",
    "\r\n",
    "def keyword_rank(word_):\r\n",
    "    rank_list = []\r\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\r\n",
    "\r\n",
    "    try:\r\n",
    "        with conn.cursor() as cursor:\r\n",
    "            cursor.execute('SELECT keyword FROM keyword_rank')\r\n",
    "            row = cursor.fetchall()\r\n",
    "\r\n",
    "        for i in range(len(row)):\r\n",
    "            rank_list.append(row[i][0])\r\n",
    "\r\n",
    "        with conn.cursor() as cursor:\r\n",
    "            for i in range(0,1):\r\n",
    "                if word_[i] in rank_list:\r\n",
    "                    query = \"UPDATE keyword_rank SET count = count + 1 WHERE keyword = (%s)\"\r\n",
    "                    val = (word_[i])\r\n",
    "                    cursor.execute(query, val)\r\n",
    "                    conn.commit()\r\n",
    "                else:\r\n",
    "                    query = \"INSERT INTO factcheck.keyword_rank (keyword, count) VALUES(%s, 1)\"\r\n",
    "                    val = (word_[i])\r\n",
    "                    cursor.execute(query, val)\r\n",
    "                    conn.commit()\r\n",
    "\r\n",
    "        top10_list = []\r\n",
    "        with conn.cursor() as cursor:\r\n",
    "            cursor.execute(\"SELECT keyword FROM keyword_rank ORDER BY count DESC LIMIT 10\")\r\n",
    "            row_top = cursor.fetchall()\r\n",
    "            for i in range(len(row_top)):\r\n",
    "                top10_list.append(row_top[i][0])\r\n",
    "\r\n",
    "    finally:\r\n",
    "        conn.close()\r\n",
    "    return top10_list\r\n",
    "\r\n",
    "\r\n",
    "# ===========================================================================================================================\r\n",
    "# SNU 기사 검색      \r\n",
    "\r\n",
    "def SNU_Search(word_):\r\n",
    "    title_list2 = []\r\n",
    "    link_list2 = []\r\n",
    "    score_list2 = []\r\n",
    "\r\n",
    "    try:\r\n",
    "#             str_expr = \"document.str.contains('{}', case=False)\".format(word_[0]) and \"document.str.contains('{}', case=False)\".format(word_[1])\r\n",
    "        str_expr = \"document.str.contains('{}', case=False)\".format(word_[0])\r\n",
    "\r\n",
    "        df_q = train_df_All_.query(str_expr, engine=\"python\")\r\n",
    "\r\n",
    "        title = list(np.array(df_q['document'].tolist()))\r\n",
    "        score_SNU = list(np.array(df_q['label'].tolist()))\r\n",
    "        link = list(np.array(df_q['link'].tolist()))\r\n",
    "\r\n",
    "        for i in range(len(title)):\r\n",
    "            title_list2.append(title[i])\r\n",
    "            link_list2.append(link[i])\r\n",
    "            score_list2.append(score_SNU[i])\r\n",
    "\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "    return title_list2, score_list2\r\n",
    "\r\n",
    "\r\n",
    "# ===========================================================================================================================\r\n",
    "# 네이버 뉴스 검색\r\n",
    "\r\n",
    "def Naver_Search(response, tokenizer, SNU_model):\r\n",
    "    baseurl = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=\"\r\n",
    "\r\n",
    "    url = baseurl + urllib.parse.quote_plus(response)\r\n",
    "\r\n",
    "    html = urllib.request.urlopen(url).read()\r\n",
    "    soup = BeautifulSoup(html, 'html.parser')\r\n",
    "\r\n",
    "    title = soup.find_all(class_ = 'news_tit')\r\n",
    "\r\n",
    "    title_list = []\r\n",
    "    link_list = []\r\n",
    "\r\n",
    "    for i in title:\r\n",
    "        title_list.append(i.attrs['title'])\r\n",
    "        link_list.append(i.attrs['href'])\r\n",
    "\r\n",
    "# ================================================================\r\n",
    "\r\n",
    "    tokens_response = []\r\n",
    "    score_list = []\r\n",
    "\r\n",
    "    for i in range(len(title_list)):\r\n",
    "        tokens = my_Tokenizer(title_list[i])\r\n",
    "        tokens_response.append(tokens)\r\n",
    "\r\n",
    "    word_seq_response = tokenizer.texts_to_sequences(tokens_response)\r\n",
    "    word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\r\n",
    "\r\n",
    "    for sco in range(len(title_list)):\r\n",
    "        word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\r\n",
    "        score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\r\n",
    "    \r\n",
    "    return title_list, link_list, score_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Flask 웹 서버**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#-*-coding:utf-8-*-\r\n",
    "\r\n",
    "from flask import Flask, request\r\n",
    "import json\r\n",
    "from urllib import parse\r\n",
    "\r\n",
    "app = Flask(__name__)\r\n",
    "\r\n",
    "@app.route('/politics', methods = ['POST'])\r\n",
    "def Article_keyword_politics():\r\n",
    "    if request.method == 'POST':\r\n",
    "        params = request.get_data()\r\n",
    "        params_decode = str(params.decode('utf-8'))\r\n",
    "        urldecode = parse.unquote(params_decode)\r\n",
    "        \r\n",
    "        # 사용자 요청 데이터\r\n",
    "        response = urldecode.split('=')[1]\r\n",
    "        \r\n",
    "        word_ = tfidf_Vectorizer(response)\r\n",
    "        \r\n",
    "        similar_list = similar_keyword(word_)\r\n",
    "        MySQL_data_save(response)\r\n",
    "        top10_list = keyword_rank(word_)\r\n",
    "        \r\n",
    "        SNU_title, SNU_score = SNU_Search(word_)\r\n",
    "#         Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_pol, SNU_model_pol)\r\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Score(response, tokenizer_pol, tokenizer_Comments_pol, SNU_model_pol, Naver_Comments_pol)\r\n",
    "        \r\n",
    "        if len(similar_list) == 0:\r\n",
    "            similar_list.append('연관 검색어가 없습니다.')\r\n",
    "        if len(SNU_title) == 0:\r\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\r\n",
    "        if len(SNU_score) == 0:\r\n",
    "            SNU_score.append('x')\r\n",
    "        if len(Naver_title) == 0:\r\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\r\n",
    "        if len(Naver_link) == 0:\r\n",
    "            Naver_link.append('x')\r\n",
    "        if len(Naver_score) == 0:\r\n",
    "            Naver_score.append('x')\r\n",
    "            \r\n",
    "        data_ = {\r\n",
    "            'response':response,\r\n",
    "            'similar_list':similar_list,\r\n",
    "            'SNU_title':SNU_title,\r\n",
    "            'SNU_score':SNU_score,\r\n",
    "            'Naver_title':Naver_title,\r\n",
    "            'Naver_link':Naver_link,\r\n",
    "            'Naver_score':Naver_score,\r\n",
    "            'top10_list':top10_list\r\n",
    "            }\r\n",
    "        \r\n",
    "        return json.dumps(data_, ensure_ascii=False)\r\n",
    "    \r\n",
    "@app.route('/economy', methods = ['POST'])\r\n",
    "def Article_keyword_economy():\r\n",
    "    if request.method == 'POST':\r\n",
    "        params = request.get_data()\r\n",
    "        params_decode = str(params.decode('utf-8'))\r\n",
    "        urldecode = parse.unquote(params_decode)\r\n",
    "        \r\n",
    "        # 사용자 요청 데이터\r\n",
    "        response = urldecode.split('=')[1]\r\n",
    "        \r\n",
    "        word_ = tfidf_Vectorizer(response)\r\n",
    "        \r\n",
    "        similar_list = similar_keyword(word_)\r\n",
    "        MySQL_data_save(response)\r\n",
    "        top10_list = keyword_rank(word_)\r\n",
    "        \r\n",
    "        SNU_title, SNU_score = SNU_Search(word_)\r\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_eco, SNU_model_eco)\r\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Score(response, tokenizer_eco, tokenizer_Comments_eco, SNU_model_eco, Naver_Comments_eco)\r\n",
    "        \r\n",
    "        if len(similar_list) == 0:\r\n",
    "            similar_list.append('연관 검색어가 없습니다.')\r\n",
    "        if len(SNU_title) == 0:\r\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\r\n",
    "        if len(SNU_score) == 0:\r\n",
    "            SNU_score.append('x')\r\n",
    "        if len(Naver_title) == 0:\r\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\r\n",
    "        if len(Naver_link) == 0:\r\n",
    "            Naver_link.append('x')\r\n",
    "        if len(Naver_score) == 0:\r\n",
    "            Naver_score.append('x')\r\n",
    "            \r\n",
    "        data_ = {\r\n",
    "            'response':response,\r\n",
    "            'similar_list':similar_list,\r\n",
    "            'SNU_title':SNU_title,\r\n",
    "            'SNU_score':SNU_score,\r\n",
    "            'Naver_title':Naver_title,\r\n",
    "            'Naver_link':Naver_link,\r\n",
    "            'Naver_score':Naver_score,\r\n",
    "            'top10_list':top10_list\r\n",
    "            }\r\n",
    "        \r\n",
    "        return json.dumps(data_, ensure_ascii=False)\r\n",
    "    \r\n",
    "@app.route('/society', methods = ['POST'])\r\n",
    "def Article_keyword_society():\r\n",
    "    if request.method == 'POST':\r\n",
    "        params = request.get_data()\r\n",
    "        params_decode = str(params.decode('utf-8'))\r\n",
    "        urldecode = parse.unquote(params_decode)\r\n",
    "        \r\n",
    "        # 사용자 요청 데이터\r\n",
    "        response = urldecode.split('=')[1]\r\n",
    "        \r\n",
    "        word_ = tfidf_Vectorizer(response)\r\n",
    "        \r\n",
    "        similar_list = similar_keyword(word_)\r\n",
    "        MySQL_data_save(response)\r\n",
    "        top10_list = keyword_rank(word_)\r\n",
    "        \r\n",
    "        SNU_title, SNU_score = SNU_Search(word_)\r\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_soc, SNU_model_soc)\r\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Score(response, tokenizer_soc, tokenizer_Comments_soc, SNU_model_soc, Naver_Comments_soc)\r\n",
    "\r\n",
    "        if len(similar_list) == 0:\r\n",
    "            similar_list.append('연관 검색어가 없습니다.')\r\n",
    "        if len(SNU_title) == 0:\r\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\r\n",
    "        if len(SNU_score) == 0:\r\n",
    "            SNU_score.append('x')\r\n",
    "        if len(Naver_title) == 0:\r\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\r\n",
    "        if len(Naver_link) == 0:\r\n",
    "            Naver_link.append('x')\r\n",
    "        if len(Naver_score) == 0:\r\n",
    "            Naver_score.append('x')\r\n",
    "            \r\n",
    "        data_ = {\r\n",
    "            'response':response,\r\n",
    "            'similar_list':similar_list,\r\n",
    "            'SNU_title':SNU_title,\r\n",
    "            'SNU_score':SNU_score,\r\n",
    "            'Naver_title':Naver_title,\r\n",
    "            'Naver_link':Naver_link,\r\n",
    "            'Naver_score':Naver_score,\r\n",
    "            'top10_list':top10_list\r\n",
    "            }\r\n",
    "        \r\n",
    "        return json.dumps(data_, ensure_ascii=False)\r\n",
    "    \r\n",
    "@app.route('/others', methods = ['POST'])\r\n",
    "def Article_keyword_others():\r\n",
    "    if request.method == 'POST':\r\n",
    "        params = request.get_data()\r\n",
    "        params_decode = str(params.decode('utf-8'))\r\n",
    "        urldecode = parse.unquote(params_decode)\r\n",
    "        \r\n",
    "        # 사용자 요청 데이터\r\n",
    "        response = urldecode.split('=')[1]\r\n",
    "        \r\n",
    "        word_ = tfidf_Vectorizer(response)\r\n",
    "        \r\n",
    "        similar_list = similar_keyword(word_)\r\n",
    "        MySQL_data_save(response)\r\n",
    "        top10_list = keyword_rank(word_)\r\n",
    "        \r\n",
    "        SNU_title, SNU_score = SNU_Search(word_)\r\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_others, SNU_model_others)\r\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Score(response, tokenizer_others, tokenizer_Comments_others, SNU_model_others, Naver_Comments_others)\r\n",
    "\r\n",
    "        if len(similar_list) == 0:\r\n",
    "            similar_list.append('연관 검색어가 없습니다.')\r\n",
    "        if len(SNU_title) == 0:\r\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\r\n",
    "        if len(SNU_score) == 0:\r\n",
    "            SNU_score.append('x')\r\n",
    "        if len(Naver_title) == 0:\r\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\r\n",
    "        if len(Naver_link) == 0:\r\n",
    "            Naver_link.append('x')\r\n",
    "        if len(Naver_score) == 0:\r\n",
    "            Naver_score.append('x')\r\n",
    "            \r\n",
    "        data_ = {\r\n",
    "            'response':response,\r\n",
    "            'similar_list':similar_list,\r\n",
    "            'SNU_title':SNU_title,\r\n",
    "            'SNU_score':SNU_score,\r\n",
    "            'Naver_title':Naver_title,\r\n",
    "            'Naver_link':Naver_link,\r\n",
    "            'Naver_score':Naver_score,\r\n",
    "            'top10_list':top10_list\r\n",
    "            }\r\n",
    "        \r\n",
    "        return json.dumps(data_, ensure_ascii=False)\r\n",
    "\r\n",
    "@app.route('/kakao', methods = ['POST'])\r\n",
    "def kakao_():\r\n",
    "    if request.method == 'POST':\r\n",
    "        params = request.get_data()\r\n",
    "        params_decode = str(params.decode('utf-8'))\r\n",
    "        urldecode = parse.unquote(params_decode)\r\n",
    "        \r\n",
    "        # 사용자 요청 데이터\r\n",
    "        response = urldecode.split('=')[1]\r\n",
    "        \r\n",
    "        objectivity = kakao_data_score(response)\r\n",
    "        word_ = tfidf_Vectorizer(response)\r\n",
    "        \r\n",
    "        similar_list = similar_keyword(word_)\r\n",
    "        MySQL_data_save(response)\r\n",
    "        top10_list = keyword_rank(word_)\r\n",
    "        \r\n",
    "        SNU_title, SNU_score = SNU_Search(word_)\r\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_All, SNU_model_All)\r\n",
    "#         Naver_title, Naver_link, Naver_score = Naver_Score(response, tokenizer_All, tokenizer_Comments_All, SNU_model_All, Naver_Comments_All)\r\n",
    "\r\n",
    "        if len(similar_list) == 0:\r\n",
    "            similar_list.append('연관 검색어가 없습니다.')\r\n",
    "        if len(SNU_title) == 0:\r\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\r\n",
    "        if len(SNU_score) == 0:\r\n",
    "            SNU_score.append('x')\r\n",
    "        if len(Naver_title) == 0:\r\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\r\n",
    "        if len(Naver_link) == 0:\r\n",
    "            Naver_link.append('x')\r\n",
    "        if len(Naver_score) == 0:\r\n",
    "            Naver_score.append('x')\r\n",
    "        \r\n",
    "        data_ = {\r\n",
    "            'response':response,\r\n",
    "            'objectivity':objectivity,\r\n",
    "            'similar_list':similar_list,\r\n",
    "            'SNU_title':SNU_title,\r\n",
    "            'SNU_score':SNU_score,\r\n",
    "            'Naver_title':Naver_title,\r\n",
    "            'Naver_link':Naver_link,\r\n",
    "            'Naver_score':Naver_score,\r\n",
    "            'top10_list':top10_list\r\n",
    "            }\r\n",
    "        \r\n",
    "        print('입력 데이터 : \\n', response)\r\n",
    "        print('\\n')\r\n",
    "        print('객관성 level : \\n', objectivity)\r\n",
    "        print('\\n')\r\n",
    "        print('연관 검색어 : \\n', similar_list)\r\n",
    "        print('\\n')\r\n",
    "        print('네이버 뉴스 제목 : \\n', Naver_title)\r\n",
    "        print('\\n')\r\n",
    "        print('네이버 뉴스 링크 : \\n', Naver_link)\r\n",
    "        print('\\n')\r\n",
    "        print('네이버 뉴스 score : \\n', Naver_score)\r\n",
    "        print('\\n')\r\n",
    "        try:\r\n",
    "            print('SNU 기사 제목 : \\n', SNU_title)\r\n",
    "            print('\\n')\r\n",
    "    #         print('SNU 기사 링크 : ', link_list2)\r\n",
    "            print('SNU 기사 score : \\n', SNU_score)\r\n",
    "            print('\\n')\r\n",
    "        except:\r\n",
    "            print('SNU 기사가 없습니다. \\n')\r\n",
    "        print('인기키워드 랭킹 : \\n', top10_list)\r\n",
    "        print('\\n')\r\n",
    "        print('\\n')\r\n",
    "        \r\n",
    "        return json.dumps(data_, ensure_ascii=False)\r\n",
    "    \r\n",
    "    \r\n",
    "if __name__ == '__main__':\r\n",
    "    app.run(host='0.0.0.0', port = 8080)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\n",
      "125.133.174.241 - - [24/Sep/2021 11:31:32] \"\u001b[37mPOST /politics HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.18024177849292755, 0.6721988916397095, 0.36808374524116516, 0.47105246782302856]\n",
      "[nan, 0.41467059403657913]\n",
      "--------------------------------------------------\n",
      "[0.09012088924646378, 0.33609944581985474, 0.18404187262058258, 0.23552623391151428]\n",
      "[0.09012088924646378, 0.20733529701828957, 0.18404187262058258, 0.23552623391151428]\n",
      "--------------------------------------------------\n",
      "[0.18024177849292755, 0.5434347428381443, 0.36808374524116516, 0.47105246782302856]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
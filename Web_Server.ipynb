{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "welcome-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import logging\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from gensim.models import FastText\n",
    "from konlpy.tag import Okt\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "# SNU_model = load_model('SNU_LSTM_Model.h5')\n",
    "SNU_model = load_model('SNU_LSTM_Model_All.h5')\n",
    "kakao_model = load_model('kakao_LSTM_model.h5')\n",
    "# SNU_model_shuffle = load_model('SNU_LSTM_Model_shuffle.h5')\n",
    "similar_model = FastText.load('similar_keyword_model')\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        clean_words = []  # 정제된 단어 리스트\n",
    "        for word in pos:\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\n",
    "                    #if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\n",
    "                    clean_words.append(word[0])\n",
    "        return clean_words\n",
    "\n",
    "\n",
    "my_Tokenizer = MyTokenizer(Okt())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "legislative-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "host = '' # rds endpoint\n",
    "port = 3306\n",
    "username = ''\n",
    "database = ''\n",
    "password = ''\n",
    "\n",
    "def connect_RDS(host, port, username, password, database):\n",
    "    try:\n",
    "        conn = pymysql.connect(host = host, user = username, password = password, db = database, port = port, use_unicode = True, charset = 'utf8')\n",
    "        cursor = conn.cursor()\n",
    "    \n",
    "    except:\n",
    "        logging.error('RDS에 연결되지 않았습니다.')\n",
    "        sys.exit(1)\n",
    "        \n",
    "    return conn, cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "hidden-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.version_info[0] < 3:\n",
    "    from io import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = ''\n",
    "aws_secret = ''\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'snucsv'\n",
    "\n",
    "object_key = 'SNU_All_.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "# load data\n",
    "train_df_All_ = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train_All_ = train_df_All_[label_names].values\n",
    "\n",
    "train_df_All_['doc_len'] = train_df_All_['document'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df_All_['doc_len'].mean() + train_df_All_['doc_len'].std()).astype(int)\n",
    "\n",
    "#############################################################\n",
    "object_key = 'SNU_All_b.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "train_df_All = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train_All = train_df_All[label_names].values\n",
    "\n",
    "train_df_All['doc_len'] = train_df_All['document'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df_All['doc_len'].mean() + train_df_All['doc_len'].std()).astype(int)\n",
    "\n",
    "#############################################################\n",
    "object_key = 'SNU_경제_b.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "train_df_eco = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train_eco = train_df_eco[label_names].values\n",
    "\n",
    "train_df_eco['doc_len'] = train_df_eco['document'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df_eco['doc_len'].mean() + train_df_eco['doc_len'].std()).astype(int)\n",
    "\n",
    "#############################################################\n",
    "object_key = 'SNU_정치_b.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "train_df_pol = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train_pol = train_df_pol[label_names].values\n",
    "\n",
    "train_df_pol['doc_len'] = train_df_pol['document'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df_pol['doc_len'].mean() + train_df_pol['doc_len'].std()).astype(int)\n",
    "\n",
    "#############################################################\n",
    "object_key = 'SNU_사회_b.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "train_df_soc = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train_soc = train_df_soc[label_names].values\n",
    "\n",
    "train_df_soc['doc_len'] = train_df_soc['document'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df_soc['doc_len'].mean() + train_df_soc['doc_len'].std()).astype(int)\n",
    "\n",
    "#############################################################\n",
    "object_key = 'SNU_기타_b.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "train_df_other = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train_other = train_df_other[label_names].values\n",
    "\n",
    "train_df_other['doc_len'] = train_df_other['document'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df_other['doc_len'].mean() + train_df_other['doc_len'].std()).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-blake",
   "metadata": {},
   "source": [
    "**단어 벡터화 및 패딩**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "inside-walter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 79/1441 [00:00<00:01, 781.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1441/1441 [00:02<00:00, 602.60it/s]\n",
      "  0%|          | 0/509 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509/509 [00:00<00:00, 549.86it/s]\n",
      " 17%|█▋        | 82/490 [00:00<00:00, 815.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 490/490 [00:00<00:00, 560.49it/s]\n",
      " 39%|███▉      | 77/198 [00:00<00:00, 764.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [00:00<00:00, 519.97it/s]\n",
      " 34%|███▍      | 87/257 [00:00<00:00, 865.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [00:00<00:00, 586.25it/s]\n"
     ]
    }
   ],
   "source": [
    "def embedding_padding(train_df):\n",
    "    raw_docs_train = train_df['document'].tolist()\n",
    "    num_classes = len(label_names)\n",
    "    print(num_classes)\n",
    "    processed_docs_train = []\n",
    "\n",
    "    for doc in tqdm(raw_docs_train):\n",
    "        tokens = my_Tokenizer(doc)\n",
    "        processed_docs_train.append(tokens)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "    tokenizer.fit_on_texts(processed_docs_train)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer_All = embedding_padding(train_df_All)\n",
    "tokenizer_pol = embedding_padding(train_df_pol)\n",
    "tokenizer_soc = embedding_padding(train_df_soc)\n",
    "tokenizer_eco = embedding_padding(train_df_eco)\n",
    "tokenizer_other = embedding_padding(train_df_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "humanitarian-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_seq_train = tokenizer_All.texts_to_sequences(processed_docs_train)\n",
    "# word_index = tokenizer_All.word_index\n",
    "# print('dictionary size :', len(word_index))\n",
    "\n",
    "# word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "# word_seq_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "compressed-apache",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# 카카오 응답 데이터 전처리\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "text = 'do!!! you expect... people~ to~ read~ the FAQ, etc. and actually accept hard~! atheism?@@'\n",
    "re.sub(r'[^a-zA-Z ]', '', text) #알파벳과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")# 한글과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "okt = Okt()\n",
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)\n",
    "\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(X_train)\n",
    "\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer2.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "tokenizer_kakao = Tokenizer(vocab_size) \n",
    "tokenizer_kakao.fit_on_texts(X_train)\n",
    "X_train = tokenizer_kakao.texts_to_sequences(X_train)\n",
    "\n",
    "max_len = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-newport",
   "metadata": {},
   "source": [
    "**사용자에게 받은 데이터 전처리 테스트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "final-champagne",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['게르마늄', '팔찌', '과학', '효능', '있을까'], ['빌려', '쓰는', 'IT', '서비스', '클라우드', '컴퓨팅', '실제', '비용', '절감', '효과', '있나'], ['심장마비', '왔을', '강하게', '기침', '하면', '있다']]\n",
      "[[6113, 6114, 1641, 1609, 557], [6121, 759, 2358, 432, 6122, 6123, 439, 460, 3259, 33, 981], [3772, 3773, 3774, 3775, 16, 1]]\n",
      "(3, 10)\n",
      "[[   0    0    0    0    0 6113 6114 1641 1609  557]]\n",
      "[[ 759 2358  432 6122 6123  439  460 3259   33  981]]\n",
      "[[   0    0    0    0 3772 3773 3774 3775   16    1]]\n",
      "[0.23313859105110168, 0.5268844962120056, 0.443982869386673]\n"
     ]
    }
   ],
   "source": [
    "response = ['게르마늄 팔찌, 과학적으로 효능 있을까?', '빌려쓰는 IT서비스 클라우드컴퓨팅, 실제 비용절감 효과 있나', '\"심장마비 왔을 때 강하게 기침하면 막을 수 있다\"\t']\n",
    "tokens_response = []\n",
    "\n",
    "for i in range(len(response)):\n",
    "    tokens = my_Tokenizer(response[i])\n",
    "    tokens_response.append(tokens)\n",
    "print(tokens_response)\n",
    "\n",
    "word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "print(word_seq_response)\n",
    "word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "print(word_seq_response.shape)\n",
    "\n",
    "score_list = []\n",
    "for sco in range(len(word_seq_response)):\n",
    "    word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "    print(word_seq_response_to_score)\n",
    "    score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\n",
    "print(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-activation",
   "metadata": {},
   "source": [
    "**입력 데이터 처리 함수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "mental-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================================================================\n",
    "# 카카오톡 응답 데이터 score\n",
    "\n",
    "def kakao_data_score(response):\n",
    "    objectivity = ''\n",
    "    response = okt.morphs(response, stem=True) # 토큰화\n",
    "    response = [word for word in response if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer_kakao.texts_to_sequences([response]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(kakao_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.8 or score < 0.2):\n",
    "        objectivity = '객관성이 매우 낮습니다.'\n",
    "    elif(score > 0.65 or score < 0.35):\n",
    "        objectivity = '객관성이 낮습니다.'\n",
    "    else:\n",
    "        objectivity = '객관성이 높습니다.'\n",
    "    \n",
    "    return objectivity\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 단어 토큰화 및 빈도순 정렬 후 추출\n",
    "\n",
    "def tfidf_Vectorizer(response_list):\n",
    "    tfidf_Vectorizer = TfidfVectorizer(tokenizer=my_Tokenizer, min_df=1) # df 값(단어가 몇 문장들에서 등장하였는지)을 최소 'min_df' 값으로 설정.\n",
    "    X = tfidf_Vectorizer.fit_transform(response_list).toarray()\n",
    "    print(X.shape)    # X(2차원 배열)의 행,열 수를 출력.\n",
    "    print(tfidf_Vectorizer.vocabulary_)   # 각 단어들이 배열 X에서 몇번째 열(인덱스 값)에 해당하는지 출력.\n",
    "\n",
    "\n",
    "    #pandas를 활용하여 각 단어들의 각 문장에서의 tf-idf 값들을 모두 더하고, 내림차순으로 정렬하여 상위 n개 출력\n",
    "    count = X.sum(axis=0)    # 2차원 배열 X에서 각 열을 기준으로 합을 구함. (각 단어들의 '최종' tf-idf 값으로 간주.)\n",
    "    word_count = pd.DataFrame({\n",
    "        '단어' : tfidf_Vectorizer.get_feature_names(),\n",
    "        '빈도' : count.flat\n",
    "    })\n",
    "    sorted_df = word_count.sort_values('빈도', ascending=False)\n",
    "    print(sorted_df.head(10), \"\\n\")\n",
    "\n",
    "    word_ = list(np.array(sorted_df['단어'].tolist()))\n",
    "    return(word_)\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 연관키워드 추출\n",
    "\n",
    "def similar_keyword(word_):\n",
    "    similar_list = []\n",
    "\n",
    "    if len(word_) == 1:\n",
    "        similar_keywords = similar_model.wv.most_similar(word_[0])\n",
    "\n",
    "        for i in range(len(similar_keywords)):\n",
    "            similar_list.append(similar_keywords[i][0])\n",
    "    return similar_list\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# MySQL 데이터베이스에 응답데이터, 시간 저장\n",
    "\n",
    "def MySQL_data_save(response):\n",
    "    global cursor, conn\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    Korea_DatetimeNow = now + datetime.timedelta(hours = 9)\n",
    "    DatetimeNow = Korea_DatetimeNow.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            query = \"INSERT INTO factcheck.article_data (article_data, date) VALUES(%s, %s)\"\n",
    "            val = (response, DatetimeNow)\n",
    "            cursor.execute(query, val)\n",
    "            conn.commit()\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "        \n",
    "        \n",
    "# ===========================================================================================================================\n",
    "# MySQL 데이터베이스에 키워드 랭킹 반영\n",
    "\n",
    "def keyword_rank(word_):\n",
    "    rank_list = []\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute('SELECT keyword FROM keyword_rank')\n",
    "            row = cursor.fetchall()\n",
    "\n",
    "        for i in range(len(row)):\n",
    "            rank_list.append(row[i][0])\n",
    "\n",
    "        with conn.cursor() as cursor:\n",
    "            for i in range(0,1):\n",
    "                if word_[i] in rank_list:\n",
    "                    query = \"UPDATE keyword_rank SET count = count + 1 WHERE keyword = (%s)\"\n",
    "                    val = (word_[i])\n",
    "                    cursor.execute(query, val)\n",
    "                    conn.commit()\n",
    "                else:\n",
    "                    query = \"INSERT INTO factcheck.keyword_rank (keyword, count) VALUES(%s, 1)\"\n",
    "                    val = (word_[i])\n",
    "                    cursor.execute(query, val)\n",
    "                    conn.commit()\n",
    "\n",
    "        top10_list = []\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT keyword FROM keyword_rank ORDER BY count DESC LIMIT 10\")\n",
    "            row_top = cursor.fetchall()\n",
    "            for i in range(len(row_top)):\n",
    "                top10_list.append(row_top[i][0])\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "    return top10_list\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# SNU 기사 검색      \n",
    "\n",
    "def SNU_Search(word_):\n",
    "    title_list2 = []\n",
    "    # link_list2 = []\n",
    "    score_list2 = []\n",
    "\n",
    "    try:\n",
    "#             str_expr = \"document.str.contains('{}', case=False)\".format(word_[0]) and \"document.str.contains('{}', case=False)\".format(word_[1])\n",
    "        str_expr = \"document.str.contains('{}', case=False)\".format(word_[0])\n",
    "\n",
    "        df_q = train_df_All_.query(str_expr, engine=\"python\")\n",
    "\n",
    "        title = list(np.array(df_q['document'].tolist()))\n",
    "        score_SNU = list(np.array(df_q['label'].tolist()))\n",
    "        # link = list(np.array(df_q['link'].tolist()))\n",
    "\n",
    "        for i in range(len(title)):\n",
    "            title_list2.append(title[i])\n",
    "        #     link_list2.append(link[i])\n",
    "            score_list2.append(score_SNU[i])\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    return title_list2, score_list2\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 네이버 뉴스 검색\n",
    "\n",
    "def Naver_Search(response, tokenizer):\n",
    "    baseurl = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=\"\n",
    "\n",
    "    url = baseurl + urllib.parse.quote_plus(response)\n",
    "\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    title = soup.find_all(class_ = 'news_tit')\n",
    "\n",
    "    title_list = []\n",
    "    link_list = []\n",
    "\n",
    "    for i in title:\n",
    "        title_list.append(i.attrs['title'])\n",
    "        link_list.append(i.attrs['href'])\n",
    "\n",
    "# ================================================================\n",
    "\n",
    "    tokens_response = []\n",
    "    score_list = []\n",
    "\n",
    "    for i in range(len(title_list)):\n",
    "        tokens = my_Tokenizer(title_list[i])\n",
    "        tokens_response.append(tokens)\n",
    "\n",
    "    word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "    word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "\n",
    "    for sco in range(len(title_list)):\n",
    "        word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "        score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\n",
    "    \n",
    "    return title_list, link_list, score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-keyboard",
   "metadata": {},
   "source": [
    "**Flask 웹 서버**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-looking",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "{'asdadad': 0}\n",
      "        단어   빈도\n",
      "0  asdadad  1.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [15/Sep/2021 15:27:45] \"\u001b[37mPOST /society HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "{'asdadad': 0}\n",
      "        단어   빈도\n",
      "0  asdadad  1.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [15/Sep/2021 15:27:48] \"\u001b[37mPOST /economy HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "{'asdadad': 0}\n",
      "        단어   빈도\n",
      "0  asdadad  1.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [15/Sep/2021 15:27:50] \"\u001b[37mPOST /politics HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "{'asdadad': 0}\n",
      "        단어   빈도\n",
      "0  asdadad  1.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [15/Sep/2021 15:27:52] \"\u001b[37mPOST /others HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "{'asdadad': 0}\n",
      "        단어   빈도\n",
      "0  asdadad  1.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [15/Sep/2021 15:27:55] \"\u001b[37mPOST /economy HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "{'asdadad': 0}\n",
      "        단어   빈도\n",
      "0  asdadad  1.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [15/Sep/2021 15:27:57] \"\u001b[37mPOST /society HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "{'asdadad': 0}\n",
      "        단어   빈도\n",
      "0  asdadad  1.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [15/Sep/2021 15:28:03] \"\u001b[37mPOST /kakao HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터 : \n",
      " asdadad\n",
      "\n",
      "\n",
      "객관성 level : \n",
      " 객관성이 높습니다.\n",
      "\n",
      "\n",
      "연관 검색어 : \n",
      " ['구', '재방', '재방송', '티비', '방송', '어제', '지상파', '케이블', '무도', 'ㅜㅜㅜ']\n",
      "\n",
      "\n",
      "네이버 뉴스 제목 : \n",
      " ['관련 뉴스 결과가 없습니다.']\n",
      "\n",
      "\n",
      "네이버 뉴스 링크 : \n",
      " ['x']\n",
      "\n",
      "\n",
      "네이버 뉴스 score : \n",
      " ['x']\n",
      "\n",
      "\n",
      "SNU 기사 제목 : \n",
      " ['관련 뉴스 결과가 없습니다.']\n",
      "\n",
      "\n",
      "SNU 기사 score : \n",
      " ['x']\n",
      "\n",
      "\n",
      "인기키워드 랭킹 : \n",
      " ['안녕하세요', '유료', '카카오', '25', '코로나', '123123', '카카오톡', 'nameasdfasdfas', '입니다', 'dcxzvdszvzcd']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(1, 2)\n",
      "{'안녕하세요': 0, '오소리': 1}\n",
      "      단어        빈도\n",
      "0  안녕하세요  0.707107\n",
      "1    오소리  0.707107 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [15/Sep/2021 15:34:35] \"\u001b[37mPOST /politics HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "{'안녕하세요': 0, '오소리': 1}\n",
      "      단어        빈도\n",
      "0  안녕하세요  0.707107\n",
      "1    오소리  0.707107 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [15/Sep/2021 15:34:39] \"\u001b[37mPOST /economy HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "{'안녕하세요': 0, '오소리': 1}\n",
      "      단어        빈도\n",
      "0  안녕하세요  0.707107\n",
      "1    오소리  0.707107 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [15/Sep/2021 15:34:42] \"\u001b[37mPOST /society HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "{'안녕하세요': 0, '오소리': 1}\n",
      "      단어        빈도\n",
      "0  안녕하세요  0.707107\n",
      "1    오소리  0.707107 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [15/Sep/2021 15:34:44] \"\u001b[37mPOST /others HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "{'안녕하세요': 0, '오소리': 1}\n",
      "      단어        빈도\n",
      "0  안녕하세요  0.707107\n",
      "1    오소리  0.707107 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [15/Sep/2021 15:34:52] \"\u001b[37mPOST /kakao HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터 : \n",
      " 안녕하세요 오소리에요\n",
      "\n",
      "\n",
      "객관성 level : \n",
      " 객관성이 높습니다.\n",
      "\n",
      "\n",
      "연관 검색어 : \n",
      " ['연관 검색어가 없습니다.']\n",
      "\n",
      "\n",
      "네이버 뉴스 제목 : \n",
      " ['독수리술·오소리술 들어는 보셨습니까? 전설적인 평안도 사냥꾼의 음식', '[어제보다 행복하기]\\xa0비움의 행복', '뮤지션 #천용성 의 두 번째 목소리', '주문을 걸어 (원슈타인 화보 & 인터뷰)', \"'가상화폐, 문제는 다 文 정부 때문'이라는 언론\"]\n",
      "\n",
      "\n",
      "네이버 뉴스 링크 : \n",
      " ['http://www.nocutnews.co.kr/news/4655459', 'http://www.newscj.com/news/articleView.html?idxno=788375', 'http://www.elle.co.kr/article/56416', 'http://www.wkorea.com/2020/12/24/%ec%a3%bc%eb%ac%b8%ec%9d%84-%ea%b1%b8%ec%96%b4-%ec%9b%90%ec%8a%88%ed%83%80%ec%9d%b8/?utm_source=naver&utm_medium=partnership', 'http://www.nocutnews.co.kr/news/4910068']\n",
      "\n",
      "\n",
      "네이버 뉴스 score : \n",
      " [0.48435699939727783, 0.48435699939727783, 0.48435699939727783, 0.2658138573169708, 0.5061073303222656]\n",
      "\n",
      "\n",
      "SNU 기사 제목 : \n",
      " ['관련 뉴스 결과가 없습니다.']\n",
      "\n",
      "\n",
      "SNU 기사 score : \n",
      " ['x']\n",
      "\n",
      "\n",
      "인기키워드 랭킹 : \n",
      " ['안녕하세요', '유료', '카카오', '25', '코로나', '123123', '카카오톡', 'nameasdfasdfas', '입니다', 'dcxzvdszvzcd']\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "\n",
    "from flask import Flask, request\n",
    "import json\n",
    "from urllib import parse\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/politics', methods = ['POST'])\n",
    "def Article_keyword_politics():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        \n",
    "        word_ = tfidf_Vectorizer(response_list)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_pol)\n",
    "        \n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "            \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "    \n",
    "@app.route('/economy', methods = ['POST'])\n",
    "def Article_keyword_economy():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        \n",
    "        word_ = tfidf_Vectorizer(response_list)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_eco)\n",
    "        \n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "            \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "    \n",
    "@app.route('/society', methods = ['POST'])\n",
    "def Article_keyword_society():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        \n",
    "        word_ = tfidf_Vectorizer(response_list)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_soc)\n",
    "        \n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "            \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "    \n",
    "@app.route('/others', methods = ['POST'])\n",
    "def Article_keyword_others():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        \n",
    "        word_ = tfidf_Vectorizer(response_list)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_other)\n",
    "        \n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "            \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "\n",
    "@app.route('/kakao', methods = ['POST'])\n",
    "def kakao_():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        \n",
    "        objectivity = kakao_data_score(response)\n",
    "        word_ = tfidf_Vectorizer(response_list)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_All)\n",
    "        \n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "        \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'objectivity':objectivity,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        print('입력 데이터 : \\n', response)\n",
    "        print('\\n')\n",
    "        print('객관성 level : \\n', objectivity)\n",
    "        print('\\n')\n",
    "        print('연관 검색어 : \\n', similar_list)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 제목 : \\n', Naver_title)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 링크 : \\n', Naver_link)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 score : \\n', Naver_score)\n",
    "        print('\\n')\n",
    "        try:\n",
    "            print('SNU 기사 제목 : \\n', SNU_title)\n",
    "            print('\\n')\n",
    "    #         print('SNU 기사 링크 : ', link_list2)\n",
    "            print('SNU 기사 score : \\n', SNU_score)\n",
    "            print('\\n')\n",
    "        except:\n",
    "            print('SNU 기사가 없습니다. \\n')\n",
    "        print('인기키워드 랭킹 : \\n', top10_list)\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port = 8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-sullivan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "welcome-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import logging\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from gensim.models import FastText\n",
    "from konlpy.tag import Okt\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "SNU_model = load_model('SNU_LSTM_Model.h5')\n",
    "# SNU_model_shuffle = load_model('SNU_LSTM_Model_shuffle.h5')\n",
    "similar_model = FastText.load('similar_keyword_model')\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        clean_words = []  # 정제된 단어 리스트\n",
    "        for word in pos:\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\n",
    "                    #if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\n",
    "                    clean_words.append(word[0])\n",
    "        return clean_words\n",
    "\n",
    "\n",
    "my_Tokenizer = MyTokenizer(Okt())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "legislative-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "host = '' # rds endpoint\n",
    "port = 3306\n",
    "username = ''\n",
    "database = ''\n",
    "password = ''\n",
    "\n",
    "def connect_RDS(host, port, username, password, database):\n",
    "    try:\n",
    "        conn = pymysql.connect(host = host, user = username, password = password, db = database, port = port, use_unicode = True, charset = 'utf8')\n",
    "        cursor = conn.cursor()\n",
    "    \n",
    "    except:\n",
    "        logging.error('RDS에 연결되지 않았습니다.')\n",
    "        sys.exit(1)\n",
    "        \n",
    "    return conn, cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hidden-tourist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘정치 세대교체’ 바람 일으킨 이준석, 대선엔 출마 못한다</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>액상형 전자담배는 니코틴이 함유되지 않아 담배가 아니다</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>한국 백신 접종률 세계 100위권 이하다</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>광역도시 없는 지역권은 실제 수요와 관계없이 획일적으로 국가교통망 정책에서 소외되고 있다</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>\" ‘당·정·청 전원회의’는 '운동권 용어'다\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>\"늘어가는 해양사고, 전북 관내 잠수구조인력 전무(全無)\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>\"청와대 하루 평균 업무추진비, 이명박 768만원, 박근혜 814만원, 문재인 55만원\"</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>국민연금공단 기금운용본부가 전주로 이전한 후, 해외투자자가 국민연금을 '패싱'한다</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>문재인 대통령이 미국 방문에서 푸대접 받았다</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1224 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               document  label  doc_len\n",
       "0                  현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다      1        9\n",
       "1                      ‘정치 세대교체’ 바람 일으킨 이준석, 대선엔 출마 못한다      1        8\n",
       "2                        액상형 전자담배는 니코틴이 함유되지 않아 담배가 아니다      1        7\n",
       "3                                한국 백신 접종률 세계 100위권 이하다      1        6\n",
       "4     광역도시 없는 지역권은 실제 수요와 관계없이 획일적으로 국가교통망 정책에서 소외되고 있다      1       11\n",
       "...                                                 ...    ...      ...\n",
       "1219                        \" ‘당·정·청 전원회의’는 '운동권 용어'다\"?      0        5\n",
       "1220                  \"늘어가는 해양사고, 전북 관내 잠수구조인력 전무(全無)\"?      0        6\n",
       "1221  \"청와대 하루 평균 업무추진비, 이명박 768만원, 박근혜 814만원, 문재인 55만원\"      0       10\n",
       "1222      국민연금공단 기금운용본부가 전주로 이전한 후, 해외투자자가 국민연금을 '패싱'한다      0        8\n",
       "1223                           문재인 대통령이 미국 방문에서 푸대접 받았다      0        6\n",
       "\n",
       "[1224 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if sys.version_info[0] < 3:\n",
    "    from io import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = ''\n",
    "aws_secret = ''\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'snucsv'\n",
    "\n",
    "object_key = 'SNU_Data_1200.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "# load data\n",
    "train_df = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train = train_df[label_names].values\n",
    "\n",
    "train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-blake",
   "metadata": {},
   "source": [
    "**단어 벡터화 및 패딩**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inside-walter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1224 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1224/1224 [00:09<00:00, 127.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size : 3999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 503, 1459,  504, ..., 1461, 1462,    8],\n",
       "       [   0,   72,  761, ...,   51,  360,    5],\n",
       "       [   0,    0, 1466, ...,  506,  763,   41],\n",
       "       ...,\n",
       "       [ 649,  177,  381, ..., 3992,   13, 3993],\n",
       "       [3994, 3995, 3996, ...,  248, 3998,    5],\n",
       "       [   0,    0,    0, ..., 1401, 3999,  496]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs_train = train_df['document'].tolist()\n",
    "num_classes = len(label_names)\n",
    "print(num_classes)\n",
    "processed_docs_train = []\n",
    "\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = my_Tokenizer(doc)\n",
    "    processed_docs_train.append(tokens)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs_train)\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('dictionary size :', len(word_index))\n",
    "\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-newport",
   "metadata": {},
   "source": [
    "**사용자에게 받은 데이터 전처리 테스트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "final-champagne",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[503, 1459, 504, 1460, 22, 71, 104, 1461, 1462, 8]\n",
      "(3, 10)\n",
      "[[ 503 1459  504 1460   22   71  104 1461 1462    8]]\n",
      "[[0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 0 0]]\n",
      "[0.5054650902748108, 0.5058199763298035, 0.5058199763298035]\n"
     ]
    }
   ],
   "source": [
    "response = ['현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다', 'ajsdlkfakljflalejlka', 'dshkjlafjsdkfjalks']\n",
    "tokens_response = []\n",
    "\n",
    "for i in range(len(response)):\n",
    "    tokens = my_Tokenizer(response[i])\n",
    "    tokens_response.append(tokens)\n",
    "#     print(tokens_response)\n",
    "\n",
    "word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "print(word_seq_response[0])\n",
    "word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "print(word_seq_response.shape)\n",
    "\n",
    "score_list = []\n",
    "for sco in range(len(word_seq_response)):\n",
    "    word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "    print(word_seq_response_to_score)\n",
    "    score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\n",
    "print(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-architect",
   "metadata": {},
   "source": [
    "**입력 데이터 처리 함수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mental-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================================================================\n",
    "# 단어 토큰화 및 빈도순 정렬 후 추출\n",
    "\n",
    "def tfidf_Vectorizer(response_list):\n",
    "    tfidf_Vectorizer = TfidfVectorizer(tokenizer=my_Tokenizer, min_df=1) # df 값(단어가 몇 문장들에서 등장하였는지)을 최소 'min_df' 값으로 설정.\n",
    "    X = tfidf_Vectorizer.fit_transform(response_list).toarray()\n",
    "    print(X.shape)    # X(2차원 배열)의 행,열 수를 출력.\n",
    "    print(tfidf_Vectorizer.vocabulary_)   # 각 단어들이 배열 X에서 몇번째 열(인덱스 값)에 해당하는지 출력.\n",
    "\n",
    "\n",
    "    #pandas를 활용하여 각 단어들의 각 문장에서의 tf-idf 값들을 모두 더하고, 내림차순으로 정렬하여 상위 n개 출력\n",
    "    count = X.sum(axis=0)    # 2차원 배열 X에서 각 열을 기준으로 합을 구함. (각 단어들의 '최종' tf-idf 값으로 간주.)\n",
    "    word_count = pd.DataFrame({\n",
    "        '단어' : tfidf_Vectorizer.get_feature_names(),\n",
    "        '빈도' : count.flat\n",
    "    })\n",
    "    sorted_df = word_count.sort_values('빈도', ascending=False)\n",
    "    print(sorted_df.head(10), \"\\n\")\n",
    "\n",
    "    word_ = list(np.array(sorted_df['단어'].tolist()))\n",
    "    return(word_)\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 연관키워드 추출\n",
    "\n",
    "def similar_keyword(word_):\n",
    "    similar_list = []\n",
    "\n",
    "    if len(word_) == 1:\n",
    "        similar_keywords = similar_model.wv.most_similar(word_[0])\n",
    "\n",
    "        for i in range(len(similar_keywords)):\n",
    "            similar_list.append(similar_keywords[i][0])\n",
    "    return similar_list\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# MySQL 데이터베이스에 응답데이터, 시간 저장\n",
    "\n",
    "def MySQL_data_save(response):\n",
    "    global cursor, conn\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    Korea_DatetimeNow = now + datetime.timedelta(hours = 9)\n",
    "    DatetimeNow = Korea_DatetimeNow.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            query = \"INSERT INTO factcheck.article_data (article_data, date) VALUES(%s, %s)\"\n",
    "            val = (response, DatetimeNow)\n",
    "            cursor.execute(query, val)\n",
    "            conn.commit()\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "        \n",
    "        \n",
    "# ===========================================================================================================================\n",
    "# MySQL 데이터베이스에 키워드 랭킹 반영\n",
    "\n",
    "def keyword_rank(word_):\n",
    "    rank_list = []\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute('SELECT keyword FROM keyword_rank')\n",
    "            row = cursor.fetchall()\n",
    "\n",
    "        for i in range(len(row)):\n",
    "            rank_list.append(row[i][0])\n",
    "\n",
    "        with conn.cursor() as cursor:\n",
    "            for i in range(0,1):\n",
    "                if word_[i] in rank_list:\n",
    "                    query = \"UPDATE keyword_rank SET count = count + 1 WHERE keyword = (%s)\"\n",
    "                    val = (word_[i])\n",
    "                    cursor.execute(query, val)\n",
    "                    conn.commit()\n",
    "                else:\n",
    "                    query = \"INSERT INTO factcheck.keyword_rank (keyword, count) VALUES(%s, 1)\"\n",
    "                    val = (word_[i])\n",
    "                    cursor.execute(query, val)\n",
    "                    conn.commit()\n",
    "\n",
    "        top10_list = []\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT keyword FROM keyword_rank ORDER BY count DESC LIMIT 10\")\n",
    "            row_top = cursor.fetchall()\n",
    "            for i in range(len(row_top)):\n",
    "                top10_list.append(row_top[i][0])\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "    return top10_list\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# SNU 기사 검색      \n",
    "\n",
    "def SNU_Search(word_):\n",
    "    title_list2 = []\n",
    "    # link_list2 = []\n",
    "    score_list2 = []\n",
    "\n",
    "    try:\n",
    "#             str_expr = \"document.str.contains('{}', case=False)\".format(word_[0]) and \"document.str.contains('{}', case=False)\".format(word_[1])\n",
    "        str_expr = \"document.str.contains('{}', case=False)\".format(word_[0])\n",
    "\n",
    "        df_q = train_df.query(str_expr, engine=\"python\")\n",
    "\n",
    "        title = list(np.array(df_q['document'].tolist()))\n",
    "        score_SNU = list(np.array(df_q['label'].tolist()))\n",
    "        # link = list(np.array(df_q['link'].tolist()))\n",
    "\n",
    "        for i in range(len(title)):\n",
    "            title_list2.append(title[i])\n",
    "        #     link_list2.append(link[i])\n",
    "            score_list2.append(score_SNU[i])\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    return title_list2, score_list2\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 네이버 뉴스 검색\n",
    "\n",
    "def Naver_Search(response):\n",
    "    baseurl = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=\"\n",
    "\n",
    "    url = baseurl + urllib.parse.quote_plus(response)\n",
    "\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    title = soup.find_all(class_ = 'news_tit')\n",
    "\n",
    "    title_list = []\n",
    "    link_list = []\n",
    "\n",
    "    for i in title:\n",
    "        title_list.append(i.attrs['title'])\n",
    "        link_list.append(i.attrs['href'])\n",
    "\n",
    "# ================================================================\n",
    "\n",
    "    tokens_response = []\n",
    "    tokens_response2 = []\n",
    "    score_list = []\n",
    "\n",
    "    for i in range(len(title_list)):\n",
    "        tokens = my_Tokenizer(title_list[i])\n",
    "        tokens_response.append(tokens)\n",
    "\n",
    "    word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "    word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "\n",
    "    for sco in range(len(title_list)):\n",
    "        word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "        score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\n",
    "    \n",
    "    return title_list, link_list, score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-biodiversity",
   "metadata": {},
   "source": [
    "**Flask 웹 서버**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "responsible-anniversary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "{'vdsadzdzcsda': 0}\n",
      "             단어   빈도\n",
      "0  vdsadzdzcsda  1.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [09/Sep/2021 08:57:18] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터 : \n",
      " vdsadzdzcsda\n",
      "\n",
      "\n",
      "연관 검색어 : \n",
      " ['재밋다', '밋다', '웃기', '후지', '잼', '재밋음', '재밌다', '재미없다', '재밋습', 'ㅛ']\n",
      "\n",
      "\n",
      "네이버 뉴스 제목 : \n",
      " []\n",
      "\n",
      "\n",
      "네이버 뉴스 링크 : \n",
      " []\n",
      "\n",
      "\n",
      "네이버 뉴스 score : \n",
      " []\n",
      "\n",
      "\n",
      "SNU 기사 제목 : \n",
      " []\n",
      "\n",
      "\n",
      "SNU 기사 score : \n",
      " []\n",
      "\n",
      "\n",
      "인기키워드 랭킹 : \n",
      " ['안녕하세요', '유료', '카카오', '25', '코로나', '123123', '카카오톡', 'nameasdfasdfas', '카카오톡 유료화', 'afsdfafdads']\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "\n",
    "from flask import Flask, request\n",
    "import json\n",
    "from urllib import parse\n",
    "\n",
    "app = Flask(__name__)\n",
    "@app.route('/', methods = ['POST'])\n",
    "\n",
    "def hello():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        \n",
    "        word_ = tfidf_Vectorizer(response_list)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response)\n",
    "        \n",
    "\n",
    "        \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'Naver_title':Naver_title\n",
    "            }\n",
    "        \n",
    "        print('입력 데이터 : \\n', response)\n",
    "        print('\\n')\n",
    "        print('연관 검색어 : \\n', similar_list)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 제목 : \\n', Naver_title)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 링크 : \\n', Naver_link)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 score : \\n', Naver_score)\n",
    "        print('\\n')\n",
    "        try:\n",
    "            print('SNU 기사 제목 : \\n', SNU_title)\n",
    "            print('\\n')\n",
    "    #         print('SNU 기사 링크 : ', link_list2)\n",
    "            print('SNU 기사 score : \\n', SNU_score)\n",
    "            print('\\n')\n",
    "        except:\n",
    "            print('SNU 기사가 없습니다. \\n')\n",
    "        print('인기키워드 랭킹 : \\n', top10_list)\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port = 8080)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

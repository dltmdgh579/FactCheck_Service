{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "welcome-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import logging\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from gensim.models import FastText\n",
    "from konlpy.tag import Okt\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "SNU_model = load_model('SNU_LSTM_Model.h5')\n",
    "kakao_model = load_model('kakao_LSTM_model.h5')\n",
    "# SNU_model_shuffle = load_model('SNU_LSTM_Model_shuffle.h5')\n",
    "similar_model = FastText.load('similar_keyword_model')\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        clean_words = []  # 정제된 단어 리스트\n",
    "        for word in pos:\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\n",
    "                    #if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\n",
    "                    clean_words.append(word[0])\n",
    "        return clean_words\n",
    "\n",
    "\n",
    "my_Tokenizer = MyTokenizer(Okt())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "legislative-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "host = '' # rds endpoint\n",
    "port = 3306\n",
    "username = ''\n",
    "database = ''\n",
    "password = ''\n",
    "\n",
    "def connect_RDS(host, port, username, password, database):\n",
    "    try:\n",
    "        conn = pymysql.connect(host = host, user = username, password = password, db = database, port = port, use_unicode = True, charset = 'utf8')\n",
    "        cursor = conn.cursor()\n",
    "    \n",
    "    except:\n",
    "        logging.error('RDS에 연결되지 않았습니다.')\n",
    "        sys.exit(1)\n",
    "        \n",
    "    return conn, cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "hidden-tourist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘정치 세대교체’ 바람 일으킨 이준석, 대선엔 출마 못한다</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>액상형 전자담배는 니코틴이 함유되지 않아 담배가 아니다</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>한국 백신 접종률 세계 100위권 이하다</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>광역도시 없는 지역권은 실제 수요와 관계없이 획일적으로 국가교통망 정책에서 소외되고 있다</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>\" ‘당·정·청 전원회의’는 '운동권 용어'다\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>\"늘어가는 해양사고, 전북 관내 잠수구조인력 전무(全無)\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>\"청와대 하루 평균 업무추진비, 이명박 768만원, 박근혜 814만원, 문재인 55만원\"</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>국민연금공단 기금운용본부가 전주로 이전한 후, 해외투자자가 국민연금을 '패싱'한다</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>문재인 대통령이 미국 방문에서 푸대접 받았다</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1224 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               document  label  doc_len\n",
       "0                  현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다      1        9\n",
       "1                      ‘정치 세대교체’ 바람 일으킨 이준석, 대선엔 출마 못한다      1        8\n",
       "2                        액상형 전자담배는 니코틴이 함유되지 않아 담배가 아니다      1        7\n",
       "3                                한국 백신 접종률 세계 100위권 이하다      1        6\n",
       "4     광역도시 없는 지역권은 실제 수요와 관계없이 획일적으로 국가교통망 정책에서 소외되고 있다      1       11\n",
       "...                                                 ...    ...      ...\n",
       "1219                        \" ‘당·정·청 전원회의’는 '운동권 용어'다\"?      0        5\n",
       "1220                  \"늘어가는 해양사고, 전북 관내 잠수구조인력 전무(全無)\"?      0        6\n",
       "1221  \"청와대 하루 평균 업무추진비, 이명박 768만원, 박근혜 814만원, 문재인 55만원\"      0       10\n",
       "1222      국민연금공단 기금운용본부가 전주로 이전한 후, 해외투자자가 국민연금을 '패싱'한다      0        8\n",
       "1223                           문재인 대통령이 미국 방문에서 푸대접 받았다      0        6\n",
       "\n",
       "[1224 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if sys.version_info[0] < 3:\n",
    "    from io import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = ''\n",
    "aws_secret = ''\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'snucsv'\n",
    "\n",
    "object_key = 'SNU_Data_1200.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "# load data\n",
    "train_df = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train = train_df[label_names].values\n",
    "\n",
    "train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-blake",
   "metadata": {},
   "source": [
    "**단어 벡터화 및 패딩**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "inside-walter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 78/1224 [00:00<00:01, 779.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1224/1224 [00:04<00:00, 293.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size : 3999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 503, 1459,  504, ..., 1461, 1462,    8],\n",
       "       [   0,   72,  761, ...,   51,  360,    5],\n",
       "       [   0,    0, 1466, ...,  506,  763,   41],\n",
       "       ...,\n",
       "       [ 649,  177,  381, ..., 3992,   13, 3993],\n",
       "       [3994, 3995, 3996, ...,  248, 3998,    5],\n",
       "       [   0,    0,    0, ..., 1401, 3999,  496]], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs_train = train_df['document'].tolist()\n",
    "num_classes = len(label_names)\n",
    "print(num_classes)\n",
    "processed_docs_train = []\n",
    "\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = my_Tokenizer(doc)\n",
    "    processed_docs_train.append(tokens)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs_train)\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('dictionary size :', len(word_index))\n",
    "\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "compressed-apache",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# 카카오 응답 데이터 전처리\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "text = 'do!!! you expect... people~ to~ read~ the FAQ, etc. and actually accept hard~! atheism?@@'\n",
    "re.sub(r'[^a-zA-Z ]', '', text) #알파벳과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")# 한글과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "okt = Okt()\n",
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)\n",
    "\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(X_train)\n",
    "\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer2.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "tokenizer_kakao = Tokenizer(vocab_size) \n",
    "tokenizer_kakao.fit_on_texts(X_train)\n",
    "X_train = tokenizer_kakao.texts_to_sequences(X_train)\n",
    "\n",
    "max_len = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-newport",
   "metadata": {},
   "source": [
    "**사용자에게 받은 데이터 전처리 테스트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "final-champagne",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['현재', 'AI', '기술', '불완전하기', '때문', '가짜', '뉴스', '완전히', '걸러', '없다'], ['정치', '세대', '교체', '바람', '일으킨', '이준석', '대선', '출마', '한다'], ['늘어가는', '해양사고', '전북', '관내', '잠수', '구조', '인력', '전무']]\n",
      "[[503, 1459, 504, 1460, 22, 71, 104, 1461, 1462, 8], [72, 761, 1463, 1464, 762, 1465, 51, 360, 5], [3988, 3989, 483, 1080, 3990, 226, 1037, 1458]]\n",
      "(3, 10)\n",
      "[[ 503 1459  504 1460   22   71  104 1461 1462    8]]\n",
      "[[   0   72  761 1463 1464  762 1465   51  360    5]]\n",
      "[[   0    0 3988 3989  483 1080 3990  226 1037 1458]]\n",
      "[0.5054650902748108, 0.5144935846328735, 0.511650025844574]\n"
     ]
    }
   ],
   "source": [
    "response = ['현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다', '‘정치 세대교체’ 바람 일으킨 이준석, 대선엔 출마 못한다', '\"늘어가는 해양사고, 전북 관내 잠수구조인력 전무(全無)\"?']\n",
    "tokens_response = []\n",
    "\n",
    "for i in range(len(response)):\n",
    "    tokens = my_Tokenizer(response[i])\n",
    "    tokens_response.append(tokens)\n",
    "print(tokens_response)\n",
    "\n",
    "word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "print(word_seq_response)\n",
    "word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "print(word_seq_response.shape)\n",
    "\n",
    "score_list = []\n",
    "for sco in range(len(word_seq_response)):\n",
    "    word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "    print(word_seq_response_to_score)\n",
    "    score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\n",
    "print(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-activation",
   "metadata": {},
   "source": [
    "**입력 데이터 처리 함수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "mental-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================================================================\n",
    "# 카카오톡 응답 데이터 score\n",
    "\n",
    "def kakao_data_score(response):\n",
    "    objectivity = ''\n",
    "    response = okt.morphs(response, stem=True) # 토큰화\n",
    "    response = [word for word in response if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer_kakao.texts_to_sequences([response]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(kakao_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.8 or score < 0.2):\n",
    "        objectivity = '객관성이 매우 낮습니다.'\n",
    "    elif(score > 0.65 or score < 0.35):\n",
    "        objectivity = '객관성이 낮습니다.'\n",
    "    else:\n",
    "        objectivity = '객관성이 높습니다.'\n",
    "    \n",
    "    return objectivity\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 단어 토큰화 및 빈도순 정렬 후 추출\n",
    "\n",
    "def tfidf_Vectorizer(response_list):\n",
    "    tfidf_Vectorizer = TfidfVectorizer(tokenizer=my_Tokenizer, min_df=1) # df 값(단어가 몇 문장들에서 등장하였는지)을 최소 'min_df' 값으로 설정.\n",
    "    X = tfidf_Vectorizer.fit_transform(response_list).toarray()\n",
    "    print(X.shape)    # X(2차원 배열)의 행,열 수를 출력.\n",
    "    print(tfidf_Vectorizer.vocabulary_)   # 각 단어들이 배열 X에서 몇번째 열(인덱스 값)에 해당하는지 출력.\n",
    "\n",
    "\n",
    "    #pandas를 활용하여 각 단어들의 각 문장에서의 tf-idf 값들을 모두 더하고, 내림차순으로 정렬하여 상위 n개 출력\n",
    "    count = X.sum(axis=0)    # 2차원 배열 X에서 각 열을 기준으로 합을 구함. (각 단어들의 '최종' tf-idf 값으로 간주.)\n",
    "    word_count = pd.DataFrame({\n",
    "        '단어' : tfidf_Vectorizer.get_feature_names(),\n",
    "        '빈도' : count.flat\n",
    "    })\n",
    "    sorted_df = word_count.sort_values('빈도', ascending=False)\n",
    "    print(sorted_df.head(10), \"\\n\")\n",
    "\n",
    "    word_ = list(np.array(sorted_df['단어'].tolist()))\n",
    "    return(word_)\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 연관키워드 추출\n",
    "\n",
    "def similar_keyword(word_):\n",
    "    similar_list = []\n",
    "\n",
    "    if len(word_) == 1:\n",
    "        similar_keywords = similar_model.wv.most_similar(word_[0])\n",
    "\n",
    "        for i in range(len(similar_keywords)):\n",
    "            similar_list.append(similar_keywords[i][0])\n",
    "    return similar_list\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# MySQL 데이터베이스에 응답데이터, 시간 저장\n",
    "\n",
    "def MySQL_data_save(response):\n",
    "    global cursor, conn\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    Korea_DatetimeNow = now + datetime.timedelta(hours = 9)\n",
    "    DatetimeNow = Korea_DatetimeNow.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            query = \"INSERT INTO factcheck.article_data (article_data, date) VALUES(%s, %s)\"\n",
    "            val = (response, DatetimeNow)\n",
    "            cursor.execute(query, val)\n",
    "            conn.commit()\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "        \n",
    "        \n",
    "# ===========================================================================================================================\n",
    "# MySQL 데이터베이스에 키워드 랭킹 반영\n",
    "\n",
    "def keyword_rank(word_):\n",
    "    rank_list = []\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute('SELECT keyword FROM keyword_rank')\n",
    "            row = cursor.fetchall()\n",
    "\n",
    "        for i in range(len(row)):\n",
    "            rank_list.append(row[i][0])\n",
    "\n",
    "        with conn.cursor() as cursor:\n",
    "            for i in range(0,1):\n",
    "                if word_[i] in rank_list:\n",
    "                    query = \"UPDATE keyword_rank SET count = count + 1 WHERE keyword = (%s)\"\n",
    "                    val = (word_[i])\n",
    "                    cursor.execute(query, val)\n",
    "                    conn.commit()\n",
    "                else:\n",
    "                    query = \"INSERT INTO factcheck.keyword_rank (keyword, count) VALUES(%s, 1)\"\n",
    "                    val = (word_[i])\n",
    "                    cursor.execute(query, val)\n",
    "                    conn.commit()\n",
    "\n",
    "        top10_list = []\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT keyword FROM keyword_rank ORDER BY count DESC LIMIT 10\")\n",
    "            row_top = cursor.fetchall()\n",
    "            for i in range(len(row_top)):\n",
    "                top10_list.append(row_top[i][0])\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "    return top10_list\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# SNU 기사 검색      \n",
    "\n",
    "def SNU_Search(word_):\n",
    "    title_list2 = []\n",
    "    # link_list2 = []\n",
    "    score_list2 = []\n",
    "\n",
    "    try:\n",
    "#             str_expr = \"document.str.contains('{}', case=False)\".format(word_[0]) and \"document.str.contains('{}', case=False)\".format(word_[1])\n",
    "        str_expr = \"document.str.contains('{}', case=False)\".format(word_[0])\n",
    "\n",
    "        df_q = train_df.query(str_expr, engine=\"python\")\n",
    "\n",
    "        title = list(np.array(df_q['document'].tolist()))\n",
    "        score_SNU = list(np.array(df_q['label'].tolist()))\n",
    "        # link = list(np.array(df_q['link'].tolist()))\n",
    "\n",
    "        for i in range(len(title)):\n",
    "            title_list2.append(title[i])\n",
    "        #     link_list2.append(link[i])\n",
    "            score_list2.append(score_SNU[i])\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    return title_list2, score_list2\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 네이버 뉴스 검색\n",
    "\n",
    "def Naver_Search(response):\n",
    "    baseurl = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=\"\n",
    "\n",
    "    url = baseurl + urllib.parse.quote_plus(response)\n",
    "\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    title = soup.find_all(class_ = 'news_tit')\n",
    "\n",
    "    title_list = []\n",
    "    link_list = []\n",
    "\n",
    "    for i in title:\n",
    "        title_list.append(i.attrs['title'])\n",
    "        link_list.append(i.attrs['href'])\n",
    "\n",
    "# ================================================================\n",
    "\n",
    "    tokens_response = []\n",
    "    score_list = []\n",
    "\n",
    "    for i in range(len(title_list)):\n",
    "        tokens = my_Tokenizer(title_list[i])\n",
    "        tokens_response.append(tokens)\n",
    "\n",
    "    word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "    word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "\n",
    "    for sco in range(len(title_list)):\n",
    "        word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "        score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\n",
    "    \n",
    "    return title_list, link_list, score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-keyboard",
   "metadata": {},
   "source": [
    "**Flask 웹 서버**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-looking",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "{'안녕하세요': 0, '오소리': 1}\n",
      "      단어        빈도\n",
      "0  안녕하세요  0.707107\n",
      "1    오소리  0.707107 \n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [09/Sep/2021 15:24:34] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "입력 데이터 : \n",
      " 안녕하세요 오소리에요\n",
      "\n",
      "\n",
      "연관 검색어 : \n",
      " ['연관 검색어가 없습니다.']\n",
      "\n",
      "\n",
      "네이버 뉴스 제목 : \n",
      " ['독수리술·오소리술 들어는 보셨습니까? 전설적인 평안도 사냥꾼의 음식', '[어제보다 행복하기]\\xa0비움의 행복', '뮤지션 #천용성 의 두 번째 목소리', '주문을 걸어 (원슈타인 화보 & 인터뷰)', \"'가상화폐, 문제는 다 文 정부 때문'이라는 언론\"]\n",
      "\n",
      "\n",
      "네이버 뉴스 링크 : \n",
      " ['http://www.nocutnews.co.kr/news/4655459', 'http://www.newscj.com/news/articleView.html?idxno=788375', 'http://www.elle.co.kr/article/56416', 'http://www.wkorea.com/2020/12/24/%ec%a3%bc%eb%ac%b8%ec%9d%84-%ea%b1%b8%ec%96%b4-%ec%9b%90%ec%8a%88%ed%83%80%ec%9d%b8/?utm_source=naver&utm_medium=partnership', 'http://www.nocutnews.co.kr/news/4910068']\n",
      "\n",
      "\n",
      "네이버 뉴스 score : \n",
      " [0.5058199763298035, 0.5058199763298035, 0.5058199763298035, 0.5060451626777649, 0.5118430256843567]\n",
      "\n",
      "\n",
      "SNU 기사 제목 : \n",
      " ['관련 뉴스 결과가 없습니다.']\n",
      "\n",
      "\n",
      "SNU 기사 score : \n",
      " ['x']\n",
      "\n",
      "\n",
      "인기키워드 랭킹 : \n",
      " ['안녕하세요', '유료', '카카오', '25', '코로나', '123123', '카카오톡', 'nameasdfasdfas', '입니다', 'dcxzvdszvzcd']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(1, 2)\n",
      "{'안녕하세요': 0, '오소리': 1}\n",
      "      단어        빈도\n",
      "0  안녕하세요  0.707107\n",
      "1    오소리  0.707107 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125.133.174.241 - - [09/Sep/2021 15:24:39] \"\u001b[37mPOST /kakao HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터 : \n",
      " 안녕하세요 오소리에요\n",
      "\n",
      "\n",
      "객관성 level : \n",
      " 객관성이 높습니다.\n",
      "\n",
      "\n",
      "연관 검색어 : \n",
      " ['연관 검색어가 없습니다.']\n",
      "\n",
      "\n",
      "네이버 뉴스 제목 : \n",
      " ['독수리술·오소리술 들어는 보셨습니까? 전설적인 평안도 사냥꾼의 음식', '[어제보다 행복하기]\\xa0비움의 행복', '뮤지션 #천용성 의 두 번째 목소리', '주문을 걸어 (원슈타인 화보 & 인터뷰)', \"'가상화폐, 문제는 다 文 정부 때문'이라는 언론\"]\n",
      "\n",
      "\n",
      "네이버 뉴스 링크 : \n",
      " ['http://www.nocutnews.co.kr/news/4655459', 'http://www.newscj.com/news/articleView.html?idxno=788375', 'http://www.elle.co.kr/article/56416', 'http://www.wkorea.com/2020/12/24/%ec%a3%bc%eb%ac%b8%ec%9d%84-%ea%b1%b8%ec%96%b4-%ec%9b%90%ec%8a%88%ed%83%80%ec%9d%b8/?utm_source=naver&utm_medium=partnership', 'http://www.nocutnews.co.kr/news/4910068']\n",
      "\n",
      "\n",
      "네이버 뉴스 score : \n",
      " [0.5058199763298035, 0.5058199763298035, 0.5058199763298035, 0.5060451626777649, 0.5118430256843567]\n",
      "\n",
      "\n",
      "SNU 기사 제목 : \n",
      " ['관련 뉴스 결과가 없습니다.']\n",
      "\n",
      "\n",
      "SNU 기사 score : \n",
      " ['x']\n",
      "\n",
      "\n",
      "인기키워드 랭킹 : \n",
      " ['안녕하세요', '유료', '카카오', '25', '코로나', '123123', '카카오톡', 'nameasdfasdfas', '입니다', 'dcxzvdszvzcd']\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "\n",
    "from flask import Flask, request\n",
    "import json\n",
    "from urllib import parse\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/', methods = ['POST'])\n",
    "def Article_keyword():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        \n",
    "        word_ = tfidf_Vectorizer(response_list)\n",
    "        print('1')\n",
    "        similar_list = similar_keyword(word_)\n",
    "        print('2')\n",
    "        MySQL_data_save(response)\n",
    "        print('3')\n",
    "        top10_list = keyword_rank(word_)\n",
    "        print('4')\n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        print('5')\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response)\n",
    "        print('6')\n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "            \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        print('입력 데이터 : \\n', response)\n",
    "        print('\\n')\n",
    "        print('연관 검색어 : \\n', similar_list)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 제목 : \\n', Naver_title)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 링크 : \\n', Naver_link)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 score : \\n', Naver_score)\n",
    "        print('\\n')\n",
    "        try:\n",
    "            print('SNU 기사 제목 : \\n', SNU_title)\n",
    "            print('\\n')\n",
    "    #         print('SNU 기사 링크 : ', link_list2)\n",
    "            print('SNU 기사 score : \\n', SNU_score)\n",
    "            print('\\n')\n",
    "        except:\n",
    "            print('SNU 기사가 없습니다. \\n')\n",
    "        print('인기키워드 랭킹 : \\n', top10_list)\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "\n",
    "@app.route('/kakao', methods = ['POST'])\n",
    "def kakao_():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        \n",
    "        objectivity = kakao_data_score(response)\n",
    "        word_ = tfidf_Vectorizer(response_list)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response)\n",
    "        \n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "        \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'objectivity':objectivity,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        print('입력 데이터 : \\n', response)\n",
    "        print('\\n')\n",
    "        print('객관성 level : \\n', objectivity)\n",
    "        print('\\n')\n",
    "        print('연관 검색어 : \\n', similar_list)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 제목 : \\n', Naver_title)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 링크 : \\n', Naver_link)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 score : \\n', Naver_score)\n",
    "        print('\\n')\n",
    "        try:\n",
    "            print('SNU 기사 제목 : \\n', SNU_title)\n",
    "            print('\\n')\n",
    "    #         print('SNU 기사 링크 : ', link_list2)\n",
    "            print('SNU 기사 score : \\n', SNU_score)\n",
    "            print('\\n')\n",
    "        except:\n",
    "            print('SNU 기사가 없습니다. \\n')\n",
    "        print('인기키워드 랭킹 : \\n', top10_list)\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port = 8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-peeing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

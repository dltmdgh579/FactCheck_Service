{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from http.server import BaseHTTPRequestHandler, HTTPServer\r\n",
    "import logging\r\n",
    "import requests\r\n",
    "import urllib.request\r\n",
    "import json\r\n",
    "import numpy as np\r\n",
    "from tensorflow.keras.models import load_model\r\n",
    "from gensim.models import FastText\r\n",
    "from konlpy.tag import Okt\r\n",
    "import boto3\r\n",
    "import sys\r\n",
    "import pandas as pd\r\n",
    "from tqdm import tqdm\r\n",
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from keras.preprocessing import sequence\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
    "import urllib.parse\r\n",
    "import urllib.request\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "import re\r\n",
    "\r\n",
    "MAX_NB_WORDS = 100000\r\n",
    "SNU_model_All = load_model('SNU_LSTM_Model_All_balance.h5')\r\n",
    "SNU_model_pol = load_model('SNU_LSTM_Model_정치_balance.h5')\r\n",
    "SNU_model_eco = load_model('SNU_LSTM_Model_경제_balance.h5')\r\n",
    "SNU_model_soc = load_model('SNU_LSTM_Model_사회_balance.h5')\r\n",
    "SNU_model_others = load_model('SNU_LSTM_Model_기타_balance.h5')\r\n",
    "kakao_model = load_model('kakao_LSTM_model.h5')\r\n",
    "# SNU_model_shuffle = load_model('SNU_LSTM_Model_shuffle.h5')\r\n",
    "similar_model = FastText.load('similar_keyword_model')\r\n",
    "\r\n",
    "class MyTokenizer:\r\n",
    "    def __init__(self, tagger):\r\n",
    "        self.tagger = tagger\r\n",
    "\r\n",
    "    def __call__(self, sent):\r\n",
    "        pos = self.tagger.pos(sent)\r\n",
    "        clean_words = []  # 정제된 단어 리스트\r\n",
    "        for word in pos:\r\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\r\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\r\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\r\n",
    "                    #if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\r\n",
    "                    clean_words.append(word[0])\r\n",
    "        return clean_words\r\n",
    "\r\n",
    "\r\n",
    "my_Tokenizer = MyTokenizer(Okt())\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pymysql\r\n",
    "import sys\r\n",
    "import os\r\n",
    "import requests\r\n",
    "import base64\r\n",
    "import json\r\n",
    "import logging\r\n",
    "import time\r\n",
    "import pandas as pd\r\n",
    "import csv\r\n",
    "import datetime\r\n",
    "\r\n",
    "host = '' # rds endpoint\r\n",
    "port = 3306\r\n",
    "username = ''\r\n",
    "database = ''\r\n",
    "password = ''\r\n",
    "\r\n",
    "def connect_RDS(host, port, username, password, database):\r\n",
    "    try:\r\n",
    "        conn = pymysql.connect(host = host, user = username, password = password, db = database, port = port, use_unicode = True, charset = 'utf8')\r\n",
    "        cursor = conn.cursor()\r\n",
    "    \r\n",
    "    except:\r\n",
    "        logging.error('RDS에 연결되지 않았습니다.')\r\n",
    "        sys.exit(1)\r\n",
    "        \r\n",
    "    return conn, cursor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "if sys.version_info[0] < 3:\r\n",
    "    from io import StringIO # Python 2.x\r\n",
    "else:\r\n",
    "    from io import StringIO # Python 3.x\r\n",
    "\r\n",
    "# get your credentials from environment variables\r\n",
    "aws_id = ''\r\n",
    "aws_secret = ''\r\n",
    "\r\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\r\n",
    "        aws_secret_access_key=aws_secret)\r\n",
    "\r\n",
    "bucket_name = 'snucsv'\r\n",
    "label_names = [\"label\"]\r\n",
    "\r\n",
    "def s3_load(category):\r\n",
    "    object_key = category\r\n",
    "    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\r\n",
    "    body = csv_obj['Body']\r\n",
    "    csv_string = body.read().decode('cp949')\r\n",
    "\r\n",
    "    # load data\r\n",
    "    train_df = pd.read_csv(StringIO(csv_string))\r\n",
    "    y_train = train_df[label_names].values\r\n",
    "\r\n",
    "    train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\r\n",
    "    max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\r\n",
    "    \r\n",
    "    return train_df, y_train, max_seq_len\r\n",
    "\r\n",
    "train_df_All_, y_train_All_, max_seq_len = s3_load('SNU_All_.csv')\r\n",
    "train_df_All_b, y_train_All_b, max_seq_len_All_b = s3_load('SNU_All_b.csv')\r\n",
    "train_df_eco, y_train_eco, max_seq_len_eco = s3_load('SNU_경제_b.csv')\r\n",
    "train_df_pol, y_train_pol, max_seq_len_pol = s3_load('SNU_정치_b.csv')\r\n",
    "train_df_soc, y_train_soc, max_seq_len_soc = s3_load('SNU_사회_b.csv')\r\n",
    "train_df_other, y_train_other, max_seq_len_other = s3_load('SNU_기타_b.csv')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "train_df_other"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>link</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>우리나라에 불어닥치는 중국발 미세먼지는 약30% 정도로 국내 요인이 더 크다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/2662</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>포털 다음 음성 서비스, 윤석열 '주120시간' 발언을 36시간으로 왜곡했다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3168</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"정부가 국민이 낸 건강보험료로 중국인에게 공짜·할인 혜택을 주고 있다.\"</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/2051</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>대북 선전용 전단 삐라 살포는 북한 주민 설득에 효과가 없다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/2677</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>유리창, 신문지로 닦으면 더 깨끗하다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/1152</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>일본은 군위안부 역사 지우기를 위해 대미 로비를 활용하고 있다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/2816</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>여자가 얀센 백신 부작용 더 심하다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3096</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>디지털세가 도입되면 삼성,현대차 등 해외 매출 비중이 높은 우리나라 기업들도 해외에...</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3100</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>일본 스가 총리는 한 달 전 미국 방문해서 1억회 분 백신 확보했다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3046</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>캐나다·독일 등의 국가에는 펫샵이 없다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3052</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              document  \\\n",
       "0           우리나라에 불어닥치는 중국발 미세먼지는 약30% 정도로 국내 요인이 더 크다   \n",
       "1           포털 다음 음성 서비스, 윤석열 '주120시간' 발언을 36시간으로 왜곡했다   \n",
       "2           \"정부가 국민이 낸 건강보험료로 중국인에게 공짜·할인 혜택을 주고 있다.\"    \n",
       "3                    대북 선전용 전단 삐라 살포는 북한 주민 설득에 효과가 없다   \n",
       "4                                 유리창, 신문지로 닦으면 더 깨끗하다   \n",
       "..                                                 ...   \n",
       "252                 일본은 군위안부 역사 지우기를 위해 대미 로비를 활용하고 있다   \n",
       "253                                여자가 얀센 백신 부작용 더 심하다   \n",
       "254  디지털세가 도입되면 삼성,현대차 등 해외 매출 비중이 높은 우리나라 기업들도 해외에...   \n",
       "255              일본 스가 총리는 한 달 전 미국 방문해서 1억회 분 백신 확보했다   \n",
       "256                              캐나다·독일 등의 국가에는 펫샵이 없다   \n",
       "\n",
       "                                          link  label  doc_len  \n",
       "0    https://factcheck.snu.ac.kr/v2/facts/2662      1       10  \n",
       "1    https://factcheck.snu.ac.kr/v2/facts/3168      1        9  \n",
       "2    https://factcheck.snu.ac.kr/v2/facts/2051      0       10  \n",
       "3    https://factcheck.snu.ac.kr/v2/facts/2677      1       10  \n",
       "4    https://factcheck.snu.ac.kr/v2/facts/1152      1        5  \n",
       "..                                         ...    ...      ...  \n",
       "252  https://factcheck.snu.ac.kr/v2/facts/2816      1        9  \n",
       "253  https://factcheck.snu.ac.kr/v2/facts/3096      1        6  \n",
       "254  https://factcheck.snu.ac.kr/v2/facts/3100      0       18  \n",
       "255  https://factcheck.snu.ac.kr/v2/facts/3046      0       12  \n",
       "256  https://factcheck.snu.ac.kr/v2/facts/3052      1        5  \n",
       "\n",
       "[257 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**단어 벡터화 및 패딩**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def embedding_padding(train_df):\n",
    "    raw_docs_train = train_df['document'].tolist()\n",
    "    num_classes = len(label_names)\n",
    "    print(num_classes)\n",
    "    processed_docs_train = []\n",
    "\n",
    "    for doc in tqdm(raw_docs_train):\n",
    "        tokens = my_Tokenizer(doc)\n",
    "        processed_docs_train.append(tokens)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "    tokenizer.fit_on_texts(processed_docs_train)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer_All = embedding_padding(train_df_All_b)\n",
    "tokenizer_pol = embedding_padding(train_df_pol)\n",
    "tokenizer_soc = embedding_padding(train_df_soc)\n",
    "tokenizer_eco = embedding_padding(train_df_eco)\n",
    "tokenizer_other = embedding_padding(train_df_other)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/1441 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1441/1441 [00:08<00:00, 160.61it/s]\n",
      " 13%|█▎        | 65/509 [00:00<00:00, 648.95it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 509/509 [00:00<00:00, 607.39it/s]\n",
      " 17%|█▋        | 84/490 [00:00<00:00, 834.43it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 490/490 [00:00<00:00, 649.20it/s]\n",
      " 11%|█         | 22/198 [00:00<00:01, 138.59it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 198/198 [00:00<00:00, 495.81it/s]\n",
      " 33%|███▎      | 86/257 [00:00<00:00, 852.89it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 257/257 [00:00<00:00, 568.21it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# word_seq_train = tokenizer_All.texts_to_sequences(processed_docs_train)\n",
    "# word_index = tokenizer_All.word_index\n",
    "# print('dictionary size :', len(word_index))\n",
    "\n",
    "# word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "# word_seq_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# 카카오 응답 데이터 전처리\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "text = 'do!!! you expect... people~ to~ read~ the FAQ, etc. and actually accept hard~! atheism?@@'\n",
    "re.sub(r'[^a-zA-Z ]', '', text) #알파벳과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")# 한글과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "okt = Okt()\n",
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)\n",
    "\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(X_train)\n",
    "\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer2.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer2.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "tokenizer_kakao = Tokenizer(vocab_size) \n",
    "tokenizer_kakao.fit_on_texts(X_train)\n",
    "X_train = tokenizer_kakao.texts_to_sequences(X_train)\n",
    "\n",
    "max_len = 30"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**사용자에게 받은 데이터 전처리 테스트**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# response = ['게르마늄 팔찌, 과학적으로 효능 있을까?', '빌려쓰는 IT서비스 클라우드컴퓨팅, 실제 비용절감 효과 있나', '\"심장마비 왔을 때 강하게 기침하면 막을 수 있다\"\t']\n",
    "# tokens_response = []\n",
    "\n",
    "# for i in range(len(response)):\n",
    "#     tokens = my_Tokenizer(response[i])\n",
    "#     tokens_response.append(tokens)\n",
    "# print(tokens_response)\n",
    "\n",
    "# word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "# print(word_seq_response)\n",
    "# word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "# print(word_seq_response.shape)\n",
    "\n",
    "# score_list = []\n",
    "# for sco in range(len(word_seq_response)):\n",
    "#     word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "#     print(word_seq_response_to_score)\n",
    "#     score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\n",
    "# print(score_list)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**입력 데이터 처리 함수**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# ===========================================================================================================================\n",
    "# 카카오톡 응답 데이터 score\n",
    "\n",
    "def kakao_data_score(response):\n",
    "    objectivity = ''\n",
    "    response = okt.morphs(response, stem=True) # 토큰화\n",
    "    response = [word for word in response if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer_kakao.texts_to_sequences([response]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(kakao_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.8 or score < 0.2):\n",
    "        objectivity = '객관성이 매우 낮습니다.'\n",
    "    elif(score > 0.65 or score < 0.35):\n",
    "        objectivity = '객관성이 낮습니다.'\n",
    "    else:\n",
    "        objectivity = '객관성이 높습니다.'\n",
    "    \n",
    "    return objectivity\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 단어 토큰화 및 빈도순 정렬 후 추출\n",
    "\n",
    "def tfidf_Vectorizer(response_list):\n",
    "    tfidf_Vectorizer = TfidfVectorizer(tokenizer=my_Tokenizer, min_df=1) # df 값(단어가 몇 문장들에서 등장하였는지)을 최소 'min_df' 값으로 설정.\n",
    "    X = tfidf_Vectorizer.fit_transform(response_list).toarray()\n",
    "#     print(X.shape)    # X(2차원 배열)의 행,열 수를 출력.\n",
    "#     print(tfidf_Vectorizer.vocabulary_)   # 각 단어들이 배열 X에서 몇번째 열(인덱스 값)에 해당하는지 출력.\n",
    "\n",
    "\n",
    "    #pandas를 활용하여 각 단어들의 각 문장에서의 tf-idf 값들을 모두 더하고, 내림차순으로 정렬하여 상위 n개 출력\n",
    "    count = X.sum(axis=0)    # 2차원 배열 X에서 각 열을 기준으로 합을 구함. (각 단어들의 '최종' tf-idf 값으로 간주.)\n",
    "    word_count = pd.DataFrame({\n",
    "        '단어' : tfidf_Vectorizer.get_feature_names(),\n",
    "        '빈도' : count.flat\n",
    "    })\n",
    "    sorted_df = word_count.sort_values('빈도', ascending=False)\n",
    "#     print(sorted_df.head(10), \"\\n\")\n",
    "\n",
    "    word_ = list(np.array(sorted_df['단어'].tolist()))\n",
    "    return(word_)\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 네이버 댓글 분석\n",
    "\n",
    "def Naver_Comments_score(response, tokenizer, Naver_Comments_Model):\n",
    "    response_list = []\n",
    "    response_list.append(response)\n",
    "    response = re.compile(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]+\").sub('',response)\n",
    "    response = okt.morphs(response, stem=True) # 토큰화\n",
    "    response = [word for word in response if not word in stopwords] # 불용어 제거\n",
    "    \n",
    "    word_ = tfidf_Vectorizer(response_list)\n",
    "    word__ = \"\"\n",
    "    for i in range(1, min(len(word_), 7)):\n",
    "        word__ += word_[i]\n",
    "        word__ += \" \"\n",
    "    \n",
    "    df_qq = []\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}\n",
    "\n",
    "    allComments = []\n",
    "    def create_soup(url):\n",
    "        res = requests.get(url, headers=headers)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        return soup\n",
    "    \n",
    "    Comments_score_list = []\n",
    "    # 네이버 뉴스 url을 입력합니다.\n",
    "    url1 = \"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}\".format(word__)\n",
    "    soup = create_soup(url1)\n",
    "    for i in range(1, 15):\n",
    "        List = []\n",
    "        try:\n",
    "            url = soup.select_one(\"#sp_nws{} > div.news_wrap.api_ani_send > div > div.news_info > div.info_group > a:nth-of-type(2)\".format(i))['href']\n",
    "            oid = url.split(\"oid=\")[1].split(\"&\")[0] #422\n",
    "            aid = url.split(\"aid=\")[1] #0000430957\n",
    "            page = 1\n",
    "            header = {\n",
    "                \"User-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\",\n",
    "                \"referer\": url,\n",
    "            }\n",
    "            # time.sleep(random.randint(1,2))\n",
    "\n",
    "            while True:\n",
    "                c_url = \"https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json?ticket=news&templateId=default_society&pool=cbox5&_callback=jQuery1707138182064460843_1523512042464&lang=ko&country=&objectId=news\" + oid + \"%2C\" + aid + \"&categoryId=&pageSize=20&indexSize=10&groupId=&listType=OBJECT&pageType=more&page=\" + str(\n",
    "                    page) + \"&refresh=false&sort=FAVORITE\"\n",
    "                # 파싱하는 단계입니다.\n",
    "                r = requests.get(c_url, headers=header)\n",
    "                cont = BeautifulSoup(r.content, \"html.parser\")\n",
    "                total_comm = str(cont).split('comment\":')[1].split(\",\")[0]\n",
    "\n",
    "                match = re.findall('\"contents\":([^\\*]*),\"userIdNo\"', str(cont))\n",
    "                # 댓글을 리스트에 중첩합니다.\n",
    "                for b in range(0,5):\n",
    "                    List.append(match[b])\n",
    "                break\n",
    "            \n",
    "            tokens_response = []\n",
    "            score_list = []\n",
    "\n",
    "            for i in range(len(List)):\n",
    "                tokens = my_Tokenizer(List[i])\n",
    "                tokens_response.append(tokens)\n",
    "\n",
    "            word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "            word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "\n",
    "            for sco in range(len(List)):\n",
    "                word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "                score_list.append(float(Naver_Comments_Model.predict(word_seq_response_to_score)))\n",
    "                \n",
    "            Comments_score_list.append(np.mean(score_list))\n",
    "\n",
    "            # 여러 리스트들을 하나로 묶어 주는 함수입니다.\n",
    "            def flatten(l):\n",
    "                flatList = []\n",
    "                for elem in l:\n",
    "                    # if an element of a list is a list\n",
    "                    # iterate over this list and add elements to flatList\n",
    "                    if type(elem) == list:\n",
    "                        for e in elem:\n",
    "                            flatList.append(e)\n",
    "                    else:\n",
    "                        flatList.append(elem)\n",
    "                return flatList\n",
    "\n",
    "            # 리스트 결과입니다.\n",
    "\n",
    "            allComments = flatten(List)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 연관키워드 추출\n",
    "\n",
    "def similar_keyword(word_):\n",
    "    similar_list = []\n",
    "\n",
    "    if len(word_) == 1:\n",
    "        similar_keywords = similar_model.wv.most_similar(word_[0])\n",
    "\n",
    "        for i in range(len(similar_keywords)):\n",
    "            similar_list.append(similar_keywords[i][0])\n",
    "    return similar_list\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# MySQL 데이터베이스에 응답데이터, 시간 저장\n",
    "\n",
    "def MySQL_data_save(response):\n",
    "    global cursor, conn\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    Korea_DatetimeNow = now + datetime.timedelta(hours = 9)\n",
    "    DatetimeNow = Korea_DatetimeNow.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            query = \"INSERT INTO factcheck.article_data (article_data, date) VALUES(%s, %s)\"\n",
    "            val = (response, DatetimeNow)\n",
    "            cursor.execute(query, val)\n",
    "            conn.commit()\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "        \n",
    "        \n",
    "# ===========================================================================================================================\n",
    "# MySQL 데이터베이스에 키워드 랭킹 반영\n",
    "\n",
    "def keyword_rank(word_):\n",
    "    rank_list = []\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute('SELECT keyword FROM keyword_rank')\n",
    "            row = cursor.fetchall()\n",
    "\n",
    "        for i in range(len(row)):\n",
    "            rank_list.append(row[i][0])\n",
    "\n",
    "        with conn.cursor() as cursor:\n",
    "            for i in range(0,1):\n",
    "                if word_[i] in rank_list:\n",
    "                    query = \"UPDATE keyword_rank SET count = count + 1 WHERE keyword = (%s)\"\n",
    "                    val = (word_[i])\n",
    "                    cursor.execute(query, val)\n",
    "                    conn.commit()\n",
    "                else:\n",
    "                    query = \"INSERT INTO factcheck.keyword_rank (keyword, count) VALUES(%s, 1)\"\n",
    "                    val = (word_[i])\n",
    "                    cursor.execute(query, val)\n",
    "                    conn.commit()\n",
    "\n",
    "        top10_list = []\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT keyword FROM keyword_rank ORDER BY count DESC LIMIT 10\")\n",
    "            row_top = cursor.fetchall()\n",
    "            for i in range(len(row_top)):\n",
    "                top10_list.append(row_top[i][0])\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "    return top10_list\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# SNU 기사 검색      \n",
    "\n",
    "def SNU_Search(word_):\n",
    "    title_list2 = []\n",
    "    # link_list2 = []\n",
    "    score_list2 = []\n",
    "\n",
    "    try:\n",
    "#             str_expr = \"document.str.contains('{}', case=False)\".format(word_[0]) and \"document.str.contains('{}', case=False)\".format(word_[1])\n",
    "        str_expr = \"document.str.contains('{}', case=False)\".format(word_[0])\n",
    "\n",
    "        df_q = train_df_All_.query(str_expr, engine=\"python\")\n",
    "\n",
    "        title = list(np.array(df_q['document'].tolist()))\n",
    "        score_SNU = list(np.array(df_q['label'].tolist()))\n",
    "        # link = list(np.array(df_q['link'].tolist()))\n",
    "\n",
    "        for i in range(len(title)):\n",
    "            title_list2.append(title[i])\n",
    "        #     link_list2.append(link[i])\n",
    "            score_list2.append(score_SNU[i])\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    return title_list2, score_list2\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 네이버 뉴스 검색\n",
    "\n",
    "def Naver_Search(response, tokenizer, SNU_model):\n",
    "    baseurl = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=\"\n",
    "\n",
    "    url = baseurl + urllib.parse.quote_plus(response)\n",
    "\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    title = soup.find_all(class_ = 'news_tit')\n",
    "\n",
    "    title_list = []\n",
    "    link_list = []\n",
    "\n",
    "    for i in title:\n",
    "        title_list.append(i.attrs['title'])\n",
    "        link_list.append(i.attrs['href'])\n",
    "\n",
    "# ================================================================\n",
    "\n",
    "    tokens_response = []\n",
    "    score_list = []\n",
    "\n",
    "    for i in range(len(title_list)):\n",
    "        tokens = my_Tokenizer(title_list[i])\n",
    "        tokens_response.append(tokens)\n",
    "\n",
    "    word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "    word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "\n",
    "    for sco in range(len(title_list)):\n",
    "        word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "        score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\n",
    "    \n",
    "    return title_list, link_list, score_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Flask 웹 서버**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#-*-coding:utf-8-*-\n",
    "\n",
    "from flask import Flask, request\n",
    "import json\n",
    "from urllib import parse\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/politics', methods = ['POST'])\n",
    "def Article_keyword_politics():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        \n",
    "        word_ = tfidf_Vectorizer(response_list)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_pol, SNU_model_pol)\n",
    "        \n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "            \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "    \n",
    "@app.route('/economy', methods = ['POST'])\n",
    "def Article_keyword_economy():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        \n",
    "        word_ = tfidf_Vectorizer(response_list)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_eco, SNU_model_eco)\n",
    "        \n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "            \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "    \n",
    "@app.route('/society', methods = ['POST'])\n",
    "def Article_keyword_society():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        \n",
    "        word_ = tfidf_Vectorizer(response_list)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_soc, SNU_model_soc)\n",
    "        \n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "            \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "    \n",
    "@app.route('/others', methods = ['POST'])\n",
    "def Article_keyword_others():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        \n",
    "        word_ = tfidf_Vectorizer(response_list)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_other, SNU_model_others)\n",
    "        \n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "            \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "\n",
    "@app.route('/kakao', methods = ['POST'])\n",
    "def kakao_():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        \n",
    "        objectivity = kakao_data_score(response)\n",
    "        word_ = tfidf_Vectorizer(response_list)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_All, SNU_model_All)\n",
    "        \n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "        \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'objectivity':objectivity,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        print('입력 데이터 : \\n', response)\n",
    "        print('\\n')\n",
    "        print('객관성 level : \\n', objectivity)\n",
    "        print('\\n')\n",
    "        print('연관 검색어 : \\n', similar_list)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 제목 : \\n', Naver_title)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 링크 : \\n', Naver_link)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 score : \\n', Naver_score)\n",
    "        print('\\n')\n",
    "        try:\n",
    "            print('SNU 기사 제목 : \\n', SNU_title)\n",
    "            print('\\n')\n",
    "    #         print('SNU 기사 링크 : ', link_list2)\n",
    "            print('SNU 기사 score : \\n', SNU_score)\n",
    "            print('\\n')\n",
    "        except:\n",
    "            print('SNU 기사가 없습니다. \\n')\n",
    "        print('인기키워드 랭킹 : \\n', top10_list)\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port = 8080)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 11) for input KerasTensor(type_spec=TensorSpec(shape=(None, 11), dtype=tf.float32, name='embedding_10_input'), name='embedding_10_input', description=\"created by layer 'embedding_10_input'\"), but it was called on an input with incompatible shape (None, 10).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "125.133.174.241 - - [17/Sep/2021 04:17:44] \"\u001b[37mPOST /kakao HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "입력 데이터 : \n",
      " 안녕하세요 오소리에요\n",
      "\n",
      "\n",
      "객관성 level : \n",
      " 객관성이 매우 낮습니다.\n",
      "\n",
      "\n",
      "연관 검색어 : \n",
      " ['연관 검색어가 없습니다.']\n",
      "\n",
      "\n",
      "네이버 뉴스 제목 : \n",
      " ['독수리술·오소리술 들어는 보셨습니까? 전설적인 평안도 사냥꾼의 음식', '[어제보다 행복하기]\\xa0비움의 행복', '뮤지션 #천용성 의 두 번째 목소리', '주문을 걸어 (원슈타인 화보 & 인터뷰)', \"'가상화폐, 문제는 다 文 정부 때문'이라는 언론\"]\n",
      "\n",
      "\n",
      "네이버 뉴스 링크 : \n",
      " ['http://www.nocutnews.co.kr/news/4655459', 'http://www.newscj.com/news/articleView.html?idxno=788375', 'http://www.elle.co.kr/article/56416', 'http://www.wkorea.com/2020/12/24/%ec%a3%bc%eb%ac%b8%ec%9d%84-%ea%b1%b8%ec%96%b4-%ec%9b%90%ec%8a%88%ed%83%80%ec%9d%b8/?utm_source=naver&utm_medium=partnership', 'http://www.nocutnews.co.kr/news/4910068']\n",
      "\n",
      "\n",
      "네이버 뉴스 score : \n",
      " [0.6721777319908142, 0.6721777319908142, 0.6721777319908142, 0.4884408414363861, 0.05930577218532562]\n",
      "\n",
      "\n",
      "SNU 기사 제목 : \n",
      " ['관련 뉴스 결과가 없습니다.']\n",
      "\n",
      "\n",
      "SNU 기사 score : \n",
      " ['x']\n",
      "\n",
      "\n",
      "인기키워드 랭킹 : \n",
      " ['안녕하세요', '유료', '카카오', '25', '코로나', '123123', '카카오톡', 'nameasdfasdfas', '입니다', 'dcxzvdszvzcd']\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "125.133.174.241 - - [17/Sep/2021 04:17:53] \"\u001b[37mPOST /society HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 11) for input KerasTensor(type_spec=TensorSpec(shape=(None, 11), dtype=tf.float32, name='embedding_7_input'), name='embedding_7_input', description=\"created by layer 'embedding_7_input'\"), but it was called on an input with incompatible shape (None, 10).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "125.133.174.241 - - [17/Sep/2021 04:18:01] \"\u001b[37mPOST /economy HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 11) for input KerasTensor(type_spec=TensorSpec(shape=(None, 11), dtype=tf.float32, name='embedding_6_input'), name='embedding_6_input', description=\"created by layer 'embedding_6_input'\"), but it was called on an input with incompatible shape (None, 10).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "125.133.174.241 - - [17/Sep/2021 04:18:06] \"\u001b[37mPOST /politics HTTP/1.1\u001b[0m\" 200 -\n",
      "125.133.174.241 - - [17/Sep/2021 04:18:29] \"\u001b[37mPOST /kakao HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "입력 데이터 : \n",
      " 안녕하세요 오소리에요\n",
      "\n",
      "\n",
      "객관성 level : \n",
      " 객관성이 매우 낮습니다.\n",
      "\n",
      "\n",
      "연관 검색어 : \n",
      " ['연관 검색어가 없습니다.']\n",
      "\n",
      "\n",
      "네이버 뉴스 제목 : \n",
      " ['독수리술·오소리술 들어는 보셨습니까? 전설적인 평안도 사냥꾼의 음식', '[어제보다 행복하기]\\xa0비움의 행복', '뮤지션 #천용성 의 두 번째 목소리', '주문을 걸어 (원슈타인 화보 & 인터뷰)', \"'가상화폐, 문제는 다 文 정부 때문'이라는 언론\"]\n",
      "\n",
      "\n",
      "네이버 뉴스 링크 : \n",
      " ['http://www.nocutnews.co.kr/news/4655459', 'http://www.newscj.com/news/articleView.html?idxno=788375', 'http://www.elle.co.kr/article/56416', 'http://www.wkorea.com/2020/12/24/%ec%a3%bc%eb%ac%b8%ec%9d%84-%ea%b1%b8%ec%96%b4-%ec%9b%90%ec%8a%88%ed%83%80%ec%9d%b8/?utm_source=naver&utm_medium=partnership', 'http://www.nocutnews.co.kr/news/4910068']\n",
      "\n",
      "\n",
      "네이버 뉴스 score : \n",
      " [0.6721777319908142, 0.6721777319908142, 0.6721777319908142, 0.4884408414363861, 0.05930577218532562]\n",
      "\n",
      "\n",
      "SNU 기사 제목 : \n",
      " ['관련 뉴스 결과가 없습니다.']\n",
      "\n",
      "\n",
      "SNU 기사 score : \n",
      " ['x']\n",
      "\n",
      "\n",
      "인기키워드 랭킹 : \n",
      " ['안녕하세요', '유료', '카카오', '25', '코로나', '123123', '카카오톡', 'nameasdfasdfas', '입니다', 'dcxzvdszvzcd']\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "125.133.174.241 - - [17/Sep/2021 04:18:33] \"\u001b[37mPOST /economy HTTP/1.1\u001b[0m\" 200 -\n",
      "125.133.174.241 - - [17/Sep/2021 04:18:36] \"\u001b[37mPOST /politics HTTP/1.1\u001b[0m\" 200 -\n",
      "125.133.174.241 - - [17/Sep/2021 04:18:38] \"\u001b[37mPOST /society HTTP/1.1\u001b[0m\" 200 -\n",
      "125.133.174.241 - - [17/Sep/2021 04:18:43] \"\u001b[37mPOST /others HTTP/1.1\u001b[0m\" 200 -\n",
      "125.133.174.241 - - [17/Sep/2021 04:18:44] \"\u001b[37mPOST /others HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
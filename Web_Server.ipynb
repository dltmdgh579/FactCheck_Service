{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "welcome-projection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extension horovod.torch has not been built: /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-37m-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n",
      "[2021-10-08 08:01:27.967 ip-172-31-32-76:1571 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-10-08 08:01:28.042 ip-172-31-32-76:1571 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    }
   ],
   "source": [
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import logging\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from gensim.models import FastText\n",
    "from konlpy.tag import Okt\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "SNU_model_All = load_model('SNU_LSTM_Model_All_balance.h5')\n",
    "SNU_model_pol = load_model('SNU_LSTM_Model_정치_balance.h5')\n",
    "SNU_model_eco = load_model('SNU_LSTM_Model_경제_balance.h5')\n",
    "SNU_model_soc = load_model('SNU_LSTM_Model_사회_balance.h5')\n",
    "SNU_model_others = load_model('SNU_LSTM_Model_기타_balance.h5')\n",
    "\n",
    "Naver_Comments_All = load_model('Naver_Comments_Model_All.h5')\n",
    "Naver_Comments_pol = load_model('Naver_Comments_Model_정치.h5')\n",
    "Naver_Comments_eco = load_model('Naver_Comments_Model_경제.h5')\n",
    "Naver_Comments_soc = load_model('Naver_Comments_Model_사회.h5')\n",
    "Naver_Comments_others = load_model('Naver_Comments_Model_기타.h5')\n",
    "\n",
    "kakao_model = load_model('kakao_LSTM_model.h5')\n",
    "# SNU_model_shuffle = load_model('SNU_LSTM_Model_shuffle.h5')\n",
    "similar_model = FastText.load('similar_keyword_model_SNU_Naver')\n",
    "\n",
    "week_time = datetime(2021,10,1)\n",
    "day_time = datetime(2021,10,1)\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        clean_words = []  # 정제된 단어 리스트\n",
    "        for word in pos:\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\n",
    "                    #if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\n",
    "                    clean_words.append(word[0])\n",
    "        return clean_words\n",
    "\n",
    "\n",
    "my_Tokenizer = MyTokenizer(Okt())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legislative-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "host = 'database-1.cze9hohzw2wl.ap-northeast-2.rds.amazonaws.com' # rds endpoint\n",
    "port = 3306\n",
    "username = 'admin'\n",
    "database = 'factcheck'\n",
    "password = 'rnrqhTmfprl'\n",
    "\n",
    "def connect_RDS(host, port, username, password, database):\n",
    "    try:\n",
    "        conn = pymysql.connect(host = host, user = username, password = password, db = database, port = port, use_unicode = True, charset = 'utf8')\n",
    "        cursor = conn.cursor()\n",
    "    \n",
    "    except:\n",
    "        logging.error('RDS에 연결되지 않았습니다.')\n",
    "        sys.exit(1)\n",
    "        \n",
    "    return conn, cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hidden-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.version_info[0] < 3:\n",
    "    from io import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = 'AKIA2EDEFCNPX2G7FWNY'\n",
    "aws_secret = 'Xt1EJXPsRdI27VI7TBSCsRMNJWsewq9FY0g4vDU7'\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'snucsv'\n",
    "label_names = [\"label\"]\n",
    "\n",
    "def s3_load(category):\n",
    "    object_key = category\n",
    "    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('cp949')\n",
    "\n",
    "    # load data\n",
    "    train_df = pd.read_csv(StringIO(csv_string))\n",
    "    y_train = train_df[label_names].values\n",
    "\n",
    "    train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\n",
    "    max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "    \n",
    "    return train_df, y_train, max_seq_len\n",
    "\n",
    "def s3_Comments_load(category):\n",
    "    object_key = category\n",
    "    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "\n",
    "    # load data\n",
    "    train_df = pd.read_csv(StringIO(csv_string))\n",
    "    y_train = train_df[label_names].values\n",
    "\n",
    "    train_df['doc_len'] = train_df['Comments'].apply(lambda words: len(words.split(\" \")))\n",
    "    max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "    \n",
    "    return train_df, y_train, max_seq_len\n",
    "\n",
    "train_df_All_summary, y_train_All_summary, max_seq_len_summary = s3_load('SNU_All_summary.csv')\n",
    "\n",
    "train_df_All_, y_train_All_, max_seq_len = s3_load('SNU_All_.csv')\n",
    "train_df_All_b, y_train_All_b, max_seq_len_All_b = s3_load('SNU_All_b.csv')\n",
    "train_df_eco, y_train_eco, max_seq_len_eco = s3_load('SNU_경제_b.csv')\n",
    "train_df_pol, y_train_pol, max_seq_len_pol = s3_load('SNU_정치_b.csv')\n",
    "train_df_soc, y_train_soc, max_seq_len_soc = s3_load('SNU_사회_b.csv')\n",
    "train_df_others, y_train_others, max_seq_len_others = s3_load('SNU_기타_b.csv')\n",
    "\n",
    "train_df_Comments_All, y_train_Comments_All, max_seq_len_Comments = s3_Comments_load('Naver_Comments_All.csv')\n",
    "train_df_Comments_pol, y_train_Comments_pol, max_seq_len_Comments_pol = s3_Comments_load('Naver_Comments_정치.csv')\n",
    "train_df_Comments_eco, y_train_Comments_eco, max_seq_len_Comments_eco = s3_Comments_load('Naver_Comments_경제.csv')\n",
    "train_df_Comments_soc, y_train_Comments_soc, max_seq_len_Comments_soc = s3_Comments_load('Naver_Comments_사회.csv')\n",
    "train_df_Comments_others, y_train_Comments_All, max_seq_len_Comments_others = s3_Comments_load('Naver_Comments_기타.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "theoretical-defeat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>link</th>\n",
       "      <th>summary</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>코로나로 하루 평균 1,000개 이상 점포가 문을 닫는다\\r\\n</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3347</td>\n",
       "      <td>\\n            이재명 더불어민주당 대선 경선 후보는 지난 15일 페이스북...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>한반도 기온상승이 전 세계 상승보다 2배 심하다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3344</td>\n",
       "      <td>\\n            추미애 더불어민주당 대선 경선 후보는 지난 8월 3호 공약...</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>셧다운제가 폐지되면 청소년들이 제한없이 게임을 즐길 수 있다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3351</td>\n",
       "      <td>\\n            청소년들의 '게임할 자유'를 침해한다는 이유로 비판이 제기...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>한국전력의 전기료 인상은 '급격한 탈원전' 탓이다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3334</td>\n",
       "      <td>\\n            정부가 지난 23일, 4분기 전기요금 인상을 결정했습니다....</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>경찰이 무력으로 백신 강제 접종할 수 있다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3340</td>\n",
       "      <td>\\n            지난 9월 10일 한 블로그에 방호복을 입은 사람들이 문을...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>자유한국당 홍준표 대선후보가 경선 토론회에서 \"민주당 문재인 경선후보는 세월호 선사...</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/15</td>\n",
       "      <td>\\n            홍준표 후보의 주장은?문재인 민주당 경선후보가 세월호 선사...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>\"홍준표 지사가 국회 운영위원장 시절, 판공비 일부를 집에서 쓰게 한 것은 문제다....</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/10</td>\n",
       "      <td>\\n            3월28일 자유한국당 대선후보 TV 토론회에서 김진태 의원...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>Q: 자유한국당 홍준표 경남지사가 위안부 합의에 대해 \"대통령이 되면 합의를 파기할...</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/9</td>\n",
       "      <td>\\n            재작년인 2015년 12월 28일 한일 양국 외교장관이 공...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>세월호 희생자들이 천안함 희생자들보다 보상을 많이 받았다는 주장과 세월호 관련 법안...</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/8</td>\n",
       "      <td>\\n            세월호 인양 이후 세월호 유가족 보상금에 대한루머들이 SN...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>“77%로 미군 주둔비 부담 커…일본은 50%”</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/2</td>\n",
       "      <td>\\n            이재명 시장은 미국 도널드 트럼프 대통령이 대선 유세 과정...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2394 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               document  \\\n",
       "0                   코로나로 하루 평균 1,000개 이상 점포가 문을 닫는다\\r\\n   \n",
       "1                            한반도 기온상승이 전 세계 상승보다 2배 심하다   \n",
       "2                     셧다운제가 폐지되면 청소년들이 제한없이 게임을 즐길 수 있다   \n",
       "3                           한국전력의 전기료 인상은 '급격한 탈원전' 탓이다   \n",
       "4                               경찰이 무력으로 백신 강제 접종할 수 있다   \n",
       "...                                                 ...   \n",
       "2389  자유한국당 홍준표 대선후보가 경선 토론회에서 \"민주당 문재인 경선후보는 세월호 선사...   \n",
       "2390  \"홍준표 지사가 국회 운영위원장 시절, 판공비 일부를 집에서 쓰게 한 것은 문제다....   \n",
       "2391  Q: 자유한국당 홍준표 경남지사가 위안부 합의에 대해 \"대통령이 되면 합의를 파기할...   \n",
       "2392  세월호 희생자들이 천안함 희생자들보다 보상을 많이 받았다는 주장과 세월호 관련 법안...   \n",
       "2393                         “77%로 미군 주둔비 부담 커…일본은 50%”   \n",
       "\n",
       "                                           link  \\\n",
       "0     https://factcheck.snu.ac.kr/v2/facts/3347   \n",
       "1     https://factcheck.snu.ac.kr/v2/facts/3344   \n",
       "2     https://factcheck.snu.ac.kr/v2/facts/3351   \n",
       "3     https://factcheck.snu.ac.kr/v2/facts/3334   \n",
       "4     https://factcheck.snu.ac.kr/v2/facts/3340   \n",
       "...                                         ...   \n",
       "2389    https://factcheck.snu.ac.kr/v2/facts/15   \n",
       "2390    https://factcheck.snu.ac.kr/v2/facts/10   \n",
       "2391     https://factcheck.snu.ac.kr/v2/facts/9   \n",
       "2392     https://factcheck.snu.ac.kr/v2/facts/8   \n",
       "2393     https://factcheck.snu.ac.kr/v2/facts/2   \n",
       "\n",
       "                                                summary  label  doc_len  \n",
       "0     \\n            이재명 더불어민주당 대선 경선 후보는 지난 15일 페이스북...      1        8  \n",
       "1     \\n            추미애 더불어민주당 대선 경선 후보는 지난 8월 3호 공약...      1        7  \n",
       "2     \\n            청소년들의 '게임할 자유'를 침해한다는 이유로 비판이 제기...      0        8  \n",
       "3     \\n            정부가 지난 23일, 4분기 전기요금 인상을 결정했습니다....      0        6  \n",
       "4     \\n            지난 9월 10일 한 블로그에 방호복을 입은 사람들이 문을...      0        7  \n",
       "...                                                 ...    ...      ...  \n",
       "2389  \\n            홍준표 후보의 주장은?문재인 민주당 경선후보가 세월호 선사...      0       14  \n",
       "2390  \\n            3월28일 자유한국당 대선후보 TV 토론회에서 김진태 의원...      1       12  \n",
       "2391  \\n            재작년인 2015년 12월 28일 한일 양국 외교장관이 공...      1       13  \n",
       "2392  \\n            세월호 인양 이후 세월호 유가족 보상금에 대한루머들이 SN...      0       16  \n",
       "2393  \\n            이재명 시장은 미국 도널드 트럼프 대통령이 대선 유세 과정...      0        6  \n",
       "\n",
       "[2394 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_All_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-blake",
   "metadata": {},
   "source": [
    "**단어 벡터화 및 패딩**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inside-walter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1441 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1441/1441 [00:08<00:00, 165.91it/s]\n",
      " 13%|█▎        | 64/509 [00:00<00:00, 637.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509/509 [00:00<00:00, 610.00it/s]\n",
      " 16%|█▋        | 80/490 [00:00<00:00, 799.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 490/490 [00:00<00:00, 638.54it/s]\n",
      " 32%|███▏      | 63/198 [00:00<00:00, 329.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [00:00<00:00, 529.60it/s]\n",
      " 34%|███▍      | 87/257 [00:00<00:00, 866.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [00:00<00:00, 802.56it/s]\n",
      "  0%|          | 3/9615 [00:00<06:36, 24.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9615/9615 [01:06<00:00, 143.68it/s]\n",
      "  0%|          | 3/3478 [00:00<02:45, 21.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3478/3478 [00:28<00:00, 121.71it/s]\n",
      "  0%|          | 6/1231 [00:00<00:27, 45.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1231/1231 [00:10<00:00, 119.04it/s]\n",
      "  1%|          | 21/2934 [00:00<00:22, 129.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2934/2934 [00:25<00:00, 113.37it/s]\n",
      "  1%|          | 16/1537 [00:00<00:16, 93.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1537/1537 [00:12<00:00, 123.61it/s]\n"
     ]
    }
   ],
   "source": [
    "def embedding_padding(train_df):\n",
    "    raw_docs_train = train_df['document'].tolist()\n",
    "    num_classes = len(label_names)\n",
    "    print(num_classes)\n",
    "    processed_docs_train = []\n",
    "\n",
    "    for doc in tqdm(raw_docs_train):\n",
    "        tokens = my_Tokenizer(doc)\n",
    "        processed_docs_train.append(tokens)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "    tokenizer.fit_on_texts(processed_docs_train)\n",
    "    return tokenizer\n",
    "\n",
    "def Comments_embedding_padding(train_df):\n",
    "    raw_docs_train = train_df['Comments'].tolist()\n",
    "    num_classes = len(label_names)\n",
    "    print(num_classes)\n",
    "    processed_docs_train = []\n",
    "\n",
    "    for doc in tqdm(raw_docs_train):\n",
    "        tokens = my_Tokenizer(doc)\n",
    "        processed_docs_train.append(tokens)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "    tokenizer.fit_on_texts(processed_docs_train)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer_All = embedding_padding(train_df_All_b)\n",
    "tokenizer_pol = embedding_padding(train_df_pol)\n",
    "tokenizer_soc = embedding_padding(train_df_soc)\n",
    "tokenizer_eco = embedding_padding(train_df_eco)\n",
    "tokenizer_others = embedding_padding(train_df_others)\n",
    "\n",
    "tokenizer_Comments_All = Comments_embedding_padding(train_df_Comments_All)\n",
    "tokenizer_Comments_pol = Comments_embedding_padding(train_df_Comments_pol)\n",
    "tokenizer_Comments_eco = Comments_embedding_padding(train_df_Comments_eco)\n",
    "tokenizer_Comments_soc = Comments_embedding_padding(train_df_Comments_soc)\n",
    "tokenizer_Comments_others = Comments_embedding_padding(train_df_Comments_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "disturbed-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_seq_train = tokenizer_All.texts_to_sequences(processed_docs_train)\n",
    "# word_index = tokenizer_All.word_index\n",
    "# print('dictionary size :', len(word_index))\n",
    "\n",
    "# word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "# word_seq_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-apache",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if sys.path[0] == '':\n",
      "  3%|▎         | 4707/145393 [00:23<11:24, 205.61it/s]"
     ]
    }
   ],
   "source": [
    "# 카카오 응답 데이터 전처리\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "text = 'do!!! you expect... people~ to~ read~ the FAQ, etc. and actually accept hard~! atheism?@@'\n",
    "re.sub(r'[^a-zA-Z ]', '', text) #알파벳과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")# 한글과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "okt = Okt()\n",
    "X_train = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)\n",
    "\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(X_train)\n",
    "\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer2.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer2.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "tokenizer_kakao = Tokenizer(vocab_size) \n",
    "tokenizer_kakao.fit_on_texts(X_train)\n",
    "X_train = tokenizer_kakao.texts_to_sequences(X_train)\n",
    "\n",
    "max_len = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-newport",
   "metadata": {},
   "source": [
    "**사용자에게 받은 데이터 전처리 테스트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-champagne",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# response = ['게르마늄 팔찌, 과학적으로 효능 있을까?', '빌려쓰는 IT서비스 클라우드컴퓨팅, 실제 비용절감 효과 있나', '\"심장마비 왔을 때 강하게 기침하면 막을 수 있다\"\t']\n",
    "# tokens_response = []\n",
    "\n",
    "# for i in range(len(response)):\n",
    "#     tokens = my_Tokenizer(response[i])\n",
    "#     tokens_response.append(tokens)\n",
    "# print(tokens_response)\n",
    "\n",
    "# word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "# print(word_seq_response)\n",
    "# word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "# print(word_seq_response.shape)\n",
    "\n",
    "# score_list = []\n",
    "# for sco in range(len(word_seq_response)):\n",
    "#     word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "#     print(word_seq_response_to_score)\n",
    "#     score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\n",
    "# print(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-activation",
   "metadata": {},
   "source": [
    "**입력 데이터 처리 함수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mental-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================================================================\n",
    "# 카카오톡 응답 데이터 score\n",
    "\n",
    "def kakao_data_score(response):\n",
    "    objectivity = ''\n",
    "    response = okt.morphs(response, stem=True) # 토큰화\n",
    "    response = [word for word in response if not word in stopwords] # 불용어 제거\n",
    "    encoded = tokenizer_kakao.texts_to_sequences([response]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "    score = float(kakao_model.predict(pad_new)) # 예측\n",
    "    if(score > 0.8 or score < 0.2):\n",
    "        objectivity = '객관성이 매우 낮습니다.'\n",
    "    elif(score > 0.65 or score < 0.35):\n",
    "        objectivity = '객관성이 낮습니다.'\n",
    "    else:\n",
    "        objectivity = '객관성이 높습니다.'\n",
    "    \n",
    "    return objectivity\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 단어 토큰화 및 빈도순 정렬 후 추출\n",
    "\n",
    "def tfidf_Vectorizer(response):\n",
    "    response_list = []\n",
    "    response_list.append(response)\n",
    "    tfidf_Vectorizer = TfidfVectorizer(tokenizer=my_Tokenizer, min_df=1) # df 값(단어가 몇 문장들에서 등장하였는지)을 최소 'min_df' 값으로 설정.\n",
    "    X = tfidf_Vectorizer.fit_transform(response_list).toarray()\n",
    "#     print(X.shape)    # X(2차원 배열)의 행,열 수를 출력.\n",
    "#     print(tfidf_Vectorizer.vocabulary_)   # 각 단어들이 배열 X에서 몇번째 열(인덱스 값)에 해당하는지 출력.\n",
    "\n",
    "\n",
    "    #pandas를 활용하여 각 단어들의 각 문장에서의 tf-idf 값들을 모두 더하고, 내림차순으로 정렬하여 상위 n개 출력\n",
    "    count = X.sum(axis=0)    # 2차원 배열 X에서 각 열을 기준으로 합을 구함. (각 단어들의 '최종' tf-idf 값으로 간주.)\n",
    "    word_count = pd.DataFrame({\n",
    "        '단어' : tfidf_Vectorizer.get_feature_names(),\n",
    "        '빈도' : count.flat\n",
    "    })\n",
    "    sorted_df = word_count.sort_values('빈도', ascending=False)\n",
    "#     print(sorted_df.head(10), \"\\n\")\n",
    "\n",
    "    word_ = list(np.array(sorted_df['단어'].tolist()))\n",
    "    return(word_)\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 네이버 뉴스 분석\n",
    "\n",
    "def Naver_Score(response, tokenizer, tokenizer_Comments, SNU_Model, Naver_Comments_Model):\n",
    "    response = re.compile(\"[^A-Za-z0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]+\").sub('',response)\n",
    "    response = okt.morphs(response, stem=True) # 토큰화\n",
    "    response = [word for word in response if not word in stopwords] # 불용어 제거\n",
    "    response = ' '.join(response)\n",
    "    start = time.time()\n",
    "    word_ = tfidf_Vectorizer(response)\n",
    "    word__ = \"\"\n",
    "    if len(word_) <= 10:\n",
    "        word__ = response\n",
    "    else:\n",
    "        for i in range(1, min(len(word_), 11)):\n",
    "            word__ += word_[i]\n",
    "            word__ += \" \"\n",
    "    \n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}\n",
    "    baseurl = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query='\n",
    "    url = baseurl + urllib.parse.quote_plus(word__) \n",
    "\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    title_ = soup.find_all(class_ = 'news_tit')\n",
    "\n",
    "    title_list = []\n",
    "    summary_list = []\n",
    "    link_list = []\n",
    "    \n",
    "    for i in title_:\n",
    "        title_list.append(i.attrs['title'])\n",
    "        link_list.append(i.attrs['href'])\n",
    "        if len(title_list) == 5:\n",
    "            break\n",
    "\n",
    "    for i in range(1,30):\n",
    "        try:\n",
    "            summary = soup.select_one('#sp_nws{} > div > div > div.news_dsc > div > a'.format(i)).get_text()\n",
    "            summary_list.append(summary)\n",
    "            if len(summary_list) == 5:\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    end = time.time()\n",
    "    print(f\"{end-start:.5f} sec__\")\n",
    "\n",
    "# ================================================================\n",
    "\n",
    "    tokens_response = []\n",
    "    SNU_score_list = []\n",
    "    \n",
    "#     start1 = time.time()\n",
    "\n",
    "    for i in range(len(title_list)):\n",
    "        tokens = my_Tokenizer(title_list[i])\n",
    "        tokens_response.append(tokens)\n",
    "\n",
    "    word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "    word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "\n",
    "    for sco in range(len(title_list)):\n",
    "        word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "        SNU_score_list.append(float(SNU_Model.predict(word_seq_response_to_score)))\n",
    "        \n",
    "    df_qq = []\n",
    "\n",
    "    allComments = []\n",
    "    def create_soup(url):\n",
    "        res = requests.get(url, headers=headers)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        return soup\n",
    "\n",
    "    Comments_score_list = []\n",
    "    # 네이버 뉴스 url을 입력합니다.\n",
    "    url1 = \"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}\".format(word__)\n",
    "    soup = create_soup(url1)\n",
    "    url_check = []\n",
    "    for i in range(1, 30):\n",
    "        List = []\n",
    "        try:\n",
    "            url = soup.select_one(\"#sp_nws{} > div.news_wrap.api_ani_send > div > div.news_info > div.info_group > a:nth-of-type(2)\".format(i))['href']\n",
    "            url_check.append(url)\n",
    "            oid = url.split(\"oid=\")[1].split(\"&\")[0] #422\n",
    "            aid = url.split(\"aid=\")[1] #0000430957\n",
    "            page = 1\n",
    "            header = {\n",
    "                \"User-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\",\n",
    "                \"referer\": url,\n",
    "            }\n",
    "            # time.sleep(random.randint(1,2))\n",
    "            \n",
    "            start = time.time()\n",
    "\n",
    "            while True:\n",
    "                c_url = \"https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json?ticket=news&templateId=default_society&pool=cbox5&_callback=jQuery1707138182064460843_1523512042464&lang=ko&country=&objectId=news\" + oid + \"%2C\" + aid + \"&categoryId=&pageSize=20&indexSize=10&groupId=&listType=OBJECT&pageType=more&page=\" + str(\n",
    "                    page) + \"&refresh=false&sort=FAVORITE\"\n",
    "                # 파싱하는 단계입니다.\n",
    "                r = requests.get(c_url, headers=header)\n",
    "                cont = BeautifulSoup(r.content, \"html.parser\")\n",
    "                total_comm = str(cont).split('comment\":')[1].split(\",\")[0]\n",
    "\n",
    "                match = re.findall('\"contents\":([^\\*]*),\"userIdNo\"', str(cont))\n",
    "                # 댓글을 리스트에 중첩합니다.\n",
    "                try:\n",
    "                    for b in range(0,3):\n",
    "                        List.append(match[b])\n",
    "                except:\n",
    "                    pass\n",
    "                break\n",
    "            \n",
    "            end = time.time()\n",
    "            print(f\"for문 한번 댓글 크롤링 {end-start:.5f} sec\")\n",
    "\n",
    "            tokens_response = []\n",
    "            score_list = []\n",
    "            \n",
    "            start = time.time()\n",
    "\n",
    "            for i in range(len(List)):\n",
    "                tokens = my_Tokenizer(List[i])\n",
    "                tokens_response.append(tokens)\n",
    "\n",
    "            word_seq_response2 = tokenizer_Comments.texts_to_sequences(tokens_response)\n",
    "            word_seq_response2 = sequence.pad_sequences(word_seq_response2, maxlen=max_seq_len_Comments)\n",
    "\n",
    "            for sco in range(len(List)):\n",
    "                word_seq_response_to_score2 = word_seq_response2[sco].reshape(1,max_seq_len_Comments)\n",
    "                score_list.append(float(Naver_Comments_Model.predict(word_seq_response_to_score2)))\n",
    "                \n",
    "            end = time.time()\n",
    "            print(f\"for문 한번 댓글 분석 {end-start:.5f} sec\")\n",
    "\n",
    "            Comments_score_list.append(np.mean(score_list))\n",
    "            \n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        if len(url_check) == len(title_list):\n",
    "            break\n",
    "    \n",
    "    end = time.time()\n",
    "#     print(f\"{end-start:.5f} sec__\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    print(SNU_score_list)\n",
    "    print(Comments_score_list)\n",
    "    a = [score * 0.5 for score in SNU_score_list]\n",
    "#     b = [score * 0.5 if math.isnan(score)==False else score2 * 0.5 for score2 in SNU_score_list for score in Comments_score_list]\n",
    "    b = [score * 0.5 for score in Comments_score_list]\n",
    "    for i in range(len(b)):\n",
    "        if math.isnan(b[i]):\n",
    "            b[i] = a[i]\n",
    "            \n",
    "    if len(b) < len(a):\n",
    "        len_b = len(b)\n",
    "        for i in range(len(a)-len(b)):\n",
    "            b.append(a[i+len_b])\n",
    "    elif len(a) < len(b):\n",
    "        len_a = len(a)\n",
    "        for i in range(len(b)-len(a)):\n",
    "            a.append(b[i+len_a])\n",
    "        \n",
    "    score_list = list(map(lambda x,y:x+y, a,b))\n",
    "    print('-'*50)\n",
    "    print(a)\n",
    "    print(b)\n",
    "    print('-'*50)\n",
    "    print(score_list)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"{end-start:.5f} sec__\")\n",
    "    \n",
    "    return title_list, link_list, summary_list, score_list\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 연관키워드 추출\n",
    "\n",
    "def similar_keyword(word_):\n",
    "    similar_list = []\n",
    "\n",
    "    if len(word_) == 1:\n",
    "        similar_keywords = similar_model.wv.most_similar(word_[0])\n",
    "\n",
    "        for i in range(len(similar_keywords)):\n",
    "            similar_list.append(similar_keywords[i][0])\n",
    "    return similar_list\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# MySQL 데이터베이스에 응답데이터, 시간 저장\n",
    "\n",
    "def MySQL_data_save(response):\n",
    "    rank_list = []\n",
    "    global cursor, conn\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "\n",
    "    now = datetime.now()\n",
    "    Korea_DatetimeNow = now + timedelta(hours = 9)\n",
    "    DatetimeNow = Korea_DatetimeNow.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    global day_time\n",
    "    if day_time < now:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"UPDATE article_data SET count = 0\")\n",
    "            conn.commit()\n",
    "        day_time = day_time + timedelta(days=1)\n",
    "    \n",
    "#     try:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('SELECT article_data FROM factcheck.article_data')\n",
    "        row = cursor.fetchall()\n",
    "\n",
    "    for i in range(len(row)):\n",
    "        rank_list.append(row[i][0])\n",
    "\n",
    "    with conn.cursor() as cursor:\n",
    "        if response in rank_list:\n",
    "            query = \"UPDATE factcheck.article_data SET count = count + 1 WHERE article_data = (%s)\"\n",
    "            val = (response)\n",
    "            cursor.execute(query, val)\n",
    "            conn.commit()\n",
    "        else:\n",
    "            query = \"INSERT INTO factcheck.article_data (article_data, date, count) VALUES(%s, %s, 1)\"\n",
    "            val = (response, DatetimeNow)\n",
    "            cursor.execute(query, val)\n",
    "            conn.commit()\n",
    "\n",
    "    top1_list = []\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(\"SELECT article_data FROM factcheck.article_data ORDER BY count DESC LIMIT 1\")\n",
    "        row_top = cursor.fetchall()\n",
    "        for i in range(len(row_top)):\n",
    "            top1_list.append(row_top[i][0])\n",
    "\n",
    "    print(top1_list)\n",
    "        \n",
    "#     finally:\n",
    "#         conn.close()\n",
    "        \n",
    "def MySQL_data_save_kakao(response):\n",
    "    rank_list = []\n",
    "    global cursor, conn\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "\n",
    "    now = datetime.now()\n",
    "    Korea_DatetimeNow = now + timedelta(hours = 9)\n",
    "    DatetimeNow = Korea_DatetimeNow.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    global day_time\n",
    "    if day_time < now:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"UPDATE kakao_data SET count = 0\")\n",
    "            conn.commit()\n",
    "        day_time = day_time + timedelta(days=1)\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute('SELECT kakao_data FROM factcheck.kakao_data')\n",
    "            row = cursor.fetchall()\n",
    "\n",
    "        for i in range(len(row)):\n",
    "            rank_list.append(row[i][0])\n",
    "\n",
    "        with conn.cursor() as cursor:\n",
    "            if response in rank_list:\n",
    "                query = \"UPDATE factcheck.kakao_data SET count = count + 1 WHERE kakao_data = (%s)\"\n",
    "                val = (response)\n",
    "                cursor.execute(query, val)\n",
    "                conn.commit()\n",
    "            else:\n",
    "                query = \"INSERT INTO factcheck.kakao_data (kakao_data, date, count) VALUES(%s, %s, 1)\"\n",
    "                val = (response, DatetimeNow)\n",
    "                cursor.execute(query, val)\n",
    "                conn.commit()\n",
    "                \n",
    "        top1_list = []\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT kakao_data FROM factcheck.kakao_data ORDER BY count DESC LIMIT 1\")\n",
    "            row_top = cursor.fetchall()\n",
    "            for i in range(len(row_top)):\n",
    "                top1_list.append(row_top[i][0])\n",
    "\n",
    "        print(top1_list)\n",
    "        \n",
    "    finally:\n",
    "        conn.close()\n",
    "        \n",
    "        \n",
    "# ===========================================================================================================================\n",
    "# 하루동안 가장 많이 검색된 뉴스  \n",
    "\n",
    "def day_top1(response):\n",
    "    rank_list = []\n",
    "    global cursor, conn\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "    try:         \n",
    "        top1_list = []\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT article_data FROM factcheck.article_data ORDER BY count DESC LIMIT 1\")\n",
    "            row_top = cursor.fetchall()\n",
    "            for i in range(len(row_top)):\n",
    "                top1_list.append(row_top[i][0])\n",
    "        \n",
    "    finally:\n",
    "        conn.close()\n",
    "    return top1_list[0]\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# MySQL 데이터베이스에 키워드 랭킹 반영\n",
    "\n",
    "def keyword_rank(word_):\n",
    "    rank_list = []\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    Korea_DatetimeNow = now + timedelta(hours = 9)\n",
    "    DatetimeNow = Korea_DatetimeNow.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute('SELECT keyword FROM keyword_rank')\n",
    "            row = cursor.fetchall()\n",
    "\n",
    "        for i in range(len(row)):\n",
    "            rank_list.append(row[i][0])\n",
    "            \n",
    "        global week_time\n",
    "        if week_time < now:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(\"UPDATE keyword_rank SET count = 0\")\n",
    "                conn.commit()\n",
    "            week_time = week_time + timedelta(days=7)\n",
    "\n",
    "        with conn.cursor() as cursor:\n",
    "            for i in range(0,2):\n",
    "                if word_[i] in rank_list:\n",
    "                    query = \"UPDATE keyword_rank SET count = count + 1 WHERE keyword = (%s)\"\n",
    "                    val = (word_[i])\n",
    "                    cursor.execute(query, val)\n",
    "                    conn.commit()\n",
    "                else:\n",
    "                    query = \"INSERT INTO factcheck.keyword_rank (keyword, date, count) VALUES(%s, %s, 1)\"\n",
    "                    val = (word_[i], DatetimeNow)\n",
    "                    cursor.execute(query, val)\n",
    "                    conn.commit()\n",
    "\n",
    "        top10_list = []\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT keyword FROM keyword_rank ORDER BY count DESC LIMIT 10\")\n",
    "            row_top = cursor.fetchall()\n",
    "            for i in range(len(row_top)):\n",
    "                top10_list.append(row_top[i][0])\n",
    "\n",
    "        print(top10_list)\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "    return top10_list\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 키워드 랭킹 조회\n",
    "\n",
    "def keyword_rank_():\n",
    "    rank_list = []\n",
    "    conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute('SELECT keyword FROM keyword_rank')\n",
    "            row = cursor.fetchall()\n",
    "\n",
    "        for i in range(len(row)):\n",
    "            rank_list.append(row[i][0])\n",
    "\n",
    "        top10_list = []\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT keyword FROM keyword_rank ORDER BY count DESC LIMIT 10\")\n",
    "            row_top = cursor.fetchall()\n",
    "            for i in range(len(row_top)):\n",
    "                top10_list.append(row_top[i][0])\n",
    "\n",
    "    finally:\n",
    "        conn.close()\n",
    "    return top10_list\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# SNU 기사 검색      \n",
    "\n",
    "def SNU_Search(word_):\n",
    "    title_list2 = []\n",
    "    link_list2 = []\n",
    "    summary_list2 = []\n",
    "    score_list2 = []\n",
    "\n",
    "    try:\n",
    "#             str_expr = \"document.str.contains('{}', case=False)\".format(word_[0]) and \"document.str.contains('{}', case=False)\".format(word_[1])\n",
    "        str_expr = \"document.str.contains('{}', case=False)\".format(word_[0])\n",
    "\n",
    "        df_q = train_df_All_summary.query(str_expr, engine=\"python\")\n",
    "\n",
    "        title = list(np.array(df_q['document'].tolist()))\n",
    "        summary = list(np.array(df_q['summary'].tolist()))\n",
    "        score_SNU = list(np.array(df_q['label'].tolist()))\n",
    "        link = list(np.array(df_q['link'].tolist()))\n",
    "\n",
    "        for i in range(len(title)):\n",
    "            title_list2.append(title[i])\n",
    "            link_list2.append(link[i])\n",
    "            summary_list2.append(summary[i])\n",
    "            score_list2.append(score_SNU[i])\n",
    "            if len(title_list2) == 5:\n",
    "                break\n",
    "        for i in range(len(title_list2)):\n",
    "            title_list2[i] = re.compile(\"[^A-Za-z0-9ㄱ-ㅎㅏ-ㅣ가-힣 '' ]+\").sub('',title_list2[i])\n",
    "            summary_list2[i] = re.compile(\"[^A-Za-z0-9ㄱ-ㅎㅏ-ㅣ가-힣 '' ]+\").sub('',summary_list2[i])\n",
    "            summary_list2[i] = summary_list2[i].strip()\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "    score_list2 = list([int(x) for x in score_list2])\n",
    "    return title_list2, score_list2\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 네이버 뉴스 검색\n",
    "\n",
    "def Naver_Search(response, tokenizer, SNU_model):\n",
    "    baseurl = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=\"\n",
    "\n",
    "    url = baseurl + urllib.parse.quote_plus(response)\n",
    "\n",
    "    html = urllib.request.urlopen(url).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    title = soup.find_all(class_ = 'news_tit')\n",
    "\n",
    "    title_list = []\n",
    "    link_list = []\n",
    "\n",
    "    for i in title:\n",
    "        title_list.append(i.attrs['title'])\n",
    "        link_list.append(i.attrs['href'])\n",
    "\n",
    "# ================================================================\n",
    "\n",
    "    tokens_response = []\n",
    "    score_list = []\n",
    "\n",
    "    for i in range(len(title_list)):\n",
    "        tokens = my_Tokenizer(title_list[i])\n",
    "        tokens_response.append(tokens)\n",
    "\n",
    "    word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "    word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "\n",
    "    for sco in range(len(title_list)):\n",
    "        word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "        score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\n",
    "    \n",
    "    return title_list, link_list, score_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-keyboard",
   "metadata": {},
   "source": [
    "**Flask 웹 서버**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-looking",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "\n",
    "from flask import Flask, request\n",
    "import json\n",
    "from urllib import parse\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/politics', methods = ['POST'])\n",
    "def Article_keyword_politics():\n",
    "    if request.method == 'POST':\n",
    "        start_first = time.time()\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        \n",
    "        start = time.time()\n",
    "        word_ = tfidf_Vectorizer(response)\n",
    "        end = time.time()\n",
    "        print(f\"{end-start:.5f} sec\")\n",
    "        \n",
    "        start = time.time()\n",
    "        similar_list = similar_keyword(word_)\n",
    "        end = time.time()\n",
    "        print(f\"{end-start:.5f} sec\")\n",
    "        \n",
    "        start = time.time()\n",
    "        MySQL_data_save(response)\n",
    "        end = time.time()\n",
    "        print(f\"{end-start:.5f} sec\")\n",
    "        \n",
    "        start = time.time()\n",
    "        top10_list = keyword_rank(word_)\n",
    "        end = time.time()\n",
    "        print(f\"{end-start:.5f} sec\")\n",
    "        \n",
    "        start = time.time()\n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        end = time.time()\n",
    "        print(f\"{end-start:.5f} sec\")\n",
    "        \n",
    "#         Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_pol, SNU_model_pol)\n",
    "        start = time.time()\n",
    "        Naver_title, Naver_link, Naver_summary, Naver_score = Naver_Score(response, tokenizer_pol, tokenizer_Comments_pol, SNU_model_pol, Naver_Comments_pol)\n",
    "        end = time.time()\n",
    "        print(f\"{end-start:.5f} sec\")\n",
    "        \n",
    "        start = time.time()\n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "        end = time.time()\n",
    "        print(f\"{end-start:.5f} sec\")\n",
    "            \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        end_last = time.time()\n",
    "        print(end_last-start_first)\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "    \n",
    "@app.route('/economy', methods = ['POST'])\n",
    "def Article_keyword_economy():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        \n",
    "        word_ = tfidf_Vectorizer(response)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_eco, SNU_model_eco)\n",
    "        Naver_title, Naver_link, Naver_summary, Naver_score = Naver_Score(response, tokenizer_eco, tokenizer_Comments_eco, SNU_model_eco, Naver_Comments_eco)\n",
    "        \n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "            \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "    \n",
    "@app.route('/society', methods = ['POST'])\n",
    "def Article_keyword_society():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        \n",
    "        word_ = tfidf_Vectorizer(response)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_soc, SNU_model_soc)\n",
    "        Naver_title, Naver_link, Naver_summary, Naver_score = Naver_Score(response, tokenizer_soc, tokenizer_Comments_soc, SNU_model_soc, Naver_Comments_soc)\n",
    "\n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "            \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "    \n",
    "@app.route('/others', methods = ['POST'])\n",
    "def Article_keyword_others():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        \n",
    "        word_ = tfidf_Vectorizer(response)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_others, SNU_model_others)\n",
    "        Naver_title, Naver_link, Naver_summary, Naver_score = Naver_Score(response, tokenizer_others, tokenizer_Comments_others, SNU_model_others, Naver_Comments_others)\n",
    "\n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "            \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "\n",
    "@app.route('/kakao', methods = ['POST'])\n",
    "def kakao_():\n",
    "    if request.method == 'POST':\n",
    "        params = request.get_data()\n",
    "        params_decode = str(params.decode('utf-8'))\n",
    "        urldecode = parse.unquote(params_decode)\n",
    "        \n",
    "        # 사용자 요청 데이터\n",
    "        response = urldecode.split('=')[1]\n",
    "        \n",
    "        objectivity = kakao_data_score(response)\n",
    "        word_ = tfidf_Vectorizer(response)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save_kakao(response)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "        Naver_title, Naver_link, Naver_score = Naver_Search(response, tokenizer_All, SNU_model_All)\n",
    "#         Naver_title, Naver_link, Naver_summary, Naver_score = Naver_Score(response, tokenizer_All, tokenizer_Comments_All, SNU_model_All, Naver_Comments_All)\n",
    "\n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "        \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'objectivity':objectivity,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        print('입력 데이터 : \\n', response)\n",
    "        print('\\n')\n",
    "        print('객관성 level : \\n', objectivity)\n",
    "        print('\\n')\n",
    "        print('연관 검색어 : \\n', similar_list)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 제목 : \\n', Naver_title)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 링크 : \\n', Naver_link)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 score : \\n', Naver_score)\n",
    "        print('\\n')\n",
    "        try:\n",
    "            print('SNU 기사 제목 : \\n', SNU_title)\n",
    "            print('\\n')\n",
    "    #         print('SNU 기사 링크 : ', link_list2)\n",
    "            print('SNU 기사 score : \\n', SNU_score)\n",
    "            print('\\n')\n",
    "        except:\n",
    "            print('SNU 기사가 없습니다. \\n')\n",
    "        print('인기키워드 랭킹 : \\n', top10_list)\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "    \n",
    "@app.route('/top10', methods = ['POST'])\n",
    "def Top_10_keywords():\n",
    "    if request.method == 'POST':\n",
    "        \n",
    "        top10_list = keyword_rank_()\n",
    "         \n",
    "        data_ = {\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "    \n",
    "@app.route('/top1', methods = ['POST'])\n",
    "def Top_1_data():\n",
    "    if request.method == 'POST':\n",
    "        \n",
    "        day_top1 = day_top1()\n",
    "         \n",
    "        word_ = tfidf_Vectorizer(day_top1)\n",
    "        \n",
    "        similar_list = similar_keyword(word_)\n",
    "        MySQL_data_save(day_top1)\n",
    "        top10_list = keyword_rank(word_)\n",
    "        \n",
    "        SNU_title, SNU_score = SNU_Search(word_)\n",
    "#         Naver_title, Naver_link, Naver_score = Naver_Search(day_top1, tokenizer_eco, SNU_model_eco)\n",
    "        Naver_title, Naver_link, Naver_summary, Naver_score = Naver_Score(day_top1, tokenizer_All, tokenizer_Comments_All, SNU_model_All, Naver_Comments_All)\n",
    "        \n",
    "        if len(similar_list) == 0:\n",
    "            similar_list.append('연관 검색어가 없습니다.')\n",
    "        if len(SNU_title) == 0:\n",
    "            SNU_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(SNU_score) == 0:\n",
    "            SNU_score.append('x')\n",
    "        if len(Naver_title) == 0:\n",
    "            Naver_title.append('관련 뉴스 결과가 없습니다.')\n",
    "        if len(Naver_link) == 0:\n",
    "            Naver_link.append('x')\n",
    "        if len(Naver_score) == 0:\n",
    "            Naver_score.append('x')\n",
    "            \n",
    "        data_ = {\n",
    "            'response':day_top1,\n",
    "            'similar_list':similar_list,\n",
    "            'SNU_title':SNU_title,\n",
    "            'SNU_score':SNU_score,\n",
    "            'Naver_title':Naver_title,\n",
    "            'Naver_link':Naver_link,\n",
    "            'Naver_score':Naver_score,\n",
    "            'top10_list':top10_list\n",
    "            }\n",
    "        \n",
    "        return json.dumps(data_, ensure_ascii=False)\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port = 8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-sullivan",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "religious-newsletter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import logging\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from gensim.models import FastText\n",
    "from konlpy.tag import Okt\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    from io import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = ''\n",
    "aws_secret = ''\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'snucsv'\n",
    "\n",
    "object_key = 'Naver_Comments_All_true.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "\n",
    "# load data\n",
    "train_df_true = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train_true = train_df_true[label_names].values\n",
    "\n",
    "train_df_true['doc_len'] = train_df_true['Comments'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df_true['doc_len'].mean() + train_df_true['doc_len'].std()).astype(int)\n",
    "\n",
    "object_key = 'Naver_Comments_All_fake.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "\n",
    "# load data\n",
    "train_df_fake = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train_fake = train_df_fake[label_names].values\n",
    "\n",
    "train_df_fake['doc_len'] = train_df_fake['Comments'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df_fake['doc_len'].mean() + train_df_fake['doc_len'].std()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "lucky-discovery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"이번에 홍준표 대통령안되면 이나라망한거니 이민간다\"</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"홍준표 속과 겉이 똑같은 사람이라 남 속일줄 모르고 정직하고 정많고 인간적인 인물...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"홍준표처럼 정치잘하면서 사람 좋은 정치인 못봤음. 홍준표는 명품임.\"</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"또라이 아니야?? 순서가 안맞지 유승민 김무성 니들이 탄핵시켰으니 수사 받고 재판...</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"깜냥도 안되는 승민아 네가 대통령 되려고 하니 네가 하는 말이 앞뒤가 안맞지? 차...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4820</th>\n",
       "      <td>\"타다가 비용이 좀 비싸도 의자 열시트부터 에어컨 의자 위치까지 완전 쾌적하고 바로...</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4821</th>\n",
       "      <td>\"기본요금거리 카드안받고욕하고:뭐하러택시타나여\\n버스나탑시당~\"</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4822</th>\n",
       "      <td>\"불법인데 사실 불법으로 내몰리는 사람들 중 가족을 책임져야 하는 사람들이 많다는 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4823</th>\n",
       "      <td>\"사정 급한건알지만 그래도 한댠500만원 준다고\\n에 속지마라\\n한달500만원직장이...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4824</th>\n",
       "      <td>\"저 행동이 잘한건 아니지만 가장이라면 가족의\\n생계를위해서라면 불구덩이라도 들어가...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4825 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Comments  label  doc_len\n",
       "0                         \"이번에 홍준표 대통령안되면 이나라망한거니 이민간다\"      1        5\n",
       "1     \"홍준표 속과 겉이 똑같은 사람이라 남 속일줄 모르고 정직하고 정많고 인간적인 인물...      1       18\n",
       "2               \"홍준표처럼 정치잘하면서 사람 좋은 정치인 못봤음. 홍준표는 명품임.\"      1        8\n",
       "3     \"또라이 아니야?? 순서가 안맞지 유승민 김무성 니들이 탄핵시켰으니 수사 받고 재판...      1       31\n",
       "4     \"깜냥도 안되는 승민아 네가 대통령 되려고 하니 네가 하는 말이 앞뒤가 안맞지? 차...      1       16\n",
       "...                                                 ...    ...      ...\n",
       "4820  \"타다가 비용이 좀 비싸도 의자 열시트부터 에어컨 의자 위치까지 완전 쾌적하고 바로...      1       42\n",
       "4821                \"기본요금거리 카드안받고욕하고:뭐하러택시타나여\\n버스나탑시당~\"      1        2\n",
       "4822  \"불법인데 사실 불법으로 내몰리는 사람들 중 가족을 책임져야 하는 사람들이 많다는 ...      1       22\n",
       "4823  \"사정 급한건알지만 그래도 한댠500만원 준다고\\n에 속지마라\\n한달500만원직장이...      1        8\n",
       "4824  \"저 행동이 잘한건 아니지만 가장이라면 가족의\\n생계를위해서라면 불구덩이라도 들어가...      1        8\n",
       "\n",
       "[4825 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "tough-guidance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"도대체 뭐가 타격이란건지 뻔한 결론인데 저게  백운규를 상대로한 심의위냐 누가봐도...</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"언제부터 수사심의회 결정대로 했냐.한동훈,이성윤 기억안나나,,,,,,참고사항이니 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"이대로 끝난게 아니란거는 니들도 알건데\"</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"정권교체해서 국난과 재앙에서 벗어나자.\"</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"진짜딴건몰라도 부동산만큼은 역대최악 악질정권이다 국민이 호구가아님 판단할텐데 그렇...</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4785</th>\n",
       "      <td>\"직접 국민의 도장받지 않는것은 위헌이다. 지들끼리 짜웅해서 의원을 새로 만들자는 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4786</th>\n",
       "      <td>\"최창렬이 저런 것도 교수가?.......대통령 선거도 표의 등가성을 따지자. 재앙...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4787</th>\n",
       "      <td>\"지금은 법과 상식이 안통하는 시기이니 보수들도 과격하게 투쟁합시다!! 나는 공산당...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4788</th>\n",
       "      <td>\"비례한국당 만든다며??\"</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4789</th>\n",
       "      <td>\"전원사퇴 먼저해라!\"</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4790 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Comments  label  doc_len\n",
       "0     \"도대체 뭐가 타격이란건지 뻔한 결론인데 저게  백운규를 상대로한 심의위냐 누가봐도...      0       18\n",
       "1     \"언제부터 수사심의회 결정대로 했냐.한동훈,이성윤 기억안나나,,,,,,참고사항이니 ...      0        7\n",
       "2                               \"이대로 끝난게 아니란거는 니들도 알건데\"      0        5\n",
       "3                               \"정권교체해서 국난과 재앙에서 벗어나자.\"      0        4\n",
       "4     \"진짜딴건몰라도 부동산만큼은 역대최악 악질정권이다 국민이 호구가아님 판단할텐데 그렇...      0       19\n",
       "...                                                 ...    ...      ...\n",
       "4785  \"직접 국민의 도장받지 않는것은 위헌이다. 지들끼리 짜웅해서 의원을 새로 만들자는 ...      0       27\n",
       "4786  \"최창렬이 저런 것도 교수가?.......대통령 선거도 표의 등가성을 따지자. 재앙...      0       20\n",
       "4787  \"지금은 법과 상식이 안통하는 시기이니 보수들도 과격하게 투쟁합시다!! 나는 공산당...      0       11\n",
       "4788                                     \"비례한국당 만든다며??\"      0        2\n",
       "4789                                       \"전원사퇴 먼저해라!\"      0        2\n",
       "\n",
       "[4790 rows x 3 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "adequate-piano",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"독립유공자 후손지원, 청년수당, 여성안심택배, 올빼미 버스, 서울로 7017, 도...</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"문정부 집값 잡을 생각이 없구나~~~그래 서민들을 우롱했다~~당신들도 가진자들이다...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"부정적으로만 보지말시구여 얼마나 살기가 개같으면 코로나인데도 불구하고 모이는지가 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 뭐? 한반도 운전자론? 중립외교? 양쪽에서 이득을 취해?...</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"성범죄는 그나마 개선된거죠 사기죄는 한결같이 제자리...사기 당한 사람들 보면 거...</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9610</th>\n",
       "      <td>\"어이 기자양반 그럼 분단비용은 싸게 먹힙니까? 돈으로 추산 안되는 한반도 디스카운...</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9611</th>\n",
       "      <td>\"주휴시간 폐지, 최저임금 차등적용, 세금, 4대보험 본인부담 하게 만들어라.\"</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9612</th>\n",
       "      <td>\"연동률 20프로로 낮추어라\"</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9613</th>\n",
       "      <td>\"헛다리에 헛발질 자한당 답다\"</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9614</th>\n",
       "      <td>\"미국영화보면 아동보호 엄격하던데 왜 그리 좋은건 못따라하냐. 미국이란 나라 축소모...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9615 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Comments  label  doc_len\n",
       "0     \"독립유공자 후손지원, 청년수당, 여성안심택배, 올빼미 버스, 서울로 7017, 도...      1       26\n",
       "1     \"문정부 집값 잡을 생각이 없구나~~~그래 서민들을 우롱했다~~당신들도 가진자들이다...      1       18\n",
       "2     \"부정적으로만 보지말시구여 얼마나 살기가 개같으면 코로나인데도 불구하고 모이는지가 ...      0       21\n",
       "3     \"ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ 뭐? 한반도 운전자론? 중립외교? 양쪽에서 이득을 취해?...      0       17\n",
       "4     \"성범죄는 그나마 개선된거죠 사기죄는 한결같이 제자리...사기 당한 사람들 보면 거...      1       41\n",
       "...                                                 ...    ...      ...\n",
       "9610  \"어이 기자양반 그럼 분단비용은 싸게 먹힙니까? 돈으로 추산 안되는 한반도 디스카운...      0       26\n",
       "9611       \"주휴시간 폐지, 최저임금 차등적용, 세금, 4대보험 본인부담 하게 만들어라.\"      0        9\n",
       "9612                                   \"연동률 20프로로 낮추어라\"      0        3\n",
       "9613                                  \"헛다리에 헛발질 자한당 답다\"      0        4\n",
       "9614  \"미국영화보면 아동보호 엄격하던데 왜 그리 좋은건 못따라하냐. 미국이란 나라 축소모...      1       16\n",
       "\n",
       "[9615 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_all = pd.concat([train_df_true, train_df_fake])\n",
    "train_df_all = train_df_all.sample(frac=1, random_state=1004).reset_index(drop=True)\n",
    "train_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "alternative-pulse",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "designing-compilation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "879131it [02:05, 7020.04it/s]\n",
      "  0%|          | 0/9615 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 879130 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9615/9615 [03:39<00:00, 43.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size :  29113\n",
      "number of null word embeddings: 9680\n",
      "sample words not found:  ['학부성적' '딱이라는' '감싸는데서' '캐는게' '넘아' '엠비' '마시던데' '윌급' '챙기는거' '듣는거다']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sys\n",
    "from gensim.models import FastText\n",
    "import os, csv, math, codecs\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "\n",
    "# =========================================================================\n",
    "# Vectorizer의 argument인 tokenizer에 KoNLPy의 pos 함수로 대체.\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        clean_words = []  # 정제된 단어 리스트\n",
    "        for word in pos:\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\n",
    "                    #if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\n",
    "                    clean_words.append(word[0])\n",
    "        return clean_words\n",
    "\n",
    "# =========================================================================\n",
    "\n",
    "embeddings_index = {}\n",
    "f = codecs.open('/home/ubuntu/FastText/wiki.ko.vec', encoding='utf-8')\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('found %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# ==================================================================================\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "my_Tokenizer = MyTokenizer(Okt())\n",
    "\n",
    "raw_docs_train = train_df['Comments'].tolist()\n",
    "# raw_docs_test = test_df['document'].tolist()\n",
    "# print(raw_docs_test)\n",
    "num_classes = len(label_names)\n",
    "\n",
    "processed_docs_train = []\n",
    "# processed_docs_test = []\n",
    "\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = my_Tokenizer(doc)\n",
    "    processed_docs_train.append(tokens)\n",
    "\n",
    "# for doc in tqdm(raw_docs_test):\n",
    "#     tokens = my_Tokenizer(doc)\n",
    "#     processed_docs_test.append(tokens)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "# tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)\n",
    "tokenizer.fit_on_texts(processed_docs_train)\n",
    "# tokenizer.fit_on_texts(processed_docs_train+proccessed_docs_test)\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "# word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size : \", len(word_index))\n",
    "\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "# word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "# ====================================================================================\n",
    "# training params\n",
    "batch_size = 256\n",
    "num_epochs = 40\n",
    "\n",
    "# model parameters\n",
    "num_filters = 64\n",
    "embed_dim = 300\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# embedding matrix\n",
    "\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "\n",
    "# print(word_index)\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "        # cc.ko.300.vec에서 찾지 못한 단어들의 리스트.\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "straight-newsletter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 33, 300)           8734200   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 33, 300)           0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 33, 32)            9632      \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 33, 128)           49664     \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 8,836,825\n",
      "Trainable params: 102,625\n",
      "Non-trainable params: 8,734,200\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "241/241 [==============================] - 61s 78ms/step - loss: 0.6944 - accuracy: 0.4946 - val_loss: 0.6906 - val_accuracy: 0.5424\n",
      "Epoch 2/40\n",
      "241/241 [==============================] - 9s 39ms/step - loss: 0.6921 - accuracy: 0.5197 - val_loss: 0.6853 - val_accuracy: 0.5679\n",
      "Epoch 3/40\n",
      "241/241 [==============================] - 9s 38ms/step - loss: 0.6864 - accuracy: 0.5539 - val_loss: 0.6819 - val_accuracy: 0.5689\n",
      "Epoch 4/40\n",
      "241/241 [==============================] - 9s 37ms/step - loss: 0.6812 - accuracy: 0.5766 - val_loss: 0.6779 - val_accuracy: 0.5777\n",
      "Epoch 5/40\n",
      "241/241 [==============================] - 9s 37ms/step - loss: 0.6739 - accuracy: 0.5804 - val_loss: 0.6762 - val_accuracy: 0.5767\n",
      "Epoch 6/40\n",
      "241/241 [==============================] - 9s 38ms/step - loss: 0.6690 - accuracy: 0.5964 - val_loss: 0.6804 - val_accuracy: 0.5627\n",
      "Epoch 7/40\n",
      "241/241 [==============================] - 9s 37ms/step - loss: 0.6667 - accuracy: 0.5982 - val_loss: 0.6691 - val_accuracy: 0.5881\n",
      "Epoch 8/40\n",
      "241/241 [==============================] - 9s 37ms/step - loss: 0.6502 - accuracy: 0.6139 - val_loss: 0.6650 - val_accuracy: 0.5892\n",
      "Epoch 9/40\n",
      "241/241 [==============================] - 9s 38ms/step - loss: 0.6380 - accuracy: 0.6355 - val_loss: 0.6634 - val_accuracy: 0.5954\n",
      "Epoch 10/40\n",
      "241/241 [==============================] - 9s 37ms/step - loss: 0.6299 - accuracy: 0.6510 - val_loss: 0.6642 - val_accuracy: 0.5970\n",
      "Epoch 11/40\n",
      "241/241 [==============================] - 9s 37ms/step - loss: 0.6162 - accuracy: 0.6626 - val_loss: 0.6610 - val_accuracy: 0.6006\n",
      "Epoch 12/40\n",
      "241/241 [==============================] - 9s 37ms/step - loss: 0.6137 - accuracy: 0.6574 - val_loss: 0.6688 - val_accuracy: 0.5933\n",
      "Epoch 13/40\n",
      "241/241 [==============================] - 9s 37ms/step - loss: 0.5985 - accuracy: 0.6712 - val_loss: 0.6633 - val_accuracy: 0.6053\n",
      "Epoch 14/40\n",
      "241/241 [==============================] - 9s 37ms/step - loss: 0.5918 - accuracy: 0.6876 - val_loss: 0.6719 - val_accuracy: 0.5939\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================================\n",
    "from keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(Embedding(nb_words, embed_dim, input_length=max_seq_len, weights=[embedding_matrix], trainable=False))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "# ================================================================================\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history = model.fit(word_seq_train, y_train, batch_size=32,\n",
    "          epochs=num_epochs, \n",
    "                    validation_split=0.2, \n",
    "                    callbacks=[es_callback], \n",
    "                    shuffle=False)\n",
    "\n",
    "# predictions = model.predict_classes(word_seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "nutritional-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Naver_Comments_Model_All.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

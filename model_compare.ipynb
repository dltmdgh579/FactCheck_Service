{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "solar-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import logging\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from gensim.models import FastText\n",
    "from konlpy.tag import Okt\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import math\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "SNU_model_All_rare = load_model('SNU_LSTM_Model_All.h5')\n",
    "\n",
    "SNU_model_All = load_model('SNU_LSTM_Model_All_balance.h5')\n",
    "SNU_model_pol = load_model('SNU_LSTM_Model_정치_balance.h5')\n",
    "SNU_model_eco = load_model('SNU_LSTM_Model_경제_balance.h5')\n",
    "SNU_model_soc = load_model('SNU_LSTM_Model_사회_balance.h5')\n",
    "SNU_model_others = load_model('SNU_LSTM_Model_기타_balance.h5')\n",
    "\n",
    "Naver_Comments_All = load_model('Naver_Comments_Model_All.h5')\n",
    "Naver_Comments_pol = load_model('Naver_Comments_Model_정치.h5')\n",
    "Naver_Comments_eco = load_model('Naver_Comments_Model_경제.h5')\n",
    "Naver_Comments_soc = load_model('Naver_Comments_Model_사회.h5')\n",
    "Naver_Comments_others = load_model('Naver_Comments_Model_기타.h5')\n",
    "\n",
    "kakao_model = load_model('kakao_LSTM_model.h5')\n",
    "# SNU_model_shuffle = load_model('SNU_LSTM_Model_shuffle.h5')\n",
    "similar_model = FastText.load('similar_keyword_model')\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        clean_words = []  # 정제된 단어 리스트\n",
    "        for word in pos:\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\n",
    "                    #if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\n",
    "                    clean_words.append(word[0])\n",
    "        return clean_words\n",
    "\n",
    "\n",
    "my_Tokenizer = MyTokenizer(Okt())\n",
    "okt = Okt()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "liberal-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.version_info[0] < 3:\n",
    "    from io import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = 'AKIA2EDEFCNPX2G7FWNY'\n",
    "aws_secret = 'Xt1EJXPsRdI27VI7TBSCsRMNJWsewq9FY0g4vDU7'\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'snucsv'\n",
    "label_names = [\"label\"]\n",
    "\n",
    "def s3_load(category):\n",
    "    object_key = category\n",
    "    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('cp949')\n",
    "\n",
    "    # load data\n",
    "    train_df = pd.read_csv(StringIO(csv_string))\n",
    "    y_train = train_df[label_names].values\n",
    "\n",
    "    train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\n",
    "    max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "    \n",
    "    return train_df, y_train, max_seq_len\n",
    "\n",
    "def s3_Comments_load(category):\n",
    "    object_key = category\n",
    "    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('utf-8')\n",
    "\n",
    "    # load data\n",
    "    train_df = pd.read_csv(StringIO(csv_string))\n",
    "    y_train = train_df[label_names].values\n",
    "\n",
    "    train_df['doc_len'] = train_df['Comments'].apply(lambda words: len(words.split(\" \")))\n",
    "    max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "    \n",
    "    return train_df, y_train, max_seq_len\n",
    "\n",
    "train_df_All_rare, y_train_All_rare, max_seq_len_rare = s3_load('SNU_All.csv')\n",
    "\n",
    "train_df_All_, y_train_All_, max_seq_len = s3_load('SNU_All_.csv')\n",
    "train_df_All_b, y_train_All_b, max_seq_len_All_b = s3_load('SNU_All_b.csv')\n",
    "train_df_eco, y_train_eco, max_seq_len_eco = s3_load('SNU_경제_b.csv')\n",
    "train_df_pol, y_train_pol, max_seq_len_pol = s3_load('SNU_정치_b.csv')\n",
    "train_df_soc, y_train_soc, max_seq_len_soc = s3_load('SNU_사회_b.csv')\n",
    "train_df_others, y_train_others, max_seq_len_others = s3_load('SNU_기타_b.csv')\n",
    "\n",
    "train_df_Comments_All, y_train_Comments_All, max_seq_len_Comments = s3_Comments_load('Naver_Comments_All.csv')\n",
    "train_df_Comments_pol, y_train_Comments_pol, max_seq_len_Comments_pol = s3_Comments_load('Naver_Comments_정치.csv')\n",
    "train_df_Comments_eco, y_train_Comments_eco, max_seq_len_Comments_eco = s3_Comments_load('Naver_Comments_경제.csv')\n",
    "train_df_Comments_soc, y_train_Comments_soc, max_seq_len_Comments_soc = s3_Comments_load('Naver_Comments_사회.csv')\n",
    "train_df_Comments_others, y_train_Comments_All, max_seq_len_Comments_others = s3_Comments_load('Naver_Comments_기타.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dated-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_All_rare['label'] = train_df_All_rare['label'].map({'전혀 사실 아님':0, '대체로 사실 아님':0, '사실':1, '대체로 사실':1})\n",
    "train_df_All_rare.dropna(axis=0, inplace=True)\n",
    "train_df_All_rare.reset_index(drop=True, inplace=True)\n",
    "train_df_All_rare['label'].astype(int)\n",
    "y_train_All_rare = train_df_All_rare['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "painful-grove",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 49/3452 [00:00<00:07, 482.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3452/3452 [00:23<00:00, 145.71it/s]\n",
      "  4%|▎         | 51/1441 [00:00<00:02, 499.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1441/1441 [00:09<00:00, 145.58it/s]\n",
      "  0%|          | 0/509 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509/509 [00:03<00:00, 133.85it/s]\n",
      "  8%|▊         | 40/490 [00:00<00:01, 390.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 490/490 [00:03<00:00, 148.84it/s]\n",
      " 19%|█▊        | 37/198 [00:00<00:00, 365.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [00:01<00:00, 164.56it/s]\n",
      "  0%|          | 0/257 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [00:01<00:00, 128.75it/s]\n",
      "  0%|          | 10/9615 [00:00<01:38, 97.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9615/9615 [03:33<00:00, 44.95it/s]\n",
      "  0%|          | 0/3478 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3478/3478 [01:14<00:00, 46.54it/s]\n",
      "  0%|          | 0/1231 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1231/1231 [00:26<00:00, 47.10it/s]\n",
      "  0%|          | 0/2934 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2934/2934 [01:06<00:00, 44.15it/s]\n",
      "  0%|          | 0/1537 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1537/1537 [00:32<00:00, 47.86it/s]\n"
     ]
    }
   ],
   "source": [
    "def embedding_padding(train_df):\n",
    "    raw_docs_train = train_df['document'].tolist()\n",
    "    num_classes = len(label_names)\n",
    "    print(num_classes)\n",
    "    processed_docs_train = []\n",
    "\n",
    "    for doc in tqdm(raw_docs_train):\n",
    "        tokens = my_Tokenizer(doc)\n",
    "        processed_docs_train.append(tokens)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "    tokenizer.fit_on_texts(processed_docs_train)\n",
    "    return tokenizer\n",
    "\n",
    "def Comments_embedding_padding(train_df):\n",
    "    raw_docs_train = train_df['Comments'].tolist()\n",
    "    num_classes = len(label_names)\n",
    "    print(num_classes)\n",
    "    processed_docs_train = []\n",
    "\n",
    "    for doc in tqdm(raw_docs_train):\n",
    "        tokens = my_Tokenizer(doc)\n",
    "        processed_docs_train.append(tokens)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "    tokenizer.fit_on_texts(processed_docs_train)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer_All_rare = embedding_padding(train_df_All_rare)\n",
    "\n",
    "tokenizer_All = embedding_padding(train_df_All_b)\n",
    "tokenizer_pol = embedding_padding(train_df_pol)\n",
    "tokenizer_soc = embedding_padding(train_df_soc)\n",
    "tokenizer_eco = embedding_padding(train_df_eco)\n",
    "tokenizer_others = embedding_padding(train_df_others)\n",
    "\n",
    "tokenizer_Comments_All = Comments_embedding_padding(train_df_Comments_All)\n",
    "tokenizer_Comments_pol = Comments_embedding_padding(train_df_Comments_pol)\n",
    "tokenizer_Comments_eco = Comments_embedding_padding(train_df_Comments_eco)\n",
    "tokenizer_Comments_soc = Comments_embedding_padding(train_df_Comments_soc)\n",
    "tokenizer_Comments_others = Comments_embedding_padding(train_df_Comments_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "tired-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================================================================\n",
    "# 단어 토큰화 및 빈도순 정렬 후 추출\n",
    "\n",
    "def tfidf_Vectorizer(response):\n",
    "    response_list = []\n",
    "    response_list.append(response)\n",
    "    tfidf_Vectorizer = TfidfVectorizer(tokenizer=my_Tokenizer, min_df=1) # df 값(단어가 몇 문장들에서 등장하였는지)을 최소 'min_df' 값으로 설정.\n",
    "    X = tfidf_Vectorizer.fit_transform(response_list).toarray()\n",
    "#     print(X.shape)    # X(2차원 배열)의 행,열 수를 출력.\n",
    "#     print(tfidf_Vectorizer.vocabulary_)   # 각 단어들이 배열 X에서 몇번째 열(인덱스 값)에 해당하는지 출력.\n",
    "\n",
    "\n",
    "    #pandas를 활용하여 각 단어들의 각 문장에서의 tf-idf 값들을 모두 더하고, 내림차순으로 정렬하여 상위 n개 출력\n",
    "    count = X.sum(axis=0)    # 2차원 배열 X에서 각 열을 기준으로 합을 구함. (각 단어들의 '최종' tf-idf 값으로 간주.)\n",
    "    word_count = pd.DataFrame({\n",
    "        '단어' : tfidf_Vectorizer.get_feature_names(),\n",
    "        '빈도' : count.flat\n",
    "    })\n",
    "    sorted_df = word_count.sort_values('빈도', ascending=False)\n",
    "#     print(sorted_df.head(10), \"\\n\")\n",
    "\n",
    "    word_ = list(np.array(sorted_df['단어'].tolist()))\n",
    "    return(word_)\n",
    "\n",
    "\n",
    "# ===========================================================================================================================\n",
    "# 네이버 댓글 분석\n",
    "\n",
    "def Naver_Comments_score(response, tokenizer, tokenizer_Comments, SNU_Model, Naver_Comments_Model):\n",
    "    score_list = []\n",
    "    for i in tqdm(range(len(response))):\n",
    "        response_ = re.compile(\"[^A-Za-z0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]+\").sub('',response[i])\n",
    "        response_ = okt.morphs(response_, stem=True) # 토큰화\n",
    "        response_ = [word for word in response_ if not word in stopwords] # 불용어 제거\n",
    "        response_ = ' '.join(response_)\n",
    "        word_ = tfidf_Vectorizer(response_)\n",
    "        word__ = \"\"\n",
    "        if len(word_) <= 10:\n",
    "            word__ = response_\n",
    "        else:\n",
    "            for i in range(1, min(len(word_), 11)):\n",
    "                word__ += word_[i]\n",
    "                word__ += \" \"\n",
    "\n",
    "    # ================================================================\n",
    "\n",
    "        tokens_response = []\n",
    "        SNU_score_list = []\n",
    "\n",
    "        tokens = my_Tokenizer(response_)\n",
    "        tokens_response.append(tokens)\n",
    "\n",
    "        word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "        word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "\n",
    "        word_seq_response_to_score = word_seq_response.reshape(1,max_seq_len)\n",
    "        SNU_score_list.append(float(SNU_Model.predict(word_seq_response_to_score)))\n",
    "\n",
    "        df_qq = []\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}\n",
    "\n",
    "        allComments = []\n",
    "        def create_soup(url):\n",
    "            res = requests.get(url, headers=headers)\n",
    "            res.raise_for_status()\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            return soup\n",
    "\n",
    "        Comments_score_list = []\n",
    "        # 네이버 뉴스 url을 입력합니다.\n",
    "        url1 = \"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={}\".format(word__)\n",
    "        soup = create_soup(url1)\n",
    "        for i in range(1, 15):\n",
    "            List = []\n",
    "            try:\n",
    "                url = soup.select_one(\"#sp_nws{} > div.news_wrap.api_ani_send > div > div.news_info > div.info_group > a:nth-of-type(2)\".format(i))['href']\n",
    "                oid = url.split(\"oid=\")[1].split(\"&\")[0] #422\n",
    "                aid = url.split(\"aid=\")[1] #0000430957\n",
    "                page = 1\n",
    "                header = {\n",
    "                    \"User-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\",\n",
    "                    \"referer\": url,\n",
    "                }\n",
    "                # time.sleep(random.randint(1,2))\n",
    "\n",
    "                while True:\n",
    "                    c_url = \"https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json?ticket=news&templateId=default_society&pool=cbox5&_callback=jQuery1707138182064460843_1523512042464&lang=ko&country=&objectId=news\" + oid + \"%2C\" + aid + \"&categoryId=&pageSize=20&indexSize=10&groupId=&listType=OBJECT&pageType=more&page=\" + str(\n",
    "                        page) + \"&refresh=false&sort=FAVORITE\"\n",
    "                    # 파싱하는 단계입니다.\n",
    "                    r = requests.get(c_url, headers=header)\n",
    "                    cont = BeautifulSoup(r.content, \"html.parser\")\n",
    "                    total_comm = str(cont).split('comment\":')[1].split(\",\")[0]\n",
    "\n",
    "                    match = re.findall('\"contents\":([^\\*]*),\"userIdNo\"', str(cont))\n",
    "                    # 댓글을 리스트에 중첩합니다.\n",
    "                    try:\n",
    "                        for b in range(0,3):\n",
    "                            List.append(match[b])\n",
    "                    except:\n",
    "                        pass\n",
    "                    break\n",
    "\n",
    "                tokens_response = []\n",
    "                score_list_sum = []\n",
    "\n",
    "                for i in range(len(List)):\n",
    "                    tokens = my_Tokenizer(List[i])\n",
    "                    tokens_response.append(tokens)\n",
    "\n",
    "                word_seq_response2 = tokenizer_Comments.texts_to_sequences(tokens_response)\n",
    "                word_seq_response2 = sequence.pad_sequences(word_seq_response2, maxlen=max_seq_len_Comments)\n",
    "                \n",
    "                for sco in range(len(List)):\n",
    "                    word_seq_response_to_score2 = word_seq_response2[sco].reshape(1,max_seq_len_Comments)\n",
    "                    score_list_sum.append(float(Naver_Comments_Model.predict(word_seq_response_to_score2)))\n",
    "\n",
    "                Comments_score_list.append(np.mean(score_list_sum))\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        Comments_score_list_final = []\n",
    "        \n",
    "        Comments_score_list = [x for x in Comments_score_list if math.isnan(x)==False]\n",
    "        Comments_score_list_final.append(np.mean(Comments_score_list))\n",
    "\n",
    "        a = [score * 0.6 for score in SNU_score_list]\n",
    "        b = [score * 0.4 for score in Comments_score_list_final]\n",
    "        for i in range(len(b)):\n",
    "            if math.isnan(b[i]):\n",
    "                b[i] = a[i]\n",
    "\n",
    "        score_list_sum = list(map(lambda x,y:x+y, a,b))\n",
    "        score_list.append(score_list_sum[0])\n",
    "\n",
    "    return score_list\n",
    "\n",
    "def Naver_Score(response, tokenizer, SNU_Model):\n",
    "    SNU_score_list = []\n",
    "    for i in tqdm(range(len(response))):\n",
    "        response_ = re.compile(\"[^A-Za-z0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]+\").sub('',response[i])\n",
    "        response_ = okt.morphs(response_, stem=True) # 토큰화\n",
    "        response_ = [word for word in response_ if not word in stopwords] # 불용어 제거\n",
    "        response_ = ' '.join(response_)\n",
    "        word_ = tfidf_Vectorizer(response_)\n",
    "        word__ = \"\"\n",
    "        if len(word_) <= 6:\n",
    "            word__ = response\n",
    "        else:\n",
    "            for i in range(1, min(len(word_), 7)):\n",
    "                word__ += word_[i]\n",
    "                word__ += \" \"\n",
    "\n",
    "    # ================================================================\n",
    "\n",
    "        tokens_response = []\n",
    "\n",
    "        tokens = my_Tokenizer(response_)\n",
    "        tokens_response.append(tokens)\n",
    "\n",
    "        word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "        word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "\n",
    "        word_seq_response_to_score = word_seq_response.reshape(1,max_seq_len)\n",
    "        SNU_score_list.append(float(SNU_Model.predict(word_seq_response_to_score)))\n",
    "    return SNU_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "handmade-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_pol_test = train_df_pol.sample(frac=0.1).reset_index(drop=True)\n",
    "y_train_pol_test = train_df_pol_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "exclusive-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = train_df_pol_test['document'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "decreased-banks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "accepted-adjustment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 11) for input KerasTensor(type_spec=TensorSpec(shape=(None, 11), dtype=tf.float32, name='embedding_6_input'), name='embedding_6_input', description=\"created by layer 'embedding_6_input'\"), but it was called on an input with incompatible shape (None, 10).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/51 [00:07<05:59,  7.19s/it]/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 51/51 [01:06<00:00,  1.31s/it]\n",
      "100%|██████████| 51/51 [00:04<00:00, 11.15it/s]\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 11) for input KerasTensor(type_spec=TensorSpec(shape=(None, 11), dtype=tf.float32, name='embedding_10_input'), name='embedding_10_input', description=\"created by layer 'embedding_10_input'\"), but it was called on an input with incompatible shape (None, 10).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:06<00:00,  7.66it/s]\n",
      "100%|██████████| 51/51 [00:07<00:00,  6.98it/s]\n"
     ]
    }
   ],
   "source": [
    "balance_Comments_score_pol = Naver_Comments_score(test_list, tokenizer_pol, tokenizer_Comments_pol, SNU_model_pol, Naver_Comments_pol)\n",
    "balance_score_pol = Naver_Score(test_list, tokenizer_pol, SNU_model_pol)\n",
    "All_score_pol = Naver_Score(test_list, tokenizer_All, SNU_model_All)\n",
    "All_rare_score_pol = Naver_Score(test_list, tokenizer_All_rare, SNU_model_All_rare)\n",
    "\n",
    "# balance_Comments_score_eco = Naver_Comments_score(test_list, tokenizer_eco, tokenizer_Comments_eco, SNU_model_eco, Naver_Comments_eco)\n",
    "# balance_score_eco = Naver_Score(test_list, tokenizer_eco, SNU_model_eco)\n",
    "# All_score_eco = Naver_Score(test_list, tokenizer_All, SNU_model_All)\n",
    "# All_rare_score_eco = Naver_Score(test_list, tokenizer_All_rare, SNU_model_All_rare)\n",
    "\n",
    "# balance_Comments_score_soc = Naver_Comments_score(test_list, tokenizer_soc, tokenizer_Comments_soc, SNU_model_soc, Naver_Comments_soc)\n",
    "# balance_score_soc = Naver_Score(test_list, tokenizer_soc, SNU_model_soc)\n",
    "# All_score_soc = Naver_Score(test_list, tokenizer_All, SNU_model_All)\n",
    "# All_rare_score_soc = Naver_Score(test_list, tokenizer_All_rare, SNU_model_All_rare)\n",
    "\n",
    "# balance_Comments_score_others = Naver_Comments_score(test_list, tokenizer_others, tokenizer_Comments_others, SNU_model_others, Naver_Comments_others)\n",
    "# balance_score_others = Naver_Score(test_list, tokenizer_others, SNU_model_others)\n",
    "# All_score_others = Naver_Score(test_list, tokenizer_All, SNU_model_All)\n",
    "# All_rare_score_others = Naver_Score(test_list, tokenizer_All_rare, SNU_model_All_rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "looking-population",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정치 카테고리 (Under Sampling) with Comments :  0.7843137254901961\n",
      "정치 카테고리 (Under Sampling) :  0.803921568627451\n",
      "All 카테고리 (underSampling) :  0.8235294117647058\n",
      "All 카테고리 :  0.6862745098039216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def data_(test_score):\n",
    "    for i in range(len(test_score)):\n",
    "        if test_score[i] >= 0.5:\n",
    "            test_score[i]=1\n",
    "        else:\n",
    "            test_score[i]=0\n",
    "    return test_score\n",
    "\n",
    "balance_Comments_score_pol = data_(balance_Comments_score_pol)\n",
    "balance_score_pol = data_(balance_score_pol)\n",
    "All_score_pol = data_(All_score_pol)\n",
    "All_rare_score_pol = data_(All_rare_score_pol)\n",
    "\n",
    "# balance_Comments_score_eco = data_(balance_Comments_score_eco)\n",
    "# balance_score_eco = data_(balance_score_eco)\n",
    "# All_score_eco = data_(All_score_eco)\n",
    "# All_rare_score_eco = data_(All_rare_score_eco)\n",
    "\n",
    "# balance_Comments_score_soc = data_(balance_Comments_score_soc)\n",
    "# balance_score_soc = data_(balance_score_soc)\n",
    "# All_score_soc = data_(All_score_soc)\n",
    "# All_rare_score_soc = data_(All_rare_score_soc)\n",
    "\n",
    "# balance_Comments_score_others = data_(balance_Comments_score_others)\n",
    "# balance_score_others = data_(balance_score_others)\n",
    "# All_score_others = data_(All_score_others)\n",
    "# All_rare_score_others = data_(All_rare_score_others)\n",
    "\n",
    "print('정치 카테고리 (Under Sampling) with Comments : ', accuracy_score(balance_Comments_score_pol, y_train_pol_test))\n",
    "print('정치 카테고리 (Under Sampling) : ', accuracy_score(balance_score_pol, y_train_pol_test))\n",
    "print('All 카테고리 (underSampling) : ', accuracy_score(All_score_pol, y_train_pol_test))\n",
    "print('All 카테고리 : ', accuracy_score(All_rare_score_pol, y_train_pol_test))\n",
    "\n",
    "# print('경제 카테고리 (Under Sampling) with Comments : ', accuracy_score(balance_Comments_score_eco, y_train_eco_test))\n",
    "# print('경제 카테고리 (Under Sampling) : ', accuracy_score(balance_score_eco, y_train_eco_test))\n",
    "# print('All 카테고리 (underSampling) : ', accuracy_score(All_score_eco, y_train_eco_test))\n",
    "# print('All 카테고리 : ', accuracy_score(All_rare_score_eco, y_train_eco_test))\n",
    "\n",
    "# print('사회 카테고리 (Under Sampling) with Comments : ' accuracy_score(balance_Comments_score_soc, y_train_soc_test))\n",
    "# print('사회 카테고리 (Under Sampling) : ' accuracy_score(balance_score_soc, y_train_soc_test))\n",
    "# print('All 카테고리 (underSampling) : 'accuracy_score(All_score_soc, y_train_soc_test))\n",
    "# print('All 카테고리 : 'accuracy_score(All_rare_score_soc, y_train_soc_test))\n",
    "\n",
    "# print('기타 카테고리 (Under Sampling) with Comments : ' accuracy_score(balance_Comments_score_others, y_train_others_test))\n",
    "# print('기타 카테고리 (Under Sampling) : ' accuracy_score(balance_score_others, y_train_others_test))\n",
    "# print('All 카테고리 (underSampling) : 'accuracy_score(All_score_others, y_train_others_test))\n",
    "# print('All 카테고리 : 'accuracy_score(All_rare_score_others, y_train_others_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "historic-thought",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>link</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OECD 회원국 중 한국 터키만 상원이 없다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3285</td>\n",
       "      <td>전혀 사실 아님</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>트럼프 \"100조 무기가 탈레반에\", 사실일까?</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3283</td>\n",
       "      <td>대체로 사실 아님</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>대선후보 경선등록 후 완주하지 않으면 독자출마 가능하다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3281</td>\n",
       "      <td>대체로 사실 아님</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>사형 집행해야 사회 안전해진다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3279</td>\n",
       "      <td>대체로 사실 아님</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>사랑제일교회 야외 예배는 방역법을 준수했다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3278</td>\n",
       "      <td>전혀 사실 아님</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>여름엔 긴팔 셔츠보다 반팔이 더 시원할까?</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/210</td>\n",
       "      <td>절반의 사실</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>단종된 갤럭시노트7의 재고 부품을 활용한 삼성전자의 '갤럭시노트FE'는 신제품일까?</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/204</td>\n",
       "      <td>절반의 사실</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>공공와이파이 20만개 설치시 연 4천800억~8천500억원의 데이터요금을 경감할 수...</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/191</td>\n",
       "      <td>대체로 사실</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>참여연대 등이  '통신 기본료 1만1천원 폐지' 공약 이행을 주장하고 있다.   L...</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/187</td>\n",
       "      <td>논쟁중</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>고농도 미세먼지가 발생할 때 최대 86%가 국외 특히 중국에서 온 것으로 나타났다는...</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/71</td>\n",
       "      <td>판단 유보</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4239 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               document  \\\n",
       "0                              OECD 회원국 중 한국 터키만 상원이 없다   \n",
       "1                            트럼프 \"100조 무기가 탈레반에\", 사실일까?   \n",
       "2                        대선후보 경선등록 후 완주하지 않으면 독자출마 가능하다   \n",
       "3                                      사형 집행해야 사회 안전해진다   \n",
       "4                               사랑제일교회 야외 예배는 방역법을 준수했다   \n",
       "...                                                 ...   \n",
       "4234                            여름엔 긴팔 셔츠보다 반팔이 더 시원할까?   \n",
       "4235     단종된 갤럭시노트7의 재고 부품을 활용한 삼성전자의 '갤럭시노트FE'는 신제품일까?   \n",
       "4236  공공와이파이 20만개 설치시 연 4천800억~8천500억원의 데이터요금을 경감할 수...   \n",
       "4237  참여연대 등이  '통신 기본료 1만1천원 폐지' 공약 이행을 주장하고 있다.   L...   \n",
       "4238  고농도 미세먼지가 발생할 때 최대 86%가 국외 특히 중국에서 온 것으로 나타났다는...   \n",
       "\n",
       "                                           link      label  doc_len  \n",
       "0     https://factcheck.snu.ac.kr/v2/facts/3285   전혀 사실 아님        7  \n",
       "1     https://factcheck.snu.ac.kr/v2/facts/3283  대체로 사실 아님        5  \n",
       "2     https://factcheck.snu.ac.kr/v2/facts/3281  대체로 사실 아님        7  \n",
       "3     https://factcheck.snu.ac.kr/v2/facts/3279  대체로 사실 아님        4  \n",
       "4     https://factcheck.snu.ac.kr/v2/facts/3278   전혀 사실 아님        5  \n",
       "...                                         ...        ...      ...  \n",
       "4234   https://factcheck.snu.ac.kr/v2/facts/210     절반의 사실        6  \n",
       "4235   https://factcheck.snu.ac.kr/v2/facts/204     절반의 사실        8  \n",
       "4236   https://factcheck.snu.ac.kr/v2/facts/191     대체로 사실        9  \n",
       "4237   https://factcheck.snu.ac.kr/v2/facts/187        논쟁중       18  \n",
       "4238    https://factcheck.snu.ac.kr/v2/facts/71      판단 유보       15  \n",
       "\n",
       "[4239 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if sys.version_info[0] < 3:\n",
    "    from io import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = 'AKIA2EDEFCNPX2G7FWNY'\n",
    "aws_secret = 'Xt1EJXPsRdI27VI7TBSCsRMNJWsewq9FY0g4vDU7'\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'snucsv'\n",
    "label_names = [\"label\"]\n",
    "\n",
    "def s3_load(category):\n",
    "    object_key = category\n",
    "    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('cp949')\n",
    "\n",
    "    # load data\n",
    "    train_df = pd.read_csv(StringIO(csv_string))\n",
    "    y_train = train_df[label_names].values\n",
    "\n",
    "    train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\n",
    "    max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "    \n",
    "    return train_df, y_train\n",
    "\n",
    "train_df_All, y_train_All = s3_load('SNU_All.csv')\n",
    "train_df_All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_model = load_model('SNU_LSTM_Model_경제_balance.h5')\n",
    "balanced_score = balanced_model.evaluate(word_seq_train, y_train, batch_size=1)\n",
    "model = load_model('SNU_LSTM_Model_경제.h5')\n",
    "scores = model.evaluate(word_seq_train, y_train, batch_size=1)\n",
    "all_model = load_model('SNU_LSTM_Model_All.h5')\n",
    "all_scores = all_model.evaluate(word_seq_train, y_train, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"경제 balanced data LSTM %s: %.2f%%\" %(balanced_model.metrics_names[1], scores[1]*100))\n",
    "print(\"경제 data LSTM %s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    "print(\"전체 data LSTM %s: %.2f%%\" %(all_model.metrics_names[1], all_scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SNU 웹스크래핑",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTOuMQCF2EghqLIuQhSs3j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dltmdgh579/FactCheck_Service/blob/master/SNU_%EC%9B%B9%EC%8A%A4%ED%81%AC%EB%9E%98%ED%95%91.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiKTDHTgV9oJ"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}\n",
        "NaverNews = \"https://news.naver.com/\"\n",
        "NaverNews2 = \"n.news.naver.com\"\n",
        "OhMyNews = \"//omn.kr/\"\n",
        "OhMyNews2 = \"www.ohmynews.com\"\n",
        "중앙일보 = \"//news.joins\"\n",
        "NEWSTOF = \"newstof.com\"\n",
        "JTBC = \"news.jtbc\"\n",
        "KBS = \"news.kbs\"\n",
        "SBS = \"news.sbs.co.kr\"\n",
        "MBC = \"imnews.imbc.com\"\n",
        "시사위크 = \"sisaweek.com\"\n",
        "YTN = \"ytn.co.kr\"\n",
        "MaeilNews = \"www.mk.co.kr\"\n",
        "Yunhapnews = \"www.yna.co.kr\"\n",
        "Hankookilbo = \"www.hankookilbo.com\"\n",
        "NewsPost = \"newspost.kr\"\n",
        "MoneyToday = \"news.mt.co.kr\"\n",
        "MBN = \"www.mbn.co.kr/news\"\n",
        "MBN2 = \"www.mbn.co.kr/vod\"\n",
        "한국경제 = \"www.hankyung.com\"\n",
        "문화일보 = \"www.munhwa.com\"\n",
        "TV조선 = \"news.tvchosun.com\"\n",
        "노컷뉴스 = \"www.nocutnews.co.kr\"\n",
        "아이뉴스24 = \"www.inews24.com\"\n",
        "아시아경제 = \"www.asiae.co.kr\"\n",
        "노컷뉴스 = \"nocutnews.co.kr\"\n",
        "서울신문 = \"seoul.co.kr\"\n",
        "NEWSIS = \"newsis.com\"\n",
        "파이낸셜뉴스 = \"fnnews.com\"\n",
        "전북일보 = \"jjan.kr\"\n",
        "\n",
        "filename = \"SNU_score1_1-28_.csv\"\n",
        "f = open(filename, \"w\", encoding=\"utf-8-sig\", newline=\"\")\n",
        "writer = csv.writer(f)\n",
        "\n",
        "title_ = [\"제목\", \"링크\", \"날짜\", \"기사내용\"]\n",
        "writer.writerow(title_)\n",
        "\n",
        "def create_soup(url):\n",
        "    res = requests.get(url, headers=headers)\n",
        "    res.raise_for_status()\n",
        "    soup = BeautifulSoup(res.text, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "def scrape_SNU():\n",
        "\n",
        "    news_df = pd.DataFrame(columns=(\"Title\", \"link\", \"Datetime\", \"Article\"))\n",
        "    idx = 0\n",
        "\n",
        "    for i in range(1, 29):\n",
        "        url = \"https://factcheck.snu.ac.kr/v2/facts?page={}&score=5\".format(i)\n",
        "        soup = create_soup(url)\n",
        "        SNUnews = soup.find_all(\"li\", attrs={\"class\": \"fcItem_wrap_li\"})\n",
        "        print(\"[SNU 팩트체크]\")\n",
        "        i = 1\n",
        "        for SNU in SNUnews:\n",
        "            link1 = SNU.select(\"#container > div > div.left_article > div > ul > li:nth-child({}) > div > div.fcItem_top.clearfix > div.prg.fcItem_li > p:nth-child(2) > a\".format(i))[\"href\"]\n",
        "            link1 = str(link1)\n",
        "            link1 = \"https://factcheck.snu.ac.kr\" + link1[9:23]\n",
        "            time.sleep(random.randint(1, 2))\n",
        "            title = SNU.select(\"#container > div > div.left_article > div > ul > li:nth-child({}) > div > div.fcItem_top.clearfix > div.prg.fcItem_li > p:nth-child(2) > a\".format(i))[0].get_text()\n",
        "            title = str(title)\n",
        "            i += 1\n",
        "            page1 = requests.get(link1, headers=headers)\n",
        "            page1_html = BeautifulSoup(page1.text, 'html.parser')\n",
        "            page2_to_link = page1_html.find('p', attrs={\"class\":\"vf_article\"})\n",
        "            link2 = page2_to_link.find('a')[\"href\"]\n",
        "            page2 = requests.get(link2, headers=headers)\n",
        "            page2_html = BeautifulSoup(page2.text, 'html.parser')\n",
        "\n",
        "            if NaverNews in link2:\n",
        "                datetime = page2_html.find('span', attrs={\"class\":\"t11\"}).get_text()\n",
        "                article_ = page2_html.find('div', attrs={\"id\":\"articleBodyContents\"}).get_text()\n",
        "                article = article_.replace(\"\\n\", \"\")\n",
        "            elif NaverNews2 in link2:\n",
        "                datetime = page2_html.find(attrs={\"class\":\"media_end_head_info_datestamp_time _ARTICLE_DATE_TIME\"}).get_text()\n",
        "                article = page2_html.find(attrs={\"id\":\"dic_area\"}).get_text()\n",
        "            elif OhMyNews in link2:\n",
        "                datetime_ = page2_html.find('div', attrs={\"class\":\"info_data\"}).get_text()\n",
        "                datetime_ = datetime_.replace(\"\\n\",\"\")\n",
        "                datetime = datetime_[0:14]\n",
        "                datetime = \"20\" + datetime\n",
        "                article = page2_html.find(attrs={\"class\":\"at_contents\"}).get_text()\n",
        "            elif OhMyNews2 in link2:\n",
        "                datetime_ = page2_html.find('div', attrs={\"class\":\"info_data\"}).get_text()\n",
        "                datetime_ = datetime_.replace(\"\\n\",\"\")\n",
        "                datetime = datetime_[0:14]\n",
        "                datetime = \"20\" + datetime\n",
        "                article = page2_html.find(attrs={\"class\":\"at_contents\"}).get_text()\n",
        "            elif 중앙일보 in link2:\n",
        "                datetime___ = page2_html.find('div', attrs={\"class\":\"byline\"}).get_text()\n",
        "                datetime__ = datetime___.replace(\"[중앙일보]\",\"\")\n",
        "                datetime_ = datetime__.replace(\"입력\",\"\")\n",
        "                datetime_ = datetime_.replace(\"\\n\",\"\")\n",
        "                datetime = datetime_[1:17]\n",
        "                article = page2_html.find(attrs={\"id\":\"article_body\"}).get_text()\n",
        "            elif NEWSTOF in link2:\n",
        "                datetime_ = page2_html.find(\"div\", attrs={\"class\":\"info-text\"})\n",
        "                datetime = datetime_.select(\"li\")[1].get_text()\n",
        "                datetime = datetime.replace(\"승인\",\"\")\n",
        "                datetime = datetime[2:]\n",
        "                article = page2_html.find(attrs={\"id\":\"article-view-content-div\"}).get_text()\n",
        "            elif JTBC in link2:\n",
        "                datetime = page2_html.find(\"span\", attrs={\"class\":\"i_date\"}).get_text()\n",
        "                datetime = datetime.replace(\"입력\", \"\")\n",
        "                datetime = datetime[1:]\n",
        "                article = page2_html.find(attrs={\"class\":\"article_content\"}).get_text()\n",
        "            elif KBS in link2:\n",
        "                datetime = page2_html.find(\"em\", attrs={\"class\":\"date\"}).get_text()\n",
        "                datetime = datetime.replace(\"입력\", \"\")\n",
        "                datetime = datetime[1:]\n",
        "                article = page2_html.find(attrs={\"id\":\"cont_newstext\"}).get_text()\n",
        "            elif SBS in link2:\n",
        "                datetime = page2_html.find(\"span\", attrs={\"class\":\"date\"}).get_text()\n",
        "                datetime = datetime.replace(\"\\n\", \"\")\n",
        "                datetime = datetime.replace(\"작성\", \"\")\n",
        "                datetime = datetime[:16]\n",
        "                article = page2_html.find(attrs={\"class\":\"text_area\"}).get_text()\n",
        "            elif MBC in link2:\n",
        "                datetime = page2_html.find(attrs={\"class\":\"input\"}).get_text()\n",
        "                datetime = datetime.replace(\"입력\", \"\")\n",
        "                datetime = datetime.replace(\"\\n\", \"\")\n",
        "                datetime = datetime.replace(\"\\t\", \"\")\n",
        "                article = \"크롤링 X\"\n",
        "                # article = page2_html.find(\"div\", attrs={\"class\":\"news_txt\"})\n",
        "                # article = page2_html.select(\"#content > div > section.wrap_article > article > div.news_cont > div.news_txt > div.report\")\n",
        "            elif 시사위크 in link2:\n",
        "                datetime = page2_html.find(attrs={\"class\":\"curation-view-dated\"}).get_text()\n",
        "                datetime = datetime.replace(\"\\n\", \"\")\n",
        "                datetime = datetime[3:15]\n",
        "                article = page2_html.find(attrs={\"class\":\"curation-content view article-veiw-body\"}).get_text()\n",
        "            elif YTN in link2:\n",
        "                datetime = page2_html.find(\"span\", attrs={\"class\":\"time\"}).get_text()\n",
        "                article = page2_html.find(attrs={\"class\":\"article\"}).get_text()\n",
        "            elif MaeilNews in link2:\n",
        "                datetime = page2_html.find(\"li\", attrs={\"class\":\"lasttime\"}).get_text()\n",
        "                datetime = datetime.replace(\"입력 :\", \"\")\n",
        "                article = page2_html.find(\"div\", attrs={\"id\":\"article_body\"}).get_text()\n",
        "            elif Yunhapnews in link2:\n",
        "                datetime = page2_html.find(attrs={\"class\":\"update-time\"}).get_text()\n",
        "                datetime = datetime.replace(\"송고시간\", \"\")\n",
        "                article = page2_html.find(attrs={\"class\":\"story-news article\"}).get_text()\n",
        "            elif Hankookilbo in link2:\n",
        "                datetime = page2_html.find(attrs={\"class\":\"info\"}).get_text()\n",
        "                datetime = datetime.replace(\"입력\", \"\")\n",
        "                article = page2_html.find_all(\"p\", attrs={\"class\":\"editor-p\"})\n",
        "                article = \"\".join(map(str, article))\n",
        "                article = article.replace(\"<p class=\", \"\")\n",
        "                article = article.replace(\"editor-p\", \"\")\n",
        "                article = article.replace(\"/p\",\"\")\n",
        "                article = article.replace(\"<br/>\",\"\")\n",
        "            elif NewsPost in link2:\n",
        "                datetime = page2_html.find(attrs={\"class\":\"infomation\"}).get_text()\n",
        "                datetime = datetime.replace(\"\\n\", \"\")\n",
        "                datetime = datetime.replace(\"\\t\", \"\")\n",
        "                datetime = datetime.replace(\"입력\", \"\")\n",
        "                datetime = datetime[13:29]\n",
        "                article = page2_html.find(attrs={\"id\":\"article-view-content-div\"}).get_text()\n",
        "            elif MoneyToday in link2:\n",
        "                datetime = page2_html.find(\"li\", attrs={\"class\":\"date\"}).get_text()\n",
        "                article = page2_html.find(attrs={\"id\":\"textBody\"}).get_text()\n",
        "            elif MBN in link2:\n",
        "                datetime = page2_html.find(\"span\", attrs={\"class\":\"time\"}).get_text()\n",
        "                datetime = datetime.replace(\"기사입력\", \"\")\n",
        "                article = page2_html.find(attrs={\"id\":\"newsViewArea\"}).get_text()\n",
        "            elif MBN2 in link2:\n",
        "                datetime = page2_html.find(attrs={\"class\":\"daily_date\"}).get_text()\n",
        "                article = page2_html.find(attrs={\"id\":\"textArea\"}).get_text()\n",
        "            elif 한국경제 in link2:\n",
        "                datetime = page2_html.find(\"span\", attrs={\"class\":\"num\"}).get_text()\n",
        "                article = page2_html.find(attrs={\"id\":\"articletxt\"}).get_text()\n",
        "            elif 문화일보 in link2:\n",
        "                datetime = page2_html.find(\"td\", attrs={\"align\":\"right\"}).get_text()\n",
        "                article = page2_html.find(attrs={\"id\":\"NewsAdContent\"}).get_text()\n",
        "            elif TV조선 in link2:\n",
        "                datetime = page2_html.find(attrs={\"class\":\"date\"}).get_text()\n",
        "                datetime = datetime.replace(\"등록\", \"\")\n",
        "                article = page2_html.find(attrs={\"class\":\"article font03\"}).get_text()\n",
        "            elif 노컷뉴스 in link2:\n",
        "                datetime = page2_html.find(attrs={\"id\":\"lblDateLine\"}).get_text()\n",
        "                article = page2_html.find(attrs={\"id\":\"pnlContent\"}).get_text()\n",
        "            elif 아이뉴스24 in link2:\n",
        "                datetime = page2_html.find(\"nav\", attrs={\"class\":\"view\"}).get_text()\n",
        "                datetime = datetime.replace(\"\\n\", \"\")\n",
        "                datetime = datetime[9:25]\n",
        "                article = page2_html.find(attrs={\"id\":\"articleBody\"})\n",
        "                article = article.find_all(\"p\")\n",
        "                article = \"\".join(map(str, article))\n",
        "                article = article.replace(\"<p>\", \"\")\n",
        "                article = article.replace(\"</p>\", \"\")\n",
        "            elif 아시아경제 in link2:\n",
        "                datetime = page2_html.find(attrs={\"class\":\"user_data\"}).get_text()\n",
        "                article = page2_html.find(attrs={\"class\":\"article fb-quotable\"}).get_text()\n",
        "            elif 노컷뉴스 in link2:\n",
        "                datetime = page2_html.find(attrs={\"id\":\"lblDateLine\"}).get_text()\n",
        "                article = page2_html.find(attrs={\"id\":\"pnlContent\"}).get_text()\n",
        "            elif 서울신문 in link2:\n",
        "                datetime = page2_html.find(attrs={\"itemprop\":\"datePublished\"}).get_text()\n",
        "                article = page2_html.find(attrs={\"id\":\"atic_txt1\"}).get_text()\n",
        "            elif NEWSIS in link2:\n",
        "                datetime = page2_html.find(\"div\", attrs={\"class\":\"date\"}).get_text()\n",
        "                datetime = datetime.replace(\"등록 \", \"\")\n",
        "                article = page2_html.find(attrs={\"id\":\"textBody\"}).get_text()\n",
        "            elif 파이낸셜뉴스 in link2:\n",
        "                datetime = page2_html.find(\"div\", attrs={\"class\":\"byline\"}).get_text()\n",
        "                datetime = datetime.replace(\"파이낸셜뉴스\", \"\")\n",
        "                datetime = datetime.replace(\"입력 \", \"\")\n",
        "                datetime = datetime[:17]\n",
        "                article = page2_html.find(\"div\", attrs={\"id\":\"article_content\"}).get_text()\n",
        "            elif 전북일보 in link2:\n",
        "                datetime = page2_html.find(\"div\", attrs={\"class\":\"info-text\"}).get_text()\n",
        "                datetime = datetime[11:27]\n",
        "                article = page2_html.find(\"div\", attrs={\"id\":\"article-view-content-div\"}).get_text()\n",
        "            else:\n",
        "                datetime = \"크롤링 X\"\n",
        "                article = \"크롤링 X\"\n",
        "            news_df.loc[idx] = [title, link2, datetime, article]\n",
        "            idx += 1\n",
        "            data = [title, link2, datetime, article]\n",
        "            print(\"#\", end=\"\")\n",
        "            writer.writerow(data)\n",
        "            print(title)\n",
        "            print(datetime)\n",
        "            print(link2)\n",
        "            print(article)\n",
        "            print(\"*\"*50)\n",
        "\n",
        "            time.sleep(random.randint(1,2))\n",
        "\n",
        "scrape_SNU()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
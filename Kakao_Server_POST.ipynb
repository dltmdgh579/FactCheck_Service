{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "powerful-situation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extension horovod.torch has not been built: /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-37m-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n",
      "[2021-08-05 05:35:35.049 ip-172-31-2-138:3111 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-08-05 05:35:35.069 ip-172-31-2-138:3111 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    }
   ],
   "source": [
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import logging\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from gensim.models import FastText\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import boto3\n",
    "from bs4 import BeautifulSoup \n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "kakao_model = load_model('kakao_LSTM_model.h5')\n",
    "SNU_model = load_model('SNU_LSTM_Model.h5')\n",
    "\n",
    "# Vectorizer의 argument인 tokenizer에 KoNLPy의 pos 함수로 대체.\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        clean_words = []  # 정제된 단어 리스트\n",
    "        for word in pos:\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\n",
    "                    clean_words.append(word[0])\n",
    "        return clean_words\n",
    "\n",
    "\n",
    "my_Tokenizer = MyTokenizer(Okt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "superior-dealer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings_train.txt', <http.client.HTTPMessage at 0x7faebc3c7790>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pretty-italian",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘정치 세대교체’ 바람 일으킨 이준석, 대선엔 출마 못한다</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>액상형 전자담배는 니코틴이 함유되지 않아 담배가 아니다</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>한국 백신 접종률 세계 100위권 이하다</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>광역도시 없는 지역권은 실제 수요와 관계없이 획일적으로 국가교통망 정책에서 소외되고 있다</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>\" ‘당·정·청 전원회의’는 '운동권 용어'다\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>\"늘어가는 해양사고, 전북 관내 잠수구조인력 전무(全無)\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>\"청와대 하루 평균 업무추진비, 이명박 768만원, 박근혜 814만원, 문재인 55만원\"</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>국민연금공단 기금운용본부가 전주로 이전한 후, 해외투자자가 국민연금을 '패싱'한다</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>문재인 대통령이 미국 방문에서 푸대접 받았다</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1224 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               document  label  doc_len\n",
       "0                  현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다      1        9\n",
       "1                      ‘정치 세대교체’ 바람 일으킨 이준석, 대선엔 출마 못한다      1        8\n",
       "2                        액상형 전자담배는 니코틴이 함유되지 않아 담배가 아니다      1        7\n",
       "3                                한국 백신 접종률 세계 100위권 이하다      1        6\n",
       "4     광역도시 없는 지역권은 실제 수요와 관계없이 획일적으로 국가교통망 정책에서 소외되고 있다      1       11\n",
       "...                                                 ...    ...      ...\n",
       "1219                        \" ‘당·정·청 전원회의’는 '운동권 용어'다\"?      0        5\n",
       "1220                  \"늘어가는 해양사고, 전북 관내 잠수구조인력 전무(全無)\"?      0        6\n",
       "1221  \"청와대 하루 평균 업무추진비, 이명박 768만원, 박근혜 814만원, 문재인 55만원\"      0       10\n",
       "1222      국민연금공단 기금운용본부가 전주로 이전한 후, 해외투자자가 국민연금을 '패싱'한다      0        8\n",
       "1223                           문재인 대통령이 미국 방문에서 푸대접 받았다      0        6\n",
       "\n",
       "[1224 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if sys.version_info[0] < 3:\n",
    "    from io import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = ''\n",
    "aws_secret = ''\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'snucsv'\n",
    "\n",
    "object_key = 'SNU_Data_1200.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "# load data\n",
    "train_df = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train = train_df[label_names].values\n",
    "\n",
    "train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-irish",
   "metadata": {},
   "source": [
    "**SNU - 단어 벡터화 및 패딩**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "living-overview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "dictionary size : 3999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 503, 1459,  504, ..., 1461, 1462,    8],\n",
       "       [   0,   72,  761, ...,   51,  360,    5],\n",
       "       [   0,    0, 1466, ...,  506,  763,   41],\n",
       "       ...,\n",
       "       [ 649,  177,  381, ..., 3992,   13, 3993],\n",
       "       [3994, 3995, 3996, ...,  248, 3998,    5],\n",
       "       [   0,    0,    0, ..., 1401, 3999,  496]], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs_train = train_df['document'].tolist()\n",
    "num_classes = len(label_names)\n",
    "print(num_classes)\n",
    "processed_docs_train = []\n",
    "\n",
    "for doc in raw_docs_train:\n",
    "    tokens = my_Tokenizer(doc)\n",
    "    processed_docs_train.append(tokens)\n",
    "\n",
    "tokenizer_SNU = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer_SNU.fit_on_texts(processed_docs_train)\n",
    "word_seq_train = tokenizer_SNU.texts_to_sequences(processed_docs_train)\n",
    "word_index = tokenizer_SNU.word_index\n",
    "print('dictionary size :', len(word_index))\n",
    "\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-better",
   "metadata": {},
   "source": [
    "**kakao - 단어 벡터화 및 패딩**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "critical-breeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "train_data = train_data.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 한글과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(how = 'any')\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "okt = Okt()\n",
    "max_len = 30\n",
    "\n",
    "X_train = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(temp_X)\n",
    "    \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cross-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = '카카오톡 유료화 서비스 시행하나? 어쩌고 저쩌고 어쩌구 저쩌구 어쩌코 저쩌코 어쩌쿠 저쩌쿠'\n",
    "response = okt.morphs(response, stem=True) #토큰화\n",
    "response = [word for word in response if not word in stopwords] #불용어 제거\n",
    "encoded = tokenizer.texts_to_sequences([response]) # 정수 인코딩\n",
    "pad_new = pad_sequences(encoded, maxlen = max_len) #패딩\n",
    "score = float(kakao_model.predict(pad_new))\n",
    "result = ''\n",
    "if(score > 0.8 or score < 0.2):\n",
    "    result = \"객관성이 매우 낮습니다\"\n",
    "elif(score > 0.65 or score < 0.35):\n",
    "    result = \"객관성이 낮습니다\"\n",
    "else:\n",
    "    result = \"객관성이 높습니다\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "immune-trademark",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting httpd...\n",
      "\n",
      "INFO:root:POST request,\n",
      "Path: /\n",
      "Headers:\n",
      "Host: 3.37.215.72:8081\n",
      "User-Agent: python-requests/2.25.1\n",
      "Accept-Encoding: gzip, deflate\n",
      "Accept: */*\n",
      "Connection: keep-alive\n",
      "Content-Length: 22\n",
      "\n",
      "\n",
      "\n",
      "Body:\n",
      "카카오톡 유료화\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카카오톡 유료화\n",
      "    단어        빈도\n",
      "0   유료  0.707107\n",
      "1  카카오  0.707107 \n",
      "\n",
      "입력 데이터 : \n",
      " 카카오톡 유료화\n",
      "\n",
      "\n",
      "객관성 여부 : \n",
      " 객관성이 높습니다\n",
      "\n",
      "\n",
      "네이버 뉴스 제목 : \n",
      " [\"카카오의 콘텐츠 구독서비스, 핵심은 '큐레이션'\", '\"어뷰징뉴스 지겨워?\" 취향저격은 카톡에서…새 \\'카카오 뉴스\\'에 쏠리는 눈', '\"취향대로 본다\"…구독 콘텐츠 강화하는 네이버-카카오', \"유료화로 내몰리는 '클라우드 난민'\", \"[FreeView] 카카오모빌리티, '독점'과 '혁신' 사이\"]\n",
      "\n",
      "\n",
      "네이버 뉴스 링크 : \n",
      " ['http://www.the-pr.co.kr/news/articleView.html?idxno=47419', 'https://www.techm.kr/news/articleView.html?idxno=86810', 'https://www.asiatime.co.kr/article/20210804500139', 'https://www.sedaily.com/NewsView/22OWSGUKYL', 'https://www.techm.kr/news/articleView.html?idxno=85957']\n",
      "\n",
      "\n",
      "네이버 뉴스 score : \n",
      " [0.503149151802063, 0.5029191970825195, 0.5038474202156067, 0.5057459473609924, 0.5059890747070312]\n",
      "\n",
      "\n",
      "SNU 기사 제목 : \n",
      " ['\"카카오페이 얼굴인식, 중국으로 개인정보가 넘어간다\"']\n",
      "\n",
      "\n",
      "SNU 기사 score : \n",
      " [0.503149151802063]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.34.91.146 - - [05/Aug/2021 05:59:58] \"POST / HTTP/1.1\" 200 -\n",
      "INFO:root:POST request,\n",
      "Path: /\n",
      "Headers:\n",
      "Host: 3.37.215.72:8081\n",
      "User-Agent: python-requests/2.25.1\n",
      "Accept-Encoding: gzip, deflate\n",
      "Accept: */*\n",
      "Connection: keep-alive\n",
      "Content-Length: 22\n",
      "\n",
      "\n",
      "\n",
      "Body:\n",
      "카카오톡 유료화\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카카오톡 유료화\n",
      "    단어        빈도\n",
      "0   유료  0.707107\n",
      "1  카카오  0.707107 \n",
      "\n",
      "입력 데이터 : \n",
      " 카카오톡 유료화\n",
      "\n",
      "\n",
      "객관성 여부 : \n",
      " 객관성이 높습니다\n",
      "\n",
      "\n",
      "네이버 뉴스 제목 : \n",
      " [\"카카오의 콘텐츠 구독서비스, 핵심은 '큐레이션'\", '\"어뷰징뉴스 지겨워?\" 취향저격은 카톡에서…새 \\'카카오 뉴스\\'에 쏠리는 눈', '\"취향대로 본다\"…구독 콘텐츠 강화하는 네이버-카카오', \"유료화로 내몰리는 '클라우드 난민'\", \"[FreeView] 카카오모빌리티, '독점'과 '혁신' 사이\"]\n",
      "\n",
      "\n",
      "네이버 뉴스 링크 : \n",
      " ['http://www.the-pr.co.kr/news/articleView.html?idxno=47419', 'https://www.techm.kr/news/articleView.html?idxno=86810', 'https://www.asiatime.co.kr/article/20210804500139', 'https://www.sedaily.com/NewsView/22OWSGUKYL', 'https://www.techm.kr/news/articleView.html?idxno=85957']\n",
      "\n",
      "\n",
      "네이버 뉴스 score : \n",
      " [0.503149151802063, 0.5029191970825195, 0.5038474202156067, 0.5057459473609924, 0.5059890747070312]\n",
      "\n",
      "\n",
      "SNU 기사 제목 : \n",
      " ['\"카카오페이 얼굴인식, 중국으로 개인정보가 넘어간다\"']\n",
      "\n",
      "\n",
      "SNU 기사 score : \n",
      " [0.503149151802063]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.34.91.146 - - [05/Aug/2021 06:01:14] \"POST / HTTP/1.1\" 200 -\n",
      "INFO:root:Stopping httpd...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://jub0t0f0nc.execute-api.ap-northeast-2.amazonaws.com/default/Python_jupyter_test'\n",
    "\n",
    "class S(BaseHTTPRequestHandler):\n",
    "    def _set_response(self):\n",
    "        self.send_response(200)\n",
    "        self.send_header('Content-type', 'text/html')\n",
    "        self.end_headers()\n",
    "\n",
    "    def do_GET(self):\n",
    "        logging.info(\"GET request,\\nPath: %s\\nHeaders:\\n%s\\n\", str(self.path), str(self.headers))\n",
    "        self._set_response()\n",
    "        self.wfile.write(\"GET request for {}\".format(self.path).encode('utf-8'))\n",
    "\n",
    "    def do_POST(self):\n",
    "        content_length = int(self.headers['Content-Length']) # <--- Gets the size of data\n",
    "        post_data = self.rfile.read(content_length) # <--- Gets the data itself\n",
    "        logging.info(\"POST request,\\nPath: %s\\nHeaders:\\n%s\\n\\nBody:\\n%s\\n\",\n",
    "                str(self.path), str(self.headers), post_data.decode('utf-8'))\n",
    "        \n",
    "        response = str(post_data.decode('utf-8'))\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        response1 = ['카카오톡 유료화 서비스 시행하나? 어쩌고 저쩌고 어쩌구 저쩌구 어쩌코 저쩌코 어쩌쿠 저쩌쿠']\n",
    "        print(response)\n",
    "            \n",
    "# ===========================================================================================================================\n",
    "# 단어 토큰화 및 빈도순 정렬 후 추출\n",
    "\n",
    "        tfidf_Vectorizer = TfidfVectorizer(tokenizer=my_Tokenizer, min_df=1) # df 값(단어가 몇 문장들에서 등장하였는지)을 최소 'min_df' 값으로 설정.\n",
    "        X = tfidf_Vectorizer.fit_transform(response_list).toarray()\n",
    "#         print(X.shape)    # X(2차원 배열)의 행,열 수를 출력.\n",
    "#         print(tfidf_Vectorizer.vocabulary_)   # 각 단어들이 배열 X에서 몇번째 열(인덱스 값)에 해당하는지 출력.\n",
    "\n",
    "\n",
    "        #pandas를 활용하여 각 단어들의 각 문장에서의 tf-idf 값들을 모두 더하고, 내림차순으로 정렬하여 상위 n개 출력\n",
    "        count = X.sum(axis=0)    # 2차원 배열 X에서 각 열을 기준으로 합을 구함. (각 단어들의 '최종' tf-idf 값으로 간주.)\n",
    "        word_count = pd.DataFrame({\n",
    "            '단어' : tfidf_Vectorizer.get_feature_names(),\n",
    "            '빈도' : count.flat\n",
    "        })\n",
    "        sorted_df = word_count.sort_values('빈도', ascending=False)\n",
    "        print(sorted_df.head(10), \"\\n\")\n",
    "\n",
    "        word_ = list(np.array(sorted_df['단어'].tolist())) \n",
    "        \n",
    "# ===========================================================================================================================\n",
    "# MySQL 데이터베이스에 응답데이터, 시간 저장\n",
    "\n",
    "        now = datetime.datetime.now()\n",
    "        Korea_DatetimeNow = now + datetime.timedelta(hours = 9)\n",
    "        DatetimeNow = Korea_DatetimeNow.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        query = \"INSERT INTO factcheck.kakao_data (kakao_data, date) VALUES(%s, %s)\"\n",
    "        val = (response, DatetimeNow)\n",
    "        cursor.execute(query, val)\n",
    "        conn.commit()\n",
    "        \n",
    "# ===========================================================================================================================\n",
    "# MySQL 데이터베이스에 키워드 랭킹\n",
    "\n",
    "        rank_list = []\n",
    "\n",
    "        cursor.execute('SELECT keyword FROM keyword_rank')\n",
    "        row = cursor.fetchall()\n",
    "\n",
    "        for i in range(len(row)):\n",
    "            rank_list.append(row[i][0])\n",
    "            \n",
    "        for i in range(0,2):\n",
    "            if word_[i] in rank_list:\n",
    "                query = \"UPDATE keyword_rank SET count = count + 1 WHERE keyword = (%s)\"\n",
    "                val = (word_[i])\n",
    "                cursor.execute(query, val)\n",
    "                conn.commit()\n",
    "            else:\n",
    "                query = \"INSERT INTO factcheck.keyword_rank (keyword, count) VALUES(%s, 1)\"\n",
    "                val = (word_[i])\n",
    "                cursor.execute(query, val)\n",
    "                conn.commit()\n",
    "\n",
    "            conn.close()\n",
    "        \n",
    "        top10_list = []\n",
    "        cursor.execute(\"SELECT keyword FROM keyword_rank ORDER BY count DESC LIMIT 10\")\n",
    "        row_top = cursor.fetchall()\n",
    "        for i in range(len(row_top)):\n",
    "            top10_list.append(row_top[i][0])\n",
    "            \n",
    "# ===========================================================================================================================\n",
    "# 텍스트 객관성 판단\n",
    "\n",
    "        response_kakao = okt.morphs(response, stem=True) #토큰화\n",
    "        response_kakao = [word for word in response if not word in stopwords] #불용어 제거\n",
    "        encoded = tokenizer.texts_to_sequences([response_kakao]) # 정수 인코딩\n",
    "        pad_new = pad_sequences(encoded, maxlen = max_len) #패딩\n",
    "        score = float(kakao_model.predict(pad_new))\n",
    "        result = ''\n",
    "        if(score > 0.8 or score < 0.2):\n",
    "            result = \"객관성이 매우 낮습니다\"\n",
    "        elif(score > 0.65 or score < 0.35):\n",
    "            result = \"객관성이 낮습니다\"\n",
    "        else:\n",
    "            result = \"객관성이 높습니다\"\n",
    "        \n",
    "# ===========================================================================================================================\n",
    "# SNU 기사 검색        \n",
    "        str_expr = \"document.str.contains('{}', case=False)\".format(word_[0]) and \"document.str.contains('{}', case=False)\".format(word_[1])\n",
    "        df_q = train_df.query(str_expr, engine=\"python\")\n",
    "        \n",
    "        title = list(np.array(df_q['document'].tolist()))\n",
    "        # link = list(np.array(df_q['link'].tolist()))\n",
    "        title_list2 = []\n",
    "        # link_list2 = []\n",
    "\n",
    "        for i in range(len(title)):\n",
    "            title_list2.append(title[i])\n",
    "        #     link_list2.append(link[i])\n",
    "        \n",
    "# ===========================================================================================================================\n",
    "# 네이버 뉴스 검색\n",
    "        \n",
    "        baseurl = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=\"\n",
    "\n",
    "        url = baseurl + urllib.parse.quote_plus(response)\n",
    "\n",
    "        html = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        title = soup.find_all(class_ = 'news_tit')\n",
    "\n",
    "        title_list = []\n",
    "        link_list = []\n",
    "\n",
    "        for i in title:\n",
    "            title_list.append(i.attrs['title'])\n",
    "            link_list.append(i.attrs['href'])\n",
    "\n",
    "# ===========================================================================================================================\n",
    "    \n",
    "        tokens_response = []\n",
    "        tokens_response2 = []\n",
    "        score_list = []\n",
    "        score_list2 = []\n",
    "        \n",
    "        for i in range(len(title_list)):\n",
    "            tokens = my_Tokenizer(title_list[i])\n",
    "            tokens_response.append(tokens)\n",
    "\n",
    "        word_seq_response = tokenizer_SNU.texts_to_sequences(tokens_response)\n",
    "        word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "        \n",
    "        for sco in range(len(title_list)):\n",
    "            word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "            score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\n",
    "            \n",
    "        for i in range(len(title_list2)):\n",
    "            tokens2 = my_Tokenizer(title_list2[i])\n",
    "            tokens_response2.append(tokens2)\n",
    "        \n",
    "        word_seq_response2 = tokenizer_SNU.texts_to_sequences(tokens_response2)\n",
    "        word_seq_response2 = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "\n",
    "        for sco2 in range(len(title_list2)):\n",
    "            word_seq_response_to_score2 = word_seq_response2[sco2].reshape(1,max_seq_len)\n",
    "            score_list2.append(float(SNU_model.predict(word_seq_response_to_score2)))\n",
    "        \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'objectivity':result,\n",
    "            'Naver_title':title_list,\n",
    "            'Naver_link':link_list,\n",
    "            'Naver_score':score_list,\n",
    "            'SNU_title':title_list2,\n",
    "            'SNU_score':score_list2\n",
    "            }\n",
    "        \n",
    "#         requests.post(url_lambda, data = response.encode())\n",
    "        requests.post(url_lambda, data = json.dumps(data_))\n",
    "        \n",
    "        print('입력 데이터 : \\n', response)\n",
    "        print('\\n')\n",
    "        print('객관성 여부 : \\n', result)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 제목 : \\n', title_list)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 링크 : \\n', link_list)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 score : \\n', score_list)\n",
    "        print('\\n')\n",
    "        print('SNU 기사 제목 : \\n', title_list2)\n",
    "        print('\\n')\n",
    "#         print('SNU 기사 링크 : ', link_list2)\n",
    "        print('SNU 기사 score : \\n', score_list2)\n",
    "        \n",
    "        self._set_response()\n",
    "        self.wfile.write(\"POST request for {}\".format(self.path).encode('utf-8'))\n",
    "\n",
    "def run(server_class=HTTPServer, handler_class=S, port=8081):\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    server_address = ('', port)\n",
    "    httpd = server_class(server_address, handler_class)\n",
    "    logging.info('Starting httpd...\\n')\n",
    "    try:\n",
    "        httpd.serve_forever()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    httpd.server_close()\n",
    "    logging.info('Stopping httpd...\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from sys import argv\n",
    "\n",
    "    if len(argv) == 2:\n",
    "        run(port=int(argv[1]))\n",
    "    else:\n",
    "        run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-journal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\r\n",
      "python  2658 root   98u  IPv4  29277      0t0  TCP *:tproxy (LISTEN)\r\n"
     ]
    }
   ],
   "source": [
    "# !sudo lsof -i :8081\n",
    "# !sudo kill -9 2658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "anticipated-court",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "{'카카오': 3, '유료': 2, '서비스': 0, '시행': 1, '하나': 4}\n",
      "    단어        빈도\n",
      "0  서비스  0.447214\n",
      "1   시행  0.447214\n",
      "2   유료  0.447214\n",
      "3  카카오  0.447214\n",
      "4   하나  0.447214 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sentences = ['카카오톡 유료화 서비스 시행하나?']\n",
    "\n",
    "my_Tokenizer = MyTokenizer(Okt())\n",
    "tfidf_Vectorizer = TfidfVectorizer(tokenizer=my_Tokenizer, min_df=1) # df 값(단어가 몇 문장들에서 등장하였는지)을 최소 'min_df' 값으로 설정.\n",
    "X = tfidf_Vectorizer.fit_transform(sentences).toarray()\n",
    "print(X.shape)    # X(2차원 배열)의 행,열 수를 출력.\n",
    "print(tfidf_Vectorizer.vocabulary_)   # 각 단어들이 배열 X에서 몇번째 열(인덱스 값)에 해당하는지 출력.\n",
    "\n",
    "\n",
    "#pandas를 활용하여 각 단어들의 각 문장에서의 tf-idf 값들을 모두 더하고, 내림차순으로 정렬하여 상위 n개 출력\n",
    "count = X.sum(axis=0)    # 2차원 배열 X에서 각 열을 기준으로 합을 구함. (각 단어들의 '최종' tf-idf 값으로 간주.)\n",
    "word_count = pd.DataFrame({\n",
    "    '단어' : tfidf_Vectorizer.get_feature_names(),\n",
    "    '빈도' : count.flat\n",
    "})\n",
    "sorted_df = word_count.sort_values('빈도', ascending=False)\n",
    "print(sorted_df.head(10), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "developing-import",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'서비스 시행 유료 카카오 '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word__ = \"\"\n",
    "word_ = list(np.array(sorted_df['단어'].tolist()))\n",
    "for i in range(0, 4):\n",
    "    word__ += word_[i]\n",
    "    word__ += \" \"\n",
    "    \n",
    "word__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "democratic-order",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "네이버↔카카오, 웹툰·웹소설 창작자 지원 경쟁\n",
      "https://zdnet.co.kr/view/?no=20210730151339\n",
      "\n",
      "[아!이뉴스] 정부, 유료방송 규제완화 선언…넷마블 vs 엔씨 '정면승부'\n",
      "http://www.inews24.com/view/1389882\n",
      "\n",
      "\"기사 평점 낮으면 배차 혜택 못 준다\" 카카오 택시 약관 변경\n",
      "http://yna.kr/AKR20210623173600017?did=1195m\n",
      "\n",
      "카카오모빌리티 서비스, 혁신이냐 독점 강화냐\n",
      "https://www.seoul.co.kr/news/newsView.php?id=20210715021008&wlog_tag3=naver\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseurl = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=\"\n",
    "\n",
    "url = baseurl + urllib.parse.quote_plus(word__)\n",
    "\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "title = soup.find_all(class_ = 'news_tit')\n",
    "\n",
    "for i in title:\n",
    "    print(i.attrs['title'])\n",
    "    print(i.attrs['href'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-holiday",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "polyphonic-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>link</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OECD 회원국 중 한국 터키만 상원이 없다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3285</td>\n",
       "      <td>전혀 사실 아님</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>트럼프 \"100조 무기가 탈레반에\", 사실일까?</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3283</td>\n",
       "      <td>대체로 사실 아님</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>대선후보 경선등록 후 완주하지 않으면 독자출마 가능하다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3281</td>\n",
       "      <td>대체로 사실 아님</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>사형 집행해야 사회 안전해진다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3279</td>\n",
       "      <td>대체로 사실 아님</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>사랑제일교회 야외 예배는 방역법을 준수했다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3278</td>\n",
       "      <td>전혀 사실 아님</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>여름엔 긴팔 셔츠보다 반팔이 더 시원할까?</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/210</td>\n",
       "      <td>절반의 사실</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>단종된 갤럭시노트7의 재고 부품을 활용한 삼성전자의 '갤럭시노트FE'는 신제품일까?</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/204</td>\n",
       "      <td>절반의 사실</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>공공와이파이 20만개 설치시 연 4천800억~8천500억원의 데이터요금을 경감할 수...</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/191</td>\n",
       "      <td>대체로 사실</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>참여연대 등이  '통신 기본료 1만1천원 폐지' 공약 이행을 주장하고 있다.   L...</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/187</td>\n",
       "      <td>논쟁중</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>고농도 미세먼지가 발생할 때 최대 86%가 국외 특히 중국에서 온 것으로 나타났다는...</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/71</td>\n",
       "      <td>판단 유보</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4239 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               document  \\\n",
       "0                              OECD 회원국 중 한국 터키만 상원이 없다   \n",
       "1                            트럼프 \"100조 무기가 탈레반에\", 사실일까?   \n",
       "2                        대선후보 경선등록 후 완주하지 않으면 독자출마 가능하다   \n",
       "3                                      사형 집행해야 사회 안전해진다   \n",
       "4                               사랑제일교회 야외 예배는 방역법을 준수했다   \n",
       "...                                                 ...   \n",
       "4234                            여름엔 긴팔 셔츠보다 반팔이 더 시원할까?   \n",
       "4235     단종된 갤럭시노트7의 재고 부품을 활용한 삼성전자의 '갤럭시노트FE'는 신제품일까?   \n",
       "4236  공공와이파이 20만개 설치시 연 4천800억~8천500억원의 데이터요금을 경감할 수...   \n",
       "4237  참여연대 등이  '통신 기본료 1만1천원 폐지' 공약 이행을 주장하고 있다.   L...   \n",
       "4238  고농도 미세먼지가 발생할 때 최대 86%가 국외 특히 중국에서 온 것으로 나타났다는...   \n",
       "\n",
       "                                           link      label  doc_len  \n",
       "0     https://factcheck.snu.ac.kr/v2/facts/3285   전혀 사실 아님        7  \n",
       "1     https://factcheck.snu.ac.kr/v2/facts/3283  대체로 사실 아님        5  \n",
       "2     https://factcheck.snu.ac.kr/v2/facts/3281  대체로 사실 아님        7  \n",
       "3     https://factcheck.snu.ac.kr/v2/facts/3279  대체로 사실 아님        4  \n",
       "4     https://factcheck.snu.ac.kr/v2/facts/3278   전혀 사실 아님        5  \n",
       "...                                         ...        ...      ...  \n",
       "4234   https://factcheck.snu.ac.kr/v2/facts/210     절반의 사실        6  \n",
       "4235   https://factcheck.snu.ac.kr/v2/facts/204     절반의 사실        8  \n",
       "4236   https://factcheck.snu.ac.kr/v2/facts/191     대체로 사실        9  \n",
       "4237   https://factcheck.snu.ac.kr/v2/facts/187        논쟁중       18  \n",
       "4238    https://factcheck.snu.ac.kr/v2/facts/71      판단 유보       15  \n",
       "\n",
       "[4239 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import logging\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from gensim.models import FastText\n",
    "from konlpy.tag import Okt\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    from io import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = ''\n",
    "aws_secret = ''\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'snucsv'\n",
    "\n",
    "object_key = 'SNU_All.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "# load data\n",
    "train_df = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train = train_df[label_names].values\n",
    "\n",
    "train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "removed-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sized-swing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "train_data = train_df.dropna(how = 'any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().values.any()) # Null 값이 존재하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "christian-dispute",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "large-seeker",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4239/4239 [27:09<00:00,  2.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23957"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        clean_words = []  # 정제된 단어 리스트\n",
    "        for word in pos:\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\n",
    "                    #if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\n",
    "                    clean_words.append(word[0])\n",
    "        return clean_words\n",
    "\n",
    "\n",
    "my_Tokenizer = MyTokenizer(Okt())\n",
    "\n",
    "def tfidf_Vectorizer(response):\n",
    "    response_list = []\n",
    "    response_list.append(response)\n",
    "    tfidf_Vectorizer = TfidfVectorizer(tokenizer=my_Tokenizer, min_df=1) # df 값(단어가 몇 문장들에서 등장하였는지)을 최소 'min_df' 값으로 설정.\n",
    "    X = tfidf_Vectorizer.fit_transform(response_list).toarray()\n",
    "#     print(X.shape)    # X(2차원 배열)의 행,열 수를 출력.\n",
    "#     print(tfidf_Vectorizer.vocabulary_)   # 각 단어들이 배열 X에서 몇번째 열(인덱스 값)에 해당하는지 출력.\n",
    "\n",
    "\n",
    "    #pandas를 활용하여 각 단어들의 각 문장에서의 tf-idf 값들을 모두 더하고, 내림차순으로 정렬하여 상위 n개 출력\n",
    "    count = X.sum(axis=0)    # 2차원 배열 X에서 각 열을 기준으로 합을 구함. (각 단어들의 '최종' tf-idf 값으로 간주.)\n",
    "    word_count = pd.DataFrame({\n",
    "        '단어' : tfidf_Vectorizer.get_feature_names(),\n",
    "        '빈도' : count.flat\n",
    "    })\n",
    "    sorted_df = word_count.sort_values('빈도', ascending=False)\n",
    "#     print(sorted_df.head(10), \"\\n\")\n",
    "\n",
    "    word_ = list(np.array(sorted_df['단어'].tolist()))\n",
    "    return(word_)\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36'}\n",
    "\n",
    "okt = Okt()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "title_list = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    response = re.compile(\"[^A-Za-z0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]+\").sub('',sentence)\n",
    "    response = okt.morphs(response, stem=True) # 토큰화\n",
    "    response = [word for word in response if not word in stopwords] # 불용어 제거\n",
    "    response = ' '.join(response)\n",
    "\n",
    "    word_ = tfidf_Vectorizer(response)\n",
    "    word__ = \"\"\n",
    "    if len(word_) <= 10:\n",
    "        word__ = response\n",
    "    else:\n",
    "        for i in range(1, min(len(word_), 11)):\n",
    "            word__ += word_[i]\n",
    "            word__ += \" \"\n",
    "            \n",
    "    baseurl = 'https://search.naver.com/search.naver?where=news&sm=tab_jum&query='\n",
    "    url = baseurl + urllib.parse.quote_plus(word__) \n",
    "\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.raise_for_status()\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    title_ = soup.find_all(class_ = 'news_tit')\n",
    "    \n",
    "    for i in range(1,30):\n",
    "        try:\n",
    "            summary = soup.select_one('#sp_nws{} > div > div > div.news_dsc > div > a'.format(i)).get_text()\n",
    "            title_list.append(summary)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for i in title_:\n",
    "        title_list.append(i.attrs['title'])\n",
    "    \n",
    "len(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ordinary-progress",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23957/23957 [04:24<00:00, 90.48it/s] \n"
     ]
    }
   ],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "okt = Okt()\n",
    "tokenized_data = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    tokenized_data.append(temp_X)\n",
    "    \n",
    "for sentence in tqdm(title_list):\n",
    "    temp_X = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    tokenized_data.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bigger-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "model = FastText(sentences = tokenized_data, window = 5, min_count = 5, workers = 4, sg = 1)\n",
    "\n",
    "model.save('similar_keyword_model_SNU_Naver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unsigned-perspective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('남매', 0.8166605234146118), ('김정일', 0.7965680956840515), ('김여정', 0.7960783243179321), ('남한', 0.7897870540618896), ('탈레반', 0.7823539972305298)]\n"
     ]
    }
   ],
   "source": [
    "loaded_model = FastText.load('similar_keyword_model_SNU_Naver')\n",
    "print(loaded_model.wv.most_similar('김정은', topn=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

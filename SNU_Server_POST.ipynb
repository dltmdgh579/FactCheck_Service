{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extensive-israel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extension horovod.torch has not been built: /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-37m-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n",
      "[2021-08-10 10:13:37.399 ip-172-31-2-138:1660 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-08-10 10:13:37.420 ip-172-31-2-138:1660 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    }
   ],
   "source": [
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import logging\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from gensim.models import FastText\n",
    "from konlpy.tag import Okt\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pymysql\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import csv\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "# SNU_model = load_model('SNU_LSTM_Model.h5')\n",
    "SNU_model = load_model('SNU_LSTM_Model_shuffle.h5')\n",
    "\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        clean_words = []  # 정제된 단어 리스트\n",
    "        for word in pos:\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\n",
    "                    #if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\n",
    "                    clean_words.append(word[0])\n",
    "        return clean_words\n",
    "\n",
    "\n",
    "my_Tokenizer = MyTokenizer(Okt())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-luther",
   "metadata": {},
   "source": [
    "**MySQL 데이터베이스 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "mysterious-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '' # rds endpoint\n",
    "port = 3306\n",
    "username = ''\n",
    "database = ''\n",
    "password = ''\n",
    "\n",
    "def connect_RDS(host, port, username, password, database):\n",
    "    try:\n",
    "        conn = pymysql.connect(host = host, user = username, password = password, db = database, port = port, use_unicode = True, charset = 'utf8')\n",
    "        cursor = conn.cursor()\n",
    "    \n",
    "    except:\n",
    "        logging.error('RDS에 연결되지 않았습니다.')\n",
    "        sys.exit(1)\n",
    "        \n",
    "    return conn, cursor\n",
    "\n",
    "conn, cursor = connect_RDS(host, port, username, password, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "disabled-course",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‘정치 세대교체’ 바람 일으킨 이준석, 대선엔 출마 못한다</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>액상형 전자담배는 니코틴이 함유되지 않아 담배가 아니다</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>한국 백신 접종률 세계 100위권 이하다</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>광역도시 없는 지역권은 실제 수요와 관계없이 획일적으로 국가교통망 정책에서 소외되고 있다</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>\" ‘당·정·청 전원회의’는 '운동권 용어'다\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220</th>\n",
       "      <td>\"늘어가는 해양사고, 전북 관내 잠수구조인력 전무(全無)\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221</th>\n",
       "      <td>\"청와대 하루 평균 업무추진비, 이명박 768만원, 박근혜 814만원, 문재인 55만원\"</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>국민연금공단 기금운용본부가 전주로 이전한 후, 해외투자자가 국민연금을 '패싱'한다</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>문재인 대통령이 미국 방문에서 푸대접 받았다</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1224 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               document  label  doc_len\n",
       "0                  현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다      1        9\n",
       "1                      ‘정치 세대교체’ 바람 일으킨 이준석, 대선엔 출마 못한다      1        8\n",
       "2                        액상형 전자담배는 니코틴이 함유되지 않아 담배가 아니다      1        7\n",
       "3                                한국 백신 접종률 세계 100위권 이하다      1        6\n",
       "4     광역도시 없는 지역권은 실제 수요와 관계없이 획일적으로 국가교통망 정책에서 소외되고 있다      1       11\n",
       "...                                                 ...    ...      ...\n",
       "1219                        \" ‘당·정·청 전원회의’는 '운동권 용어'다\"?      0        5\n",
       "1220                  \"늘어가는 해양사고, 전북 관내 잠수구조인력 전무(全無)\"?      0        6\n",
       "1221  \"청와대 하루 평균 업무추진비, 이명박 768만원, 박근혜 814만원, 문재인 55만원\"      0       10\n",
       "1222      국민연금공단 기금운용본부가 전주로 이전한 후, 해외투자자가 국민연금을 '패싱'한다      0        8\n",
       "1223                           문재인 대통령이 미국 방문에서 푸대접 받았다      0        6\n",
       "\n",
       "[1224 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if sys.version_info[0] < 3:\n",
    "    from io import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = ''\n",
    "aws_secret = ''\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'snucsv'\n",
    "\n",
    "object_key = 'SNU_Data_1200.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "# load data\n",
    "train_df = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train = train_df[label_names].values\n",
    "\n",
    "train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-climb",
   "metadata": {},
   "source": [
    "**단어 벡터화 및 패딩**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sunrise-squad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1224 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1224/1224 [00:05<00:00, 209.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size : 3999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 503, 1459,  504, ..., 1461, 1462,    8],\n",
       "       [   0,   72,  761, ...,   51,  360,    5],\n",
       "       [   0,    0, 1466, ...,  506,  763,   41],\n",
       "       ...,\n",
       "       [ 649,  177,  381, ..., 3992,   13, 3993],\n",
       "       [3994, 3995, 3996, ...,  248, 3998,    5],\n",
       "       [   0,    0,    0, ..., 1401, 3999,  496]], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs_train = train_df['document'].tolist()\n",
    "num_classes = len(label_names)\n",
    "print(num_classes)\n",
    "processed_docs_train = []\n",
    "\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = my_Tokenizer(doc)\n",
    "    processed_docs_train.append(tokens)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(processed_docs_train)\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "word_index = tokenizer.word_index\n",
    "print('dictionary size :', len(word_index))\n",
    "\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "word_seq_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "distributed-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-operator",
   "metadata": {},
   "source": [
    "**사용자에게 받은 데이터 전처리 테스트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "floating-cheese",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[503, 1459, 504, 1460, 22, 71, 104, 1461, 1462, 8]\n",
      "(3, 10)\n",
      "[[ 503 1459  504 1460   22   71  104 1461 1462    8]]\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "   Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/bidirectional/forward_lstm/PartitionedCall]] [Op:__inference_predict_function_10128]\n\nFunction call stack:\npredict_function -> predict_function -> predict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a0f6523c1c32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mword_seq_response_to_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_seq_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msco\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_seq_response_to_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mscore_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSNU_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_seq_response_to_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1684\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1686\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1687\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 895\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_latest_p37/gpu_cuda11.0/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m:    Fail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential/bidirectional/forward_lstm/PartitionedCall]] [Op:__inference_predict_function_10128]\n\nFunction call stack:\npredict_function -> predict_function -> predict_function\n"
     ]
    }
   ],
   "source": [
    "response = ['현재 AI기술은 불완전하기 때문에 가짜뉴스 완전히 걸러낼 수 없다', 'ajsdlkfakljflalejlka', 'dshkjlafjsdkfjalks']\n",
    "tokens_response = []\n",
    "\n",
    "for i in range(len(response)):\n",
    "    tokens = my_Tokenizer(response[i])\n",
    "    tokens_response.append(tokens)\n",
    "#     print(tokens_response)\n",
    "\n",
    "word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "print(word_seq_response[0])\n",
    "word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "print(word_seq_response.shape)\n",
    "\n",
    "score_list = []\n",
    "for sco in range(len(word_seq_response)):\n",
    "    word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "    print(word_seq_response_to_score)\n",
    "    score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\n",
    "    print(score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-extra",
   "metadata": {},
   "source": [
    "**키워드, 기사제목 입력시 웹서버**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "continued-sample",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting httpd...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2)\n",
      "{'카카오': 1, '유료': 0}\n",
      "    단어        빈도\n",
      "0   유료  0.707107\n",
      "1  카카오  0.707107 \n",
      "\n",
      "입력 데이터 : \n",
      " 카카오톡 유료화\n",
      "\n",
      "\n",
      "네이버 뉴스 제목 : \n",
      " ['카카오모빌리티 잇따른 요금인상...\"수금본색\"vs\"시장논리\"', \"넷플릭스식 '네이버' 유튜브식 '카카오'...같은듯 다른 인터넷 양강의 구독 실험\", '대리운전연합 \"카카오·SKT, 전화콜마저 진출…골목시장 침탈\"', '“카카오·티맵, 대리콜까지 빼앗냐”']\n",
      "\n",
      "\n",
      "네이버 뉴스 링크 : \n",
      " ['https://zdnet.co.kr/view/?no=20210809105033', 'https://www.techm.kr/news/articleView.html?idxno=86902', 'https://news.joins.com/article/olink/23716744', 'https://news.joins.com/article/olink/23716962']\n",
      "\n",
      "\n",
      "네이버 뉴스 score : \n",
      " [0.5055016875267029, 0.5024815797805786, 0.5055860280990601, 0.5053588151931763]\n",
      "\n",
      "\n",
      "SNU 기사 제목 : \n",
      " ['\"카카오페이 얼굴인식, 중국으로 개인정보가 넘어간다\"']\n",
      "\n",
      "\n",
      "SNU 기사 score : \n",
      " [0.5055016875267029]\n",
      "(1, 2)\n",
      "{'카카오': 1, '유료': 0}\n",
      "    단어        빈도\n",
      "0   유료  0.707107\n",
      "1  카카오  0.707107 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.36.48.61 - - [09/Aug/2021 05:51:04] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터 : \n",
      " 카카오톡 유료화\n",
      "\n",
      "\n",
      "네이버 뉴스 제목 : \n",
      " ['카카오모빌리티 잇따른 요금인상...\"수금본색\"vs\"시장논리\"', \"넷플릭스식 '네이버' 유튜브식 '카카오'...같은듯 다른 인터넷 양강의 구독 실험\", '대리운전연합 \"카카오·SKT, 전화콜마저 진출…골목시장 침탈\"', '“카카오·티맵, 대리콜까지 빼앗냐”']\n",
      "\n",
      "\n",
      "네이버 뉴스 링크 : \n",
      " ['https://zdnet.co.kr/view/?no=20210809105033', 'https://www.techm.kr/news/articleView.html?idxno=86902', 'https://news.joins.com/article/olink/23716744', 'https://news.joins.com/article/olink/23716962']\n",
      "\n",
      "\n",
      "네이버 뉴스 score : \n",
      " [0.5055016875267029, 0.5024815797805786, 0.5055860280990601, 0.5053588151931763]\n",
      "\n",
      "\n",
      "SNU 기사 제목 : \n",
      " ['\"카카오페이 얼굴인식, 중국으로 개인정보가 넘어간다\"']\n",
      "\n",
      "\n",
      "SNU 기사 score : \n",
      " [0.5055016875267029]\n",
      "(1, 2)\n",
      "{'카카오': 1, '유료': 0}\n",
      "    단어        빈도\n",
      "0   유료  0.707107\n",
      "1  카카오  0.707107 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13.209.10.79 - - [09/Aug/2021 05:51:08] \"POST / HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터 : \n",
      " 카카오톡 유료화\n",
      "\n",
      "\n",
      "네이버 뉴스 제목 : \n",
      " ['카카오모빌리티 잇따른 요금인상...\"수금본색\"vs\"시장논리\"', \"넷플릭스식 '네이버' 유튜브식 '카카오'...같은듯 다른 인터넷 양강의 구독 실험\", '대리운전연합 \"카카오·SKT, 전화콜마저 진출…골목시장 침탈\"', '“카카오·티맵, 대리콜까지 빼앗냐”']\n",
      "\n",
      "\n",
      "네이버 뉴스 링크 : \n",
      " ['https://zdnet.co.kr/view/?no=20210809105033', 'https://www.techm.kr/news/articleView.html?idxno=86902', 'https://news.joins.com/article/olink/23716744', 'https://news.joins.com/article/olink/23716962']\n",
      "\n",
      "\n",
      "네이버 뉴스 score : \n",
      " [0.5055016875267029, 0.5024815797805786, 0.5055860280990601, 0.5053588151931763]\n",
      "\n",
      "\n",
      "SNU 기사 제목 : \n",
      " ['\"카카오페이 얼굴인식, 중국으로 개인정보가 넘어간다\"']\n",
      "\n",
      "\n",
      "SNU 기사 score : \n",
      " [0.5055016875267029]\n",
      "(1, 2)\n",
      "{'카카오': 1, '유료': 0}\n",
      "    단어        빈도\n",
      "0   유료  0.707107\n",
      "1  카카오  0.707107 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13.209.10.79 - - [09/Aug/2021 05:51:11] \"POST / HTTP/1.1\" 200 -\n",
      "INFO:root:Stopping httpd...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url_lambda = 'https://jub0t0f0nc.execute-api.ap-northeast-2.amazonaws.com/default/Python_jupyter_test'\n",
    "\n",
    "class S(BaseHTTPRequestHandler):\n",
    "    def _set_response(self):\n",
    "        self.send_response(200)\n",
    "        self.send_header('Content-type', 'text/html')\n",
    "        self.end_headers()\n",
    "\n",
    "    def do_GET(self):\n",
    "        logging.info(\"GET request,\\nPath: %s\\nHeaders:\\n%s\\n\", str(self.path), str(self.headers))\n",
    "        self._set_response()\n",
    "        self.wfile.write(\"GET request for {}\".format(self.path).encode('utf-8'))\n",
    "\n",
    "    def do_POST(self):\n",
    "        content_length = int(self.headers['Content-Length']) # <--- Gets the size of data\n",
    "        post_data = self.rfile.read(content_length) # <--- Gets the data itself\n",
    "#         logging.info(\"POST request,\\nPath: %s\\nHeaders:\\n%s\\n\\nBody:\\n%s\\n\",\n",
    "#                 str(self.path), str(self.headers), post_data.decode('utf-8'))\n",
    "        \n",
    "        response = str(post_data.decode('utf-8'))\n",
    "        response_list = []\n",
    "        response_list.append(response)\n",
    "        response1 = ['카카오톡 유료화 서비스 시행하나?']\n",
    "        \n",
    "        \n",
    "# ===========================================================================================================================\n",
    "# 단어 토큰화 및 빈도순 정렬 후 추출\n",
    "\n",
    "        tfidf_Vectorizer = TfidfVectorizer(tokenizer=my_Tokenizer, min_df=1) # df 값(단어가 몇 문장들에서 등장하였는지)을 최소 'min_df' 값으로 설정.\n",
    "        X = tfidf_Vectorizer.fit_transform(response_list).toarray()\n",
    "        print(X.shape)    # X(2차원 배열)의 행,열 수를 출력.\n",
    "        print(tfidf_Vectorizer.vocabulary_)   # 각 단어들이 배열 X에서 몇번째 열(인덱스 값)에 해당하는지 출력.\n",
    "\n",
    "\n",
    "        #pandas를 활용하여 각 단어들의 각 문장에서의 tf-idf 값들을 모두 더하고, 내림차순으로 정렬하여 상위 n개 출력\n",
    "        count = X.sum(axis=0)    # 2차원 배열 X에서 각 열을 기준으로 합을 구함. (각 단어들의 '최종' tf-idf 값으로 간주.)\n",
    "        word_count = pd.DataFrame({\n",
    "            '단어' : tfidf_Vectorizer.get_feature_names(),\n",
    "            '빈도' : count.flat\n",
    "        })\n",
    "        sorted_df = word_count.sort_values('빈도', ascending=False)\n",
    "        print(sorted_df.head(10), \"\\n\")\n",
    "\n",
    "        word_ = list(np.array(sorted_df['단어'].tolist())) \n",
    "        \n",
    "# ===========================================================================================================================\n",
    "# MySQL 데이터베이스에 응답데이터, 시간 저장\n",
    "        \n",
    "        global cursor, conn\n",
    "        conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "        \n",
    "        now = datetime.datetime.now()\n",
    "        Korea_DatetimeNow = now + datetime.timedelta(hours = 9)\n",
    "        DatetimeNow = Korea_DatetimeNow.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        query = \"INSERT INTO factcheck.article_data (article_data, date) VALUES(%s, %s)\"\n",
    "        val = (response, DatetimeNow)\n",
    "        cursor.execute(query, val)\n",
    "        conn.commit()\n",
    "        \n",
    "# ===========================================================================================================================\n",
    "# MySQL 데이터베이스에 키워드 랭킹\n",
    "\n",
    "        rank_list = []\n",
    "\n",
    "        cursor.execute('SELECT keyword FROM keyword_rank')\n",
    "        row = cursor.fetchall()\n",
    "\n",
    "        for i in range(len(row)):\n",
    "            rank_list.append(row[i][0])\n",
    "            \n",
    "        for i in range(0,2):\n",
    "            if word_[i] in rank_list:\n",
    "                query = \"UPDATE keyword_rank SET count = count + 1 WHERE keyword = (%s)\"\n",
    "                val = (word_[i])\n",
    "                cursor.execute(query, val)\n",
    "                conn.commit()\n",
    "            else:\n",
    "                query = \"INSERT INTO factcheck.keyword_rank (keyword, count) VALUES(%s, 1)\"\n",
    "                val = (word_[i])\n",
    "                cursor.execute(query, val)\n",
    "                conn.commit()\n",
    "\n",
    "            conn.close()\n",
    "        \n",
    "        top10_list = []\n",
    "        cursor.execute(\"SELECT keyword FROM keyword_rank ORDER BY count DESC LIMIT 10\")\n",
    "        row_top = cursor.fetchall()\n",
    "        for i in range(len(row_top)):\n",
    "            top10_list.append(row_top[i][0])\n",
    "        \n",
    "# ===========================================================================================================================\n",
    "# SNU 기사 검색      \n",
    "\n",
    "        try:\n",
    "            str_expr = \"document.str.contains('{}', case=False)\".format(word_[0]) and \"document.str.contains('{}', case=False)\".format(word_[1])\n",
    "            df_q = train_df.query(str_expr, engine=\"python\")\n",
    "\n",
    "            title = list(np.array(df_q['document'].tolist()))\n",
    "            # link = list(np.array(df_q['link'].tolist()))\n",
    "            title_list2 = []\n",
    "            # link_list2 = []\n",
    "\n",
    "            for i in range(len(title)):\n",
    "                title_list2.append(title[i])\n",
    "            #     link_list2.append(link[i])\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "# ===========================================================================================================================\n",
    "# 네이버 뉴스 검색\n",
    "\n",
    "        baseurl = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=\"\n",
    "\n",
    "        url = baseurl + urllib.parse.quote_plus(response)\n",
    "\n",
    "        html = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        title = soup.find_all(class_ = 'news_tit')\n",
    "\n",
    "        title_list = []\n",
    "        link_list = []\n",
    "\n",
    "        for i in title:\n",
    "            title_list.append(i.attrs['title'])\n",
    "            link_list.append(i.attrs['href'])\n",
    "\n",
    "# ===========================================================================================================================\n",
    "\n",
    "        tokens_response = []\n",
    "        tokens_response2 = []\n",
    "        score_list = []\n",
    "        score_list2 = []\n",
    "        \n",
    "        for i in range(len(title_list)):\n",
    "            tokens = my_Tokenizer(title_list[i])\n",
    "            tokens_response.append(tokens)\n",
    "\n",
    "        word_seq_response = tokenizer.texts_to_sequences(tokens_response)\n",
    "        word_seq_response = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "        \n",
    "        for sco in range(len(title_list)):\n",
    "            word_seq_response_to_score = word_seq_response[sco].reshape(1,max_seq_len)\n",
    "            score_list.append(float(SNU_model.predict(word_seq_response_to_score)))\n",
    "        \n",
    "        try:\n",
    "            for i in range(len(title_list2)):\n",
    "                tokens2 = my_Tokenizer(title_list2[i])\n",
    "                tokens_response2.append(tokens2)\n",
    "\n",
    "            word_seq_response2 = tokenizer.texts_to_sequences(tokens_response2)\n",
    "            word_seq_response2 = sequence.pad_sequences(word_seq_response, maxlen=max_seq_len)\n",
    "\n",
    "            for sco2 in range(len(title_list2)):\n",
    "                word_seq_response_to_score2 = word_seq_response2[sco2].reshape(1,max_seq_len)\n",
    "                score_list2.append(float(SNU_model.predict(word_seq_response_to_score2)))\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        data_ = {\n",
    "            'response':response,\n",
    "            'Naver_title':title_list,\n",
    "            'Naver_link':link_list,\n",
    "            'Naver_score':score_list,\n",
    "            'SNU_title':title_list2,\n",
    "            'SNU_score':score_list2,\n",
    "            'top10_keyword':top10_list\n",
    "            }\n",
    "        \n",
    "#         requests.post(url_lambda, data = response.encode())\n",
    "        requests.post(url_lambda, data = json.dumps(data_))\n",
    "        \n",
    "        print('입력 데이터 : \\n', response)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 제목 : \\n', title_list)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 링크 : \\n', link_list)\n",
    "        print('\\n')\n",
    "        print('네이버 뉴스 score : \\n', score_list)\n",
    "        print('\\n')\n",
    "        try:\n",
    "            print('SNU 기사 제목 : \\n', title_list2)\n",
    "            print('\\n')\n",
    "    #         print('SNU 기사 링크 : ', link_list2)\n",
    "            print('SNU 기사 score : \\n', score_list2)\n",
    "        except:\n",
    "            print('SNU 기사가 없습니다. \\n')\n",
    "\n",
    "        self._set_response()\n",
    "        self.wfile.write(\"POST request for {}\".format(self.path).encode('utf-8'))\n",
    "\n",
    "def run(server_class=HTTPServer, handler_class=S, port=8080):\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    server_address = ('', port)\n",
    "    httpd = server_class(server_address, handler_class)\n",
    "    logging.info('Starting httpd...\\n')\n",
    "    try:\n",
    "        httpd.serve_forever()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    httpd.server_close()\n",
    "    logging.info('Stopping httpd...\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from sys import argv\n",
    "\n",
    "    if len(argv) == 2:\n",
    "        run(port=int(argv[1]))\n",
    "    else:\n",
    "        run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "external-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "response = '유후~'\n",
    "rank_list = []\n",
    "\n",
    "cursor.execute('SELECT keyword FROM keyword_rank')\n",
    "row = cursor.fetchall()\n",
    "\n",
    "for i in range(len(row)):\n",
    "    rank_list.append(row[i][0])\n",
    "    \n",
    "if response in rank_list:\n",
    "    query = \"UPDATE keyword_rank SET count = count + 1 WHERE keyword = (%s)\"\n",
    "    val = (response)\n",
    "    cursor.execute(query, val)\n",
    "    conn.commit()\n",
    "else:\n",
    "    query = \"INSERT INTO factcheck.keyword_rank (keyword, count) VALUES(%s, 1)\"\n",
    "    val = (response)\n",
    "    cursor.execute(query, val)\n",
    "    conn.commit()\n",
    "    \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "grand-africa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['카카오톡 유료화', '성공했군', '성공했나', '유후~']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn, cursor = connect_RDS(host, port, username, password, database)\n",
    "top10_list = []\n",
    "cursor.execute(\"SELECT keyword FROM keyword_rank ORDER BY count DESC LIMIT 10\")\n",
    "# cursor.execute(\"SELECT keyword FROM keyword_rank LIMIT 4\")\n",
    "row = cursor.fetchall()\n",
    "for i in range(len(row)):\n",
    "    top10_list.append(row[i][0])\n",
    "\n",
    "top10_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "radio-lyric",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import codecs\n",
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "\n",
    "# embeddings_index = {}\n",
    "# f = codecs.open('/home/ubuntu/FastText/wiki.ko.vec', encoding='utf-8')\n",
    "\n",
    "# for line in tqdm(f):\n",
    "#     values = line.rstrip().rsplit(' ')\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "trying-repeat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   try:\n",
    "#     # Currently, memory growth needs to be the same across GPUs\n",
    "#     for gpu in gpus:\n",
    "#       tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#     print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#   except RuntimeError as e:\n",
    "#     # Memory growth must be set before GPUs have been initialized\n",
    "#     print(e)\n",
    "    \n",
    "# SNU_model = load_model('SNU_LSTM_Model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "prerequisite-liver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5)\n",
      "{'카카오': 3, '유료': 2, '서비스': 0, '시행': 1, '하나': 4}\n",
      "    단어        빈도\n",
      "0  서비스  0.447214\n",
      "1   시행  0.447214\n",
      "2   유료  0.447214\n",
      "3  카카오  0.447214\n",
      "4   하나  0.447214 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sentences = ['카카오톡 유료화 서비스 시행하나?']\n",
    "\n",
    "tfidf_Vectorizer = TfidfVectorizer(tokenizer=my_Tokenizer, min_df=1) # df 값(단어가 몇 문장들에서 등장하였는지)을 최소 'min_df' 값으로 설정.\n",
    "X = tfidf_Vectorizer.fit_transform(sentences).toarray()\n",
    "print(X.shape)    # X(2차원 배열)의 행,열 수를 출력.\n",
    "print(tfidf_Vectorizer.vocabulary_)   # 각 단어들이 배열 X에서 몇번째 열(인덱스 값)에 해당하는지 출력.\n",
    "\n",
    "\n",
    "#pandas를 활용하여 각 단어들의 각 문장에서의 tf-idf 값들을 모두 더하고, 내림차순으로 정렬하여 상위 n개 출력\n",
    "count = X.sum(axis=0)    # 2차원 배열 X에서 각 열을 기준으로 합을 구함. (각 단어들의 '최종' tf-idf 값으로 간주.)\n",
    "word_count = pd.DataFrame({\n",
    "    '단어' : tfidf_Vectorizer.get_feature_names(),\n",
    "    '빈도' : count.flat\n",
    "})\n",
    "sorted_df = word_count.sort_values('빈도', ascending=False)\n",
    "print(sorted_df.head(10), \"\\n\")\n",
    "\n",
    "word_ = list(np.array(sorted_df['단어'].tolist())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sealed-action",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카카오式 문어발 확장에 진통…\"불공정 유료화 국토부와 논의중\"\n",
      "http://www.dizzotv.com/site/data/html_dir/2021/04/22/2021042280184.html\n",
      "\n",
      "카카오T 유료화에 뿔난 택시업계... \"불공정 배차 조사해야\"\n",
      "http://biz.newdaily.co.kr/site/data/html/2021/04/27/2021042700001.html\n",
      "\n",
      "서민의 '프렌즈' 되랬더니, 고신용 고객만 쓸어담은 카카오뱅크\n",
      "http://news.mt.co.kr/mtview.php?no=2021041601224584614\n",
      "\n",
      "카카오 쏟아지는 이슈…주가 어디까지(종합)\n",
      "http://www.smedaily.co.kr/news/articleView.html?idxno=201337\n",
      "\n",
      "['카카오式 문어발 확장에 진통…\"불공정 유료화 국토부와 논의중\"', '카카오T 유료화에 뿔난 택시업계... \"불공정 배차 조사해야\"', \"서민의 '프렌즈' 되랬더니, 고신용 고객만 쓸어담은 카카오뱅크\", '카카오 쏟아지는 이슈…주가 어디까지(종합)']\n",
      "['http://www.dizzotv.com/site/data/html_dir/2021/04/22/2021042280184.html', 'http://biz.newdaily.co.kr/site/data/html/2021/04/27/2021042700001.html', 'http://news.mt.co.kr/mtview.php?no=2021041601224584614', 'http://www.smedaily.co.kr/news/articleView.html?idxno=201337']\n"
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "baseurl = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=\"\n",
    "response = ['카카오톡 유료화 서비스 시행하나?']\n",
    "\n",
    "url = baseurl + urllib.parse.quote_plus(response[0])\n",
    "\n",
    "html = urllib.request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "title = soup.find_all(class_ = 'news_tit')\n",
    "\n",
    "title_list = []\n",
    "link_list = []\n",
    "\n",
    "for i in title:\n",
    "    print(i.attrs['title'])\n",
    "    print(i.attrs['href'])\n",
    "    print()\n",
    "    title_list.append(i.attrs['title'])\n",
    "    link_list.append(i.attrs['href'])\n",
    "    \n",
    "print(title_list)\n",
    "print(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "biological-collector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>\"유료광고 속인 인플루언서, 제재 못 한다\"</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     document  label  doc_len\n",
       "293  \"유료광고 속인 인플루언서, 제재 못 한다\"      1        6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_expr = \"document.str.contains('{}', case=False)\".format(word_[3]) and \"document.str.contains('{}', case=False)\".format(word_[2])\n",
    "df_q = train_df.query(str_expr, engine=\"python\")\n",
    "df_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = list(np.array(df_q['document'].tolist()))\n",
    "# link = list(np.array(df_q['link'].tolist()))\n",
    "title_list2 = []\n",
    "# link_list2 = []\n",
    "\n",
    "for i in range(len(title)):\n",
    "    title_list2.append(title[i])\n",
    "#     link_list2.append(link[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

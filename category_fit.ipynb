{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import codecs\r\n",
    "import boto3\r\n",
    "import numpy as np\r\n",
    "from keras.callbacks import EarlyStopping\r\n",
    "from keras.layers import Embedding, Dropout, Dense, Bidirectional, LSTM\r\n",
    "from konlpy.tag import Okt\r\n",
    "import pandas as pd\r\n",
    "from tqdm import tqdm\r\n",
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from keras.preprocessing import sequence\r\n",
    "from io import StringIO\r\n",
    "\r\n",
    "OBJECT_KEY_DIC = {\"all\": \"SNU_All_b.csv\", \"politics\": \"SNU_정치_b.csv\", \"economics\": \"SNU_경제_b.csv\",\r\n",
    "                  \"society\": \"SNU_사회_b.csv\", \"etc\": \"SNU_기타_b.csv\"}\r\n",
    "\r\n",
    "# AWS에서는 loadWikiModel 메소드의 f.open 파라미터를 아래로 바꿀것.\r\n",
    "WIKI_VEC_DIR = \"/home/ubuntu/FastText/wiki.ko.vec\"\r\n",
    "\r\n",
    "LOCAL_WIKI_VEC_DIR = \"./wiki.ko.vec\"\r\n",
    "\r\n",
    "# training params\r\n",
    "BATCH_SIZE = 256\r\n",
    "NUM_EPOCHS = 40\r\n",
    "\r\n",
    "# model parameters\r\n",
    "MAX_NB_WORDS = 100000\r\n",
    "NUM_FILTERS = 64\r\n",
    "EMBED_DIM = 300\r\n",
    "WEIGHT_DECAY = 1e-4\r\n",
    "EMBEDDING_INDEX = {}\r\n",
    "\r\n",
    "\r\n",
    "# Vectorizer의 argument인 tokenizer에 KoNLPy의 pos 함수로 대체.\r\n",
    "class MyTokenizer:\r\n",
    "    def __init__(self, tagger):\r\n",
    "        self.tagger = tagger\r\n",
    "\r\n",
    "    def __call__(self, sent):\r\n",
    "        pos = self.tagger.pos(sent)\r\n",
    "        clean_words = []  # 정제된 단어 리스트\r\n",
    "        for word in pos:\r\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\r\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\r\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\r\n",
    "                    # if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\r\n",
    "                    clean_words.append(word[0])\r\n",
    "        return clean_words\r\n",
    "\r\n",
    "\r\n",
    "def string_from_AWS() -> str:\r\n",
    "    # get your credentials from environment variables\r\n",
    "    AWS_ID = ''\r\n",
    "    AWS_SECRET = ''\r\n",
    "\r\n",
    "    client = boto3.client('s3', aws_access_key_id=AWS_ID,\r\n",
    "                          aws_secret_access_key=AWS_SECRET)\r\n",
    "    bucket_name = 'snucsv'\r\n",
    "\r\n",
    "    # OBJECT_KEY_DIC 참고.\r\n",
    "    object_key = OBJECT_KEY_DIC[\"economics\"]\r\n",
    "    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\r\n",
    "    body = csv_obj['Body']\r\n",
    "    csv_string = body.read().decode('cp949')\r\n",
    "\r\n",
    "    return csv_string\r\n",
    "\r\n",
    "\r\n",
    "# load data\r\n",
    "def stringToTrainDf(csv_string_: str) -> tuple:\r\n",
    "    train_df = pd.read_csv(StringIO(csv_string_))\r\n",
    "\r\n",
    "    train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\r\n",
    "\r\n",
    "    # 모델 학습에 파라미터로 쓰임.\r\n",
    "    max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\r\n",
    "    return train_df, max_seq_len\r\n",
    "\r\n",
    "\r\n",
    "def loadWikiModel():\r\n",
    "    f = codecs.open(WIKI_VEC_DIR, encoding='utf-8')\r\n",
    "\r\n",
    "    for line in tqdm(f):\r\n",
    "        values = line.rstrip().rsplit(' ')\r\n",
    "        word = values[0]\r\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\r\n",
    "        EMBEDDING_INDEX[word] = coefs\r\n",
    "    f.close()\r\n",
    "\r\n",
    "\r\n",
    "def word_indexing(train_df_, max_seq_len_) -> tuple:\r\n",
    "    my_Tokenizer = MyTokenizer(Okt())\r\n",
    "\r\n",
    "    raw_docs_train = train_df_['document'].tolist()\r\n",
    "    # raw_docs_test = test_df['document'].tolist()\r\n",
    "    # print(raw_docs_test)\r\n",
    "\r\n",
    "    processed_docs_train = []\r\n",
    "    # processed_docs_test = []\r\n",
    "\r\n",
    "    for doc in tqdm(raw_docs_train):\r\n",
    "        tokens = my_Tokenizer(doc)\r\n",
    "        processed_docs_train.append(tokens)\r\n",
    "\r\n",
    "    # for doc in tqdm(raw_docs_test):\r\n",
    "    #     tokens = my_Tokenizer(doc)\r\n",
    "    #     processed_docs_test.append(tokens)\r\n",
    "\r\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\r\n",
    "    # tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)\r\n",
    "    tokenizer.fit_on_texts(processed_docs_train)\r\n",
    "    # tokenizer.fit_on_texts(processed_docs_train+proccessed_docs_test)\r\n",
    "    word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\r\n",
    "    word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len_)\r\n",
    "    # word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\r\n",
    "\r\n",
    "    word_index = tokenizer.word_index\r\n",
    "    print(\"dictionary size : \", len(word_index))\r\n",
    "    return word_index, word_seq_train\r\n",
    "\r\n",
    "\r\n",
    "def makingModel(word_index_, max_seq_len_):\r\n",
    "    # embedding matrix\r\n",
    "    words_not_found = []\r\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index_) + 1)\r\n",
    "    embedding_matrix = np.zeros((nb_words, EMBED_DIM))\r\n",
    "\r\n",
    "    # print(word_index)\r\n",
    "    for word, i in word_index_.items():\r\n",
    "        if i >= nb_words:\r\n",
    "            continue\r\n",
    "        embedding_vector = EMBEDDING_INDEX.get(word)\r\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0:\r\n",
    "            embedding_matrix[i] = embedding_vector\r\n",
    "        else:\r\n",
    "            words_not_found.append(word)\r\n",
    "            # cc.ko.300.vec에서 찾지 못한 단어들의 리스트.\r\n",
    "    print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\r\n",
    "    print(\"sample words not found: \", np.random.choice(words_not_found, 10))\r\n",
    "\r\n",
    "    from keras.layers import BatchNormalization\r\n",
    "    import tensorflow as tf\r\n",
    "\r\n",
    "    model = tf.keras.Sequential()\r\n",
    "\r\n",
    "    model.add(Embedding(nb_words, EMBED_DIM, input_length=max_seq_len_, weights=[embedding_matrix], trainable=False))\r\n",
    "\r\n",
    "    model.add(Dropout(0.3))\r\n",
    "    model.add(Dense(32, activation='relu'))\r\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\r\n",
    "    model.add(Bidirectional(LSTM(32)))\r\n",
    "    model.add(Dense(32, activation='relu'))\r\n",
    "    model.add(Dropout(0.3))\r\n",
    "\r\n",
    "    model.add(Dense(1, activation='sigmoid'))\r\n",
    "    return model\r\n",
    "\r\n",
    "\r\n",
    "def learning(model_, word_seq_train_, train_df_):\r\n",
    "    y_train = train_df_['label'].values\r\n",
    "    model_.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
    "\r\n",
    "    es_callback = EarlyStopping(monitor='val_loss', patience=3)\r\n",
    "\r\n",
    "    history = model_.fit(word_seq_train_, y_train, batch_size=32,\r\n",
    "                         epochs=NUM_EPOCHS,\r\n",
    "                         validation_split=0.2,\r\n",
    "                         callbacks=[es_callback],\r\n",
    "                         shuffle=False)\r\n",
    "    print(history)\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    csv_string = string_from_AWS()\r\n",
    "\r\n",
    "    train_df, max_seq_len = stringToTrainDf(csv_string)\r\n",
    "\r\n",
    "    loadWikiModel()\r\n",
    "\r\n",
    "    word_index, word_seq_train = word_indexing(train_df, max_seq_len)\r\n",
    "\r\n",
    "    model = makingModel(word_index, max_seq_len)\r\n",
    "\r\n",
    "    learning(model, word_seq_train, train_df)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "879131it [01:49, 8027.60it/s]\n",
      "100%|██████████| 198/198 [00:05<00:00, 33.29it/s] \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dictionary size :  969\n",
      "number of null word embeddings: 79\n",
      "sample words not found:  ['5만' '대우받는다' '15억' '1억원' '19' '대우받는다' '바꾸는거다' '250' '6~7' '2012년']\n",
      "Extension horovod.torch has not been built: /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-37m-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n",
      "[2021-10-06 08:55:36.598 ip-172-31-32-76:3677 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-10-06 08:55:37.001 ip-172-31-32-76:3677 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Epoch 1/40\n",
      "5/5 [==============================] - 18s 782ms/step - loss: 0.6975 - accuracy: 0.4357 - val_loss: 0.6917 - val_accuracy: 0.5500\n",
      "Epoch 2/40\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.6873 - accuracy: 0.5775 - val_loss: 0.6912 - val_accuracy: 0.5750\n",
      "Epoch 3/40\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.6863 - accuracy: 0.5805 - val_loss: 0.6915 - val_accuracy: 0.5250\n",
      "Epoch 4/40\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.6830 - accuracy: 0.5745 - val_loss: 0.6895 - val_accuracy: 0.5250\n",
      "Epoch 5/40\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.6766 - accuracy: 0.6206 - val_loss: 0.6792 - val_accuracy: 0.5750\n",
      "Epoch 6/40\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.6536 - accuracy: 0.6764 - val_loss: 0.6710 - val_accuracy: 0.5750\n",
      "Epoch 7/40\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.6216 - accuracy: 0.7451 - val_loss: 0.6715 - val_accuracy: 0.5750\n",
      "Epoch 8/40\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.5949 - accuracy: 0.7584 - val_loss: 0.6527 - val_accuracy: 0.6000\n",
      "Epoch 9/40\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 0.5427 - accuracy: 0.7730 - val_loss: 0.6741 - val_accuracy: 0.6000\n",
      "Epoch 10/40\n",
      "5/5 [==============================] - 0s 28ms/step - loss: 0.4967 - accuracy: 0.7497 - val_loss: 0.7564 - val_accuracy: 0.5500\n",
      "Epoch 11/40\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.4194 - accuracy: 0.8286 - val_loss: 0.7637 - val_accuracy: 0.5500\n",
      "<tensorflow.python.keras.callbacks.History object at 0x7f5bb065d850>\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
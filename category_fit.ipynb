{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hollywood-indonesia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "879131it [03:27, 4235.34it/s]\n",
      "100%|██████████| 509/509 [00:12<00:00, 40.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size :  2095\n",
      "number of null word embeddings: 167\n",
      "sample words not found:  ['60%' '2.1%' '30일' '2%' '스쿨존' '1987년' '메가시티' '199' '2007년' '못연']\n",
      "Extension horovod.torch has not been built: /home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/horovod/torch/mpi_lib/_mpi_lib.cpython-37m-x86_64-linux-gnu.so not found\n",
      "If this is not expected, reinstall Horovod with HOROVOD_WITH_PYTORCH=1 to debug the build error.\n",
      "Warning! MPI libs are missing, but python applications are still avaiable.\n",
      "[2021-10-12 08:55:59.012 ip-172-31-32-76:3836 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-10-12 08:55:59.822 ip-172-31-32-76:3836 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Epoch 1/40\n",
      "13/13 [==============================] - 46s 596ms/step - loss: 0.6959 - accuracy: 0.5079 - val_loss: 0.6937 - val_accuracy: 0.4510\n",
      "Epoch 2/40\n",
      "13/13 [==============================] - 1s 74ms/step - loss: 0.6931 - accuracy: 0.5099 - val_loss: 0.6904 - val_accuracy: 0.5882\n",
      "Epoch 3/40\n",
      "13/13 [==============================] - 0s 34ms/step - loss: 0.6867 - accuracy: 0.5506 - val_loss: 0.6926 - val_accuracy: 0.5588\n",
      "Epoch 4/40\n",
      "13/13 [==============================] - 0s 35ms/step - loss: 0.6814 - accuracy: 0.5467 - val_loss: 0.6946 - val_accuracy: 0.6275\n",
      "Epoch 5/40\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.6700 - accuracy: 0.6023 - val_loss: 0.6632 - val_accuracy: 0.5882\n",
      "Epoch 6/40\n",
      "13/13 [==============================] - 0s 35ms/step - loss: 0.6286 - accuracy: 0.6763 - val_loss: 0.7080 - val_accuracy: 0.5980\n",
      "Epoch 7/40\n",
      "13/13 [==============================] - 0s 35ms/step - loss: 0.5901 - accuracy: 0.6701 - val_loss: 0.7321 - val_accuracy: 0.5784\n",
      "Epoch 8/40\n",
      "13/13 [==============================] - 0s 33ms/step - loss: 0.5448 - accuracy: 0.7287 - val_loss: 0.7827 - val_accuracy: 0.5882\n",
      "<tensorflow.python.keras.callbacks.History object at 0x7f2b8c611fd0>\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import boto3\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Embedding, Dropout, Dense, Bidirectional, LSTM\n",
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from io import StringIO\n",
    "\n",
    "OBJECT_KEY_DIC = {\"all\": \"SNU_All_b.csv\", \"politics\": \"SNU_정치_b.csv\", \"economics\": \"SNU_경제_b.csv\",\n",
    "                  \"society\": \"SNU_사회_b.csv\", \"etc\": \"SNU_기타_b.csv\"}\n",
    "\n",
    "# AWS에서는 loadWikiModel 메소드의 f.open 파라미터를 아래로 바꿀것.\n",
    "WIKI_VEC_DIR = \"/home/ubuntu/FastText/wiki.ko.vec\"\n",
    "\n",
    "LOCAL_WIKI_VEC_DIR = \"./wiki.ko.vec\"\n",
    "\n",
    "# training params\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 40\n",
    "\n",
    "# model parameters\n",
    "MAX_NB_WORDS = 100000\n",
    "NUM_FILTERS = 64\n",
    "EMBED_DIM = 300\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EMBEDDING_INDEX = {}\n",
    "\n",
    "\n",
    "# Vectorizer의 argument인 tokenizer에 KoNLPy의 pos 함수로 대체.\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        clean_words = []  # 정제된 단어 리스트\n",
    "        for word in pos:\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\n",
    "                    # if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\n",
    "                    clean_words.append(word[0])\n",
    "        return clean_words\n",
    "\n",
    "\n",
    "def string_from_AWS() -> str:\n",
    "    # get your credentials from environment variables\n",
    "    AWS_ID = ''\n",
    "    AWS_SECRET = ''\n",
    "\n",
    "    client = boto3.client('s3', aws_access_key_id=AWS_ID,\n",
    "                          aws_secret_access_key=AWS_SECRET)\n",
    "    bucket_name = 'snucsv'\n",
    "\n",
    "    # OBJECT_KEY_DIC 참고.\n",
    "    object_key = OBJECT_KEY_DIC[\"politics\"]\n",
    "    csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    body = csv_obj['Body']\n",
    "    csv_string = body.read().decode('cp949')\n",
    "\n",
    "    return csv_string\n",
    "\n",
    "\n",
    "# load data\n",
    "def stringToTrainDf(csv_string_: str) -> tuple:\n",
    "    train_df = pd.read_csv(StringIO(csv_string_))\n",
    "\n",
    "    train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\n",
    "\n",
    "    # 모델 학습에 파라미터로 쓰임.\n",
    "    max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "    return train_df, max_seq_len\n",
    "\n",
    "\n",
    "def loadWikiModel():\n",
    "    f = codecs.open(WIKI_VEC_DIR, encoding='utf-8')\n",
    "\n",
    "    for line in tqdm(f):\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        EMBEDDING_INDEX[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def word_indexing(train_df_, max_seq_len_) -> tuple:\n",
    "    my_Tokenizer = MyTokenizer(Okt())\n",
    "\n",
    "    raw_docs_train = train_df_['document'].tolist()\n",
    "    # raw_docs_test = test_df['document'].tolist()\n",
    "    # print(raw_docs_test)\n",
    "\n",
    "    processed_docs_train = []\n",
    "    # processed_docs_test = []\n",
    "\n",
    "    for doc in tqdm(raw_docs_train):\n",
    "        tokens = my_Tokenizer(doc)\n",
    "        processed_docs_train.append(tokens)\n",
    "\n",
    "    # for doc in tqdm(raw_docs_test):\n",
    "    #     tokens = my_Tokenizer(doc)\n",
    "    #     processed_docs_test.append(tokens)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "    # tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)\n",
    "    tokenizer.fit_on_texts(processed_docs_train)\n",
    "    # tokenizer.fit_on_texts(processed_docs_train+proccessed_docs_test)\n",
    "    word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "    word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len_)\n",
    "    # word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print(\"dictionary size : \", len(word_index))\n",
    "    return word_index, word_seq_train\n",
    "\n",
    "\n",
    "def makingModel(word_index_, max_seq_len_):\n",
    "    # embedding matrix\n",
    "    words_not_found = []\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index_) + 1)\n",
    "    embedding_matrix = np.zeros((nb_words, EMBED_DIM))\n",
    "\n",
    "    # print(word_index)\n",
    "    for word, i in word_index_.items():\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        embedding_vector = EMBEDDING_INDEX.get(word)\n",
    "        if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            words_not_found.append(word)\n",
    "            # cc.ko.300.vec에서 찾지 못한 단어들의 리스트.\n",
    "    print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    print(\"sample words not found: \", np.random.choice(words_not_found, 10))\n",
    "\n",
    "    from keras.layers import BatchNormalization\n",
    "    import tensorflow as tf\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(Embedding(nb_words, EMBED_DIM, input_length=max_seq_len_, weights=[embedding_matrix], trainable=False))\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def learning(model_, word_seq_train_, train_df_):\n",
    "    y_train = train_df_['label'].values\n",
    "    model_.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "    history = model_.fit(word_seq_train_, y_train, batch_size=32,\n",
    "                         epochs=NUM_EPOCHS,\n",
    "                         validation_split=0.2,\n",
    "                         callbacks=[es_callback],\n",
    "                         shuffle=False)\n",
    "    print(history)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    csv_string = string_from_AWS()\n",
    "\n",
    "    train_df, max_seq_len = stringToTrainDf(csv_string)\n",
    "\n",
    "    loadWikiModel()\n",
    "\n",
    "    word_index, word_seq_train = word_indexing(train_df, max_seq_len)\n",
    "\n",
    "    model = makingModel(word_index, max_seq_len)\n",
    "\n",
    "    learning(model, word_seq_train, train_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

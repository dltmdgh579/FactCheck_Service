{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "religious-newsletter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>link</th>\n",
       "      <th>label</th>\n",
       "      <th>doc_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>대선에 출마하는 국회의원은 12월까지 자리에서 물러나야 한다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/3191</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>노령연금을 더 빨리 또는 더 늦게 받을 수 있다?</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/964</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"문 정부 탈원전 정책이 산불 원인이다\"</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/1499</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>해병대, 육·해·공군 보다 구타·가혹행위 심하다?</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/414</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"언론이 정신질환에 대한 사회적 혐오를 부추겨왔다\"</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/1655</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>\"공정거래위원회 전속고발권 없애면 검찰권한 확대된다.\"</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/2697</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>\"박근혜 정부 때 카풀법안 허용\"?</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/1245</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>“대외경제정책연구원의 유럽 사무소 설치 예산을 전액 삭감했다”</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/662</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>\"연동형 비례대표제는 위헌이다\"</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/1909</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1440</th>\n",
       "      <td>전동킥보드 ‘중학생’이 타면 불법이다</td>\n",
       "      <td>https://factcheck.snu.ac.kr/v2/facts/2927</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1441 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                document  \\\n",
       "0      대선에 출마하는 국회의원은 12월까지 자리에서 물러나야 한다   \n",
       "1            노령연금을 더 빨리 또는 더 늦게 받을 수 있다?   \n",
       "2                 \"문 정부 탈원전 정책이 산불 원인이다\"   \n",
       "3            해병대, 육·해·공군 보다 구타·가혹행위 심하다?   \n",
       "4           \"언론이 정신질환에 대한 사회적 혐오를 부추겨왔다\"   \n",
       "...                                  ...   \n",
       "1436      \"공정거래위원회 전속고발권 없애면 검찰권한 확대된다.\"   \n",
       "1437                 \"박근혜 정부 때 카풀법안 허용\"?   \n",
       "1438  “대외경제정책연구원의 유럽 사무소 설치 예산을 전액 삭감했다”   \n",
       "1439                  \"연동형 비례대표제는 위헌이다\"    \n",
       "1440                전동킥보드 ‘중학생’이 타면 불법이다   \n",
       "\n",
       "                                           link  label  doc_len  \n",
       "0     https://factcheck.snu.ac.kr/v2/facts/3191      0        7  \n",
       "1      https://factcheck.snu.ac.kr/v2/facts/964      1        9  \n",
       "2     https://factcheck.snu.ac.kr/v2/facts/1499      0        6  \n",
       "3      https://factcheck.snu.ac.kr/v2/facts/414      1        5  \n",
       "4     https://factcheck.snu.ac.kr/v2/facts/1655      1        7  \n",
       "...                                         ...    ...      ...  \n",
       "1436  https://factcheck.snu.ac.kr/v2/facts/2697      0        5  \n",
       "1437  https://factcheck.snu.ac.kr/v2/facts/1245      0        5  \n",
       "1438   https://factcheck.snu.ac.kr/v2/facts/662      0        7  \n",
       "1439  https://factcheck.snu.ac.kr/v2/facts/1909      0        4  \n",
       "1440  https://factcheck.snu.ac.kr/v2/facts/2927      1        4  \n",
       "\n",
       "[1441 rows x 4 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
    "import logging\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from gensim.models import FastText\n",
    "from konlpy.tag import Okt\n",
    "import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "if sys.version_info[0] < 3:\n",
    "    from io import StringIO # Python 2.x\n",
    "else:\n",
    "    from io import StringIO # Python 3.x\n",
    "\n",
    "# get your credentials from environment variables\n",
    "aws_id = ''\n",
    "aws_secret = ''\n",
    "\n",
    "client = boto3.client('s3', aws_access_key_id=aws_id,\n",
    "        aws_secret_access_key=aws_secret)\n",
    "\n",
    "bucket_name = 'snucsv'\n",
    "\n",
    "object_key = 'SNU_All_b.csv'\n",
    "csv_obj = client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('cp949')\n",
    "\n",
    "# load data\n",
    "train_df = pd.read_csv(StringIO(csv_string))\n",
    "label_names = [\"label\"]\n",
    "y_train = train_df[label_names].values\n",
    "\n",
    "train_df['doc_len'] = train_df['document'].apply(lambda words: len(words.split(\" \")))\n",
    "max_seq_len = np.round(train_df['doc_len'].mean() + train_df['doc_len'].std()).astype(int)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "reported-setting",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_df['label'] = train_df['label'].map({'전혀 사실 아님':0, '대체로 사실 아님':0, '사실':1, '대체로 사실':1})\n",
    "# train_df.dropna(axis=0, inplace=True)\n",
    "# train_df.reset_index(drop=True, inplace=True)\n",
    "# train_df['label'].astype(int)\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-fancy",
   "metadata": {},
   "source": [
    "**데이터 불균형 -> Under Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "extensive-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_true = train_df[train_df['label'] == 1]\n",
    "# train_df_true.reset_index(drop=True, inplace=True)\n",
    "# train_df_fake = train_df[train_df['label'] == 0]\n",
    "# train_df_fake.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "instant-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "distributed-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_fake_sample = train_df_fake.loc[0:255]\n",
    "# train_df_fake_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "loved-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df_balance = pd.concat([train_df_true, train_df_fake_sample])\n",
    "# train_df_balance = train_df_balance.sample(frac=1).reset_index(drop=True)\n",
    "# train_df_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "digital-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "designing-compilation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "879131it [02:09, 6766.10it/s]\n",
      "  6%|▌         | 84/1441 [00:00<00:01, 838.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 879130 word vectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1441/1441 [00:03<00:00, 369.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size :  3974\n",
      "number of null word embeddings: 349\n",
      "sample words not found:  ['25' '150만원' '50%' '8000' '공시가격' '2년' '2~3년' '200만' '펼쳐졌습니다' '2014년']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sys\n",
    "from gensim.models import FastText\n",
    "import os, csv, math, codecs\n",
    "from tqdm import tqdm\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "MAX_NB_WORDS = 100000\n",
    "\n",
    "# =========================================================================\n",
    "# Vectorizer의 argument인 tokenizer에 KoNLPy의 pos 함수로 대체.\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        clean_words = []  # 정제된 단어 리스트\n",
    "        for word in pos:\n",
    "            # word[1]은 품사를 의미하며, 여기서는 조사, 문장기호, 접두사, Foreign('\\n'을 빼주기 위함)인 것은 제외시킴.\n",
    "            if word[1] not in ['Josa', 'Punctuation', 'Suffix', 'Foreign']:\n",
    "                if len(word[0]) >= 2:  # 한 글자인 단어들도 의미가 없는 경우가 많으므로 일단 제외.\n",
    "                    #if word[0] not in ['있다', '했다', '한다', '없다', '된다']:\n",
    "                    clean_words.append(word[0])\n",
    "        return clean_words\n",
    "\n",
    "# =========================================================================\n",
    "\n",
    "embeddings_index = {}\n",
    "f = codecs.open('/home/ubuntu/FastText/wiki.ko.vec', encoding='utf-8')\n",
    "\n",
    "for line in tqdm(f):\n",
    "    values = line.rstrip().rsplit(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('found %s word vectors' % len(embeddings_index))\n",
    "\n",
    "# ==================================================================================\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "my_Tokenizer = MyTokenizer(Okt())\n",
    "\n",
    "raw_docs_train = train_df['document'].tolist()\n",
    "# raw_docs_test = test_df['document'].tolist()\n",
    "# print(raw_docs_test)\n",
    "num_classes = len(label_names)\n",
    "\n",
    "processed_docs_train = []\n",
    "# processed_docs_test = []\n",
    "\n",
    "for doc in tqdm(raw_docs_train):\n",
    "    tokens = my_Tokenizer(doc)\n",
    "    processed_docs_train.append(tokens)\n",
    "\n",
    "# for doc in tqdm(raw_docs_test):\n",
    "#     tokens = my_Tokenizer(doc)\n",
    "#     processed_docs_test.append(tokens)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
    "# tokenizer.fit_on_texts(processed_docs_train + processed_docs_test)\n",
    "tokenizer.fit_on_texts(processed_docs_train)\n",
    "# tokenizer.fit_on_texts(processed_docs_train+proccessed_docs_test)\n",
    "word_seq_train = tokenizer.texts_to_sequences(processed_docs_train)\n",
    "# word_seq_test = tokenizer.texts_to_sequences(processed_docs_test)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"dictionary size : \", len(word_index))\n",
    "\n",
    "word_seq_train = sequence.pad_sequences(word_seq_train, maxlen=max_seq_len)\n",
    "# word_seq_test = sequence.pad_sequences(word_seq_test, maxlen=max_seq_len)\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "# ====================================================================================\n",
    "# training params\n",
    "batch_size = 256\n",
    "num_epochs = 40\n",
    "\n",
    "# model parameters\n",
    "num_filters = 64\n",
    "embed_dim = 300\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# embedding matrix\n",
    "\n",
    "words_not_found = []\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "\n",
    "# print(word_index)\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "        # cc.ko.300.vec에서 찾지 못한 단어들의 리스트.\n",
    "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "print(\"sample words not found: \", np.random.choice(words_not_found, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "straight-newsletter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 11, 300)           1192500   \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 11, 300)           0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 11, 32)            9632      \n",
      "_________________________________________________________________\n",
      "bidirectional_20 (Bidirectio (None, 11, 128)           49664     \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,295,125\n",
      "Trainable params: 102,625\n",
      "Non-trainable params: 1,192,500\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "36/36 [==============================] - 40s 274ms/step - loss: 0.6937 - accuracy: 0.5028 - val_loss: 0.6761 - val_accuracy: 0.6263\n",
      "Epoch 2/40\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 0.6723 - accuracy: 0.5917 - val_loss: 0.6565 - val_accuracy: 0.6194\n",
      "Epoch 3/40\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 0.6354 - accuracy: 0.6417 - val_loss: 0.6328 - val_accuracy: 0.6263\n",
      "Epoch 4/40\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 0.6055 - accuracy: 0.6781 - val_loss: 0.6194 - val_accuracy: 0.6367\n",
      "Epoch 5/40\n",
      "36/36 [==============================] - 1s 26ms/step - loss: 0.5709 - accuracy: 0.6991 - val_loss: 0.6177 - val_accuracy: 0.6574\n",
      "Epoch 6/40\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 0.5607 - accuracy: 0.7273 - val_loss: 0.6129 - val_accuracy: 0.6540\n",
      "Epoch 7/40\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 0.5284 - accuracy: 0.7517 - val_loss: 0.6082 - val_accuracy: 0.6713\n",
      "Epoch 8/40\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 0.5306 - accuracy: 0.7443 - val_loss: 0.6122 - val_accuracy: 0.6782\n",
      "Epoch 9/40\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.4967 - accuracy: 0.7463 - val_loss: 0.6005 - val_accuracy: 0.6817\n",
      "Epoch 10/40\n",
      "36/36 [==============================] - 1s 21ms/step - loss: 0.4956 - accuracy: 0.7710 - val_loss: 0.6028 - val_accuracy: 0.6990\n",
      "Epoch 11/40\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 0.4548 - accuracy: 0.7875 - val_loss: 0.6418 - val_accuracy: 0.7128\n",
      "Epoch 12/40\n",
      "36/36 [==============================] - 1s 28ms/step - loss: 0.4317 - accuracy: 0.8042 - val_loss: 0.6576 - val_accuracy: 0.7024\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================================\n",
    "from keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "model.add(Embedding(nb_words, embed_dim, input_length=max_seq_len, weights=[embedding_matrix], trainable=False))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "# ================================================================================\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history = model.fit(word_seq_train, y_train, batch_size=32,\n",
    "          epochs=num_epochs, \n",
    "                    validation_split=0.2, \n",
    "                    callbacks=[es_callback], \n",
    "                    shuffle=False)\n",
    "\n",
    "# predictions = model.predict_classes(word_seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "nutritional-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('SNU_LSTM_Model_All_balance.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-venice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "certified-composition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 [==============================] - 9s 9ms/step - loss: 0.6917 - accuracy: 0.5076\n",
      "197/197 [==============================] - 8s 6ms/step - loss: 0.8514 - accuracy: 0.5076\n",
      "197/197 [==============================] - 7s 6ms/step - loss: 0.9111 - accuracy: 0.4975\n"
     ]
    }
   ],
   "source": [
    "balanced_model = load_model('SNU_LSTM_Model_경제_balance.h5')\n",
    "balanced_score = balanced_model.evaluate(word_seq_train, y_train, batch_size=1)\n",
    "model = load_model('SNU_LSTM_Model_경제.h5')\n",
    "scores = model.evaluate(word_seq_train, y_train, batch_size=1)\n",
    "all_model = load_model('SNU_LSTM_Model_All.h5')\n",
    "all_scores = all_model.evaluate(word_seq_train, y_train, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eastern-november",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경제 balanced data LSTM accuracy: 50.76%\n",
      "경제 data LSTM accuracy: 50.76%\n",
      "전체 data LSTM accuracy: 49.75%\n"
     ]
    }
   ],
   "source": [
    "print(\"경제 balanced data LSTM %s: %.2f%%\" %(balanced_model.metrics_names[1], scores[1]*100))\n",
    "print(\"경제 data LSTM %s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    "print(\"전체 data LSTM %s: %.2f%%\" %(all_model.metrics_names[1], all_scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
